### 记面试题


- [面试3-总结](/src/main/resources//note/mianshi/面试3-总结.md)

<p>大概总结
<p> Java并发、mysql、网络、JDK集合、jvm、spring源码、tomcat、linux、系统设计、生产实践


<p>java 集合
<p> <a href="/src/main/resources/note/集合目录.md"> 这个可以看看之前写的集合</a>

<h3>1 HashMap   </h3>
<p><a href="/src/main/resources/note/conllection/HashMap源码分析.md">HashMap源码解析 </a>
<p> 数组+链表+红黑树
<p>在添加元素时，会根据hash值算出元素在数组中的位置，如果该位置没有元素，则直接把元素放置在此处，如果该位置有元素了，则把元素以链表的形式放置在链表的尾部。
<p>
<p>当一个链表的元素个数达到一定的数量（且数组的长度达到一定的长度）后，则把链表转化为红黑树，从而提高效率。
<p>
<p>数组的查询效率为O(1)，链表的查询效率是O(k)，红黑树的查询效率是O(log k)，k为桶中的元素个数，所以当元素数量非常多的时候，转化为红黑树能极大地提高效率。

<h4> hash 算法和寻址优化  (这个原来笔记没有)  </h4>
<pre>
  static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}</pre>

<p>  比如说：有一个key的hash值
<pre>
     1111 1111 1111 1111 1111 1010 0111 1100
     0000 0000 0000 0000 1111 1111 1111 1111
     1111 1111 1111 1111 0000 0101 1000 0011 -> int值，32位
     </pre>
<p> 简而言之就是,高16 不变,低16变成 高16位于低16位异或结果
<h4>寻址算法优化    </h4>
<p>(n - 1) & hash -> 数组里的一个位置
<li> 用& 是因为 & 运算比取摸效率高,当然结果一样,这是个数学问题
<li> &也会导致一个问题 ,高16 的&运算 没有实际意义
<p>假设有两个hash值  ,低16一样,高16不一样, 这样&运算结果就会有问题
  <pre>
1111 1111 1111 1111 1111 1010 0111 1100 -> 1111 1111 1111 1111 0000 0101 1000 0011
1111 1111 1111 1110 1111 1010 0111 1100 -> 1111 1111 1111 1110 0000 0101 1000 0010</pre>
<p> 这时就是hash算法的优化,也就是上面是异或,  异或之后 ,低16位现在包含高低16两段的特征
<h4>总结   </h4>
<p>hash算法的优化：对每个hash值，在他的低16位中，让高低16位进行了异或，让他的低16位同时保持了高低16位的特征，尽量避免一些hash值后续出现冲突，大家可能会进入数组的同一个位置
<p>寻址算法的优化：用与运算替代取模，提升性能

<h4> HashMap是如何解决hash碰撞问题     </h4>
<p>两个key，多个key，他们算出来的hash的值，与n-1，与运算之后，发现定位出来的数组的位置还是一样的，hash碰撞，hash冲突
<p>get，如果定位到数组里发现这个位置挂了一个链表，此时遍历链表，从里面找到自己的要找的那个key-value对就可以了
<p>假设你的链表很长，可能会导致遍历链表，性能会比较差，O(n)
<p>优化，如果链表的长度达到了一定的长度之后，其实会把链表转换为红黑树，遍历一颗红黑树找一个元素，此时O(logn)，性能会比链表高一些
<p> 转红黑树条件:数组长度64,链表长度8 
<p>当单个桶中元素数量小于6时，进行反树化

<h4> HashMap是如何进行扩容的  </h4>
<p>（1）如果使用是默认构造方法，则第一次插入元素时初始化为默认值，容量为16，扩容门槛为12；
<p>（2）如果使用的是非默认构造方法，则第一次插入元素时初始化容量等于扩容门槛，扩容门槛在构造方法里等于传入容量向上最近的2的n次方；
<p>（3）如果旧容量大于0，则新容量等于旧容量的2倍，但不超过最大容量2的30次方，新扩容门槛为旧扩容门槛的2倍；
<p>（4）创建一个新容量的桶；
<p>（5）搬移元素，原链表分化成两个链表，低位链表存储在原来桶的位置，高位链表搬移到原来桶的位置加旧容量的位置；
<p>如果数组的长度扩容之后 = 32，重新对每个hash值进行寻址，也就是用每个hash值跟新数组的length - 1进行与操作
<pre>
n-1        0000 0000 0000 0000 0000 0000 0001 1111
hash1     1111 1111 1111 1111 0000 1111 0000 0101
&结果    0000 0000 0000 0000 0000 0000 0000 0101 = 5（index = 5的位置）
n-1        0000 0000 0000 0000 0000 0000 0001 1111
hash2     1111 1111 1111 1111 0000 1111 0001 0101
&结果    0000 0000 0000 0000 0000 0000 0001 0101 = 21（index = 21的位置）  </pre>
<p>判断二进制结果中是否多出一个bit的1，如果没多，那么就是原来的index，如果多了出来，那么就是index + oldCap，通过这个方式，就避免了rehash的时候，用每个hash对新数组.length取模，取模性能不高，位运算的性能比较高

<h3> 2  并发编程  </h3>
<p> synchronized实现原理、CAS无锁化的原理、AQS是什么、Lock锁、ConcurrentHashMap的分段加锁的原理、线程池的原理、java内存模型、volatile、对java并发包

<h4> synchronized关键字    </h4>
<p>synchronized关键字，在底层编译后的jvm指令中，会有monitorenter和monitorexit两个指令
<p>每个对象都有一个关联的monitor，比如一个对象实例就有一个monitor，一个类的Class对象也有一个monitor，如果要对这个对象加锁，那么必须获取这个对象关联的monitor的lock锁
<p>他里面的原理和思路大概是这样的，monitor里面有一个计数器，从0开始的。如果一个线程要获取monitor的锁，就看看他的计数器是不是0，如果是0的话，那么说明没人获取锁，他就可以获取锁了，然后对计数器加1

<p>如果一个线程第一次synchronized那里，获取到了myObject对象的monitor的锁，计数器加1，然后第二次synchronized那里，会再次获取myObject对象的monitor的锁，这个就是重入加锁了，然后计数器会再次加1，变成2
<p>这个时候，其他的线程在第一次synchronized那里，会发现说myObject对象的monitor锁的计数器是大于0的，意味着被别人加锁了，然后此时线程就会进入block阻塞状态，什么都干不了，就是等着获取锁
<p>接着如果出了synchronized修饰的代码片段的范围，就会有一个monitorexit的指令，在底层。此时获取锁的线程就会对那个对象的monitor的计数器减1，如果有多次重入加锁就会对应多次减1，直到最后，计数器是0
<p>然后后面block住阻塞的线程，会再次尝试获取锁，但是只有一个线程可以获取到锁

<h4> CAS的理解   </h4>
<p>有多个线程要同时读写类似上面的这种内存里的数据，此时必然出现多线程的并发安全问题
<p>CAS，compare and set
<p>CAS在底层的硬件级别给你保证一定是原子的，同一时间只有一个线程可以执行CAS，先比较再设置，其他的线程的CAS同时间去执行此时会失败

<h4> ConcurrentHashMap实现线程安全的底层原理
<p><a href="/src/main/resources/note/conllection/ConcurrentHashMap.md"> ConcurrentHashMap笔记</a>
<p>JDK并发包里推出了一个ConcurrentHashMap，他默认实现了线程安全性
<p>ps:对同一个元素执行put操作，此时是需要多线程是需要进行同步的
<p>[一个大的数组]，数组里每个元素进行put操作，都是有一个不同的锁，刚开始进行put的时候，如果两个线程都是在数组[5]这个位置进行put，这个时候，对数组[5]这个位置进行put的时候，采取的是CAS的策略
<p>同一个时间，只有一个线程能成功执行这个CAS，就是说他刚开始先获取一下数组[5]这个位置的值，null，然后执行CAS，线程1，比较一下，put进去我的这条数据，同时间，其他的线程执行CAS，都会失败
<p>分段加锁，通过对数组每个元素执行CAS的策略，如果是很多线程对数组里不同的元素执行put，大家是没有关系的，如果其他人失败了，其他人此时会发现说，数组[5]这位置，已经给刚才又人放进去值了
<p>就需要在这个位置基于链表+红黑树来进行处理，synchronized(数组[5])，加锁，基于链表或者是红黑树在这个位置插进去自己的数据
<p>如果你是对数组里同一个位置的元素进行操作，才会加锁串行化处理；如果是对数组不同位置的元素操作，此时大家可以并发执行的

<h6> 总结   </h6>
<p>（1）CAS + 自旋，乐观锁的思想，减少线程上下文切换的时间；
<p>（2）分段锁的思想，减少同一把锁争用带来的低效问题；
<p>（3）CounterCell，分段存储元素个数，减少多线程同时更新一个字段带来的低效；
<p>（4）@sun.misc.Contended（CounterCell上的注解），避免伪共享；（p.s.看笔记）
<p>（5）多线程协同进行扩容；
<h6> 不能解决的问题
<p>1  特定值put
<pre>
 public void unsafeUpdate(Integer key, Integer value) {
     Integer oldValue = map.get(key);
     if (oldValue == null) {
         map.put(key, value);
     }
 }</pre>
<p> putIfAbsent ,null才插入,
<p>另一个方法叫replace(K key, V oldValue, V newValue) 如果传入的ne程池中的最大线程数量。程池中的最大线程数量。程池中的最大线程数量。wValue是null，则会删除元素
<p> 2   不具备原子性  (多线程如果对同一个key操作 )
<p> 自己加锁,保证安全

<h4> JDK中的AQS理解     </h4>
<p> AQS，Abstract Queue Synchronizer，抽象队列同步器
<p>state变量 -> CAS -> 失败后进入队列等待 -> 释放锁后唤醒
<p>它维护了一个 volatile int state（代表共享资源）和一个 FIFO 线程等待队列（多线程争用资源被阻塞时会进入此队列）。这里 volatile 是核心关键词
<p> 等待线程进入等待队列
<p>ReentrantLock lock = new ReentrantLock();  => 非公平锁

<h4>线程池的底层工作原理   </h4>
<p>系统是不可能说让他无限制的创建很多很多的线程的，会构建一个线程池，有一定数量的线程，让他们执行各种各样的任务，线程执行完任务之后，不要销毁掉自己，继续去等待执行下一个任务
<li>提交任务，先看一下线程池里的线程数量是否小于corePoolSize，也就是3，如果小于，直接创建一个线程出来执行你的任务
<li>如果执行完你的任务之后，这个线程是不会死掉的，他会尝试从一个无界的LinkedBlockingQueue里获取新的任务，如果没有新的任务，此时就会阻塞住，等待新的任务到来
<li>你持续提交任务，上述流程反复执行，只要线程池的线程数量小于corePoolSize，都会直接创建新线程来执行这个任务，执行完了就尝试从无界队列里获取任务，直到线程池里有corePoolSize个线程
<li>接着再次提交任务，会发现线程数量已经跟corePoolSize一样大了，此时就直接把任务放入队列中就可以了，线程会争抢获取任务执行的，如果所有的线程此时都在执行任务，那么无界队列里的任务就可能会越来越多
<p>fixed，队列，LinkedBlockingQueue，无界阻塞队列

<h4> 线程池的核心配置参数   </h4>
<p>创建一个线程池就是这样子的，corePoolSize，maximumPoolSize，keepAliveTime，queue，这几个参数，如果你不用fixed之类的线程池，自己完全可以通过这个构造函数就创建自己的线程池
<li> corePoolSize：指定了线程池中的线程数量。
<li> maximumPoolSize：指定了线程池中的最大线程数量。
<li> keepAliveTime：当前线程池数量超过 corePoolSize 时，多余的空闲线程的存活时间，即多次时间内会被销毁。
<li> unit：keepAliveTime 的单位。
<li> workQueue：任务队列，被提交但尚未被执行的任务。
<li> threadFactory：线程工厂，用于创建线程，一般用默认的即可。
<li> handler：拒绝策略，当任务太多来不及处理，如何拒绝任务。
<p>拒绝策略如下：
<li>AbortPolicy ： 直接抛出异常，阻止系统正常运行。
<li>CallerRunsPolicy ： 只要线程池未关闭，该策略直接在调用者线程中，运行当前被丢弃的任务。显然这样做不会真的丢弃任务，但是，任务提交线程的性能极有可能会急剧下降。
<li> DiscardOldestPolicy ： 丢弃最老的一个请求，也就是即将被执行的一个任务，并尝试再次提交当前任务。
<li> DiscardPolicy ： 该策略默默地丢弃无法处理的任务，不予任何处理。如果允许任务丢失，这是最好的一种方案。
<li>  以上内置拒绝策略均实现了 RejectedExecutionHandler 接口，若以上策略仍无法满足实际需要，完全可以自己扩展 RejectedExecutionHandler 接口。   
      
<h4> 线程池中使用无界阻塞队列会发生什么问题   </h4>
<p>在远程服务异常的情况下，使用无界阻塞队列，是否会导致内存异常飙升？
<p>调用超时，队列变得越来越大，此时会导致内存飙升起来，而且还可能会导致你会OOM，内存溢出

<h4> 线上机器突然宕机，线程池的阻塞队列中的请求  ?    </h4>
<p>必然会导致线程池里的积压的任务实际上来说都是会丢失的
<p>如果说你要提交一个任务到线程池里去，在提交之前，麻烦你先在数据库里插入这个任务的信息，更新他的状态：未提交、已提交、已完成。提交成功之后，更新他的状态是已提交状态
<p>系统重启，后台线程去扫描数据库里的未提交和已提交状态的任务，可以把任务的信息读取出来，重新提交到线程池里去，继续进行执行

<h3> Java内存模型    </h3>
<p> 之前计算机组成原理写过一些( 说下volatile 在java中的实现)
<p>Java 提供给我们的 8 个原子操作：lock、unlock、read、load、use、assign、store、write
<p>一个变量从主内存拷贝到工作内存，再从工作内存同步回主内存的流程为：

>|主内存| -> read -> load -> |工作内存| -> user -> |Java线程| -> assign -> |工作内存| -> store -> write -> |主内存|

<p> 8 个原子操作
<li>lock：作用于主内存，把一个变量标识为一个线程独占状态。
<li>unlock：作用于主内存，释放一个处于锁定状态的变量。
<li>read：作用于主内存，把一个变量的值从主内存传输到线程工作内存中，供之后的 load 操作使用。
<li>load：作用于工作内存，把 read 操作从主内存中得到的变量值放入工作内存的变量副本中。
<li>use：作用于工作内存，把工作内存中的一个变量传递给执行引擎，虚拟机遇到使用变量值的字节码指令时会执行。
<li>assign：作用于工作内存，把一个从执行引擎得到的值赋给工作内存的变量，虚拟机遇到给变量赋值的字节码指令时会执行。
<li>store：作用于工作内存，把工作内存中的一个变量传送到主内存中，供之后的 write 操作使用。
<li>write：作用于主内存，把 store 操作从工作内存中得到的变量值存入主内存的变量中。
<p>主内存对应 系统内存空间,工作内存对应CPU高速缓存内存
<li>use 操作必须与 load、read 操作同时出现，不能只 use，不 load、read。
<ul>use <- load <- read     </ul>     

<p> 使用时必须去取主内存

<li>assign 操作必须与 store、write 操作同时出现，不能只 assign，不 store、write。
<ul>assign -> store -> write

<p> 更新必须写主内存


<h4> Java内存模型中的原子性、有序性、可见性</h4>
<p> Java内存模型 -> 原子性、可见性、有序性 -> volatile -> happens-before / 内存屏障
<p> 可见性
<p> 如果有一个线程修改了,会让其他线程工作空间的值失效,重新读主内存的值
<p> 原子性
<p>data++，必须是独立执行的，没有人影响我的，一定是我自己执行成功之后，别人才能来进行下一次data++的执行
<p>有序性
<p> 对于代码，同时还有一个问题是指令重排序，编译器和指令器，有的时候为了提高代码执行效率，会将指令重排序，就是说比如下面的代码
<p> 具备有序性，不会发生指令重排导致我们的代码异常；不具备有序性，可能会发生一些指令重排，导致代码可能会出现一些问题

<h4>volatile关键字的原理</h4>
<p>volatile关键字是用来解决可见性和有序性，在有些罕见的条件之下，可以有限的保证原子性，他主要不是用来保证原子性的


<h4> 指令重排以及happens-before原则</h4>
<p> volatile关键字和有序性的关系，volatlie是如何保证有序性的，如何避免发生指令重排的
<p>根据语义，Happens-Before，就是即便是对于不同的线程，前面的操作也应该发生在后面操作的前面，也就是说，<strong>Happens-Before 规则保证：前面的操作的结果对后面的操作一定是可见的</strong>。</p>
<p><strong>Happens-Before 规则本质上是一种顺序约束规范，用来约束编译器的优化行为</strong>。就是说，为了执行效率，我们允许编译器的优化行为，但是为了保证程序运行的正确性，我们要求编译器优化后需要满足 Happens-Before 规则。</p>
<p>根据类别，我们将 Happens-Before 规则分为了以下 4 类：</p>
<ul>
<li>操作的顺序：
<ul>
<li><strong>程序顺序规则：</strong> 如果代码中操作 A 在操作 B 之前，那么同一个线程中 A 操作一定在 B 操作前执行，即在本线程内观察，所有操作都是有序的。</li>
<li><strong>传递性：</strong> 在同一个线程中，如果 A 先于 B ，B 先于 C 那么 A 必然先于 C。</li>
</ul>
</li>
<li>锁和 volatile：
<ul>
<li><strong>监视器锁规则：</strong> 监视器锁的解锁操作必须在同一个监视器锁的加锁操作前执行。</li>
<li><strong>volatile 变量规则：</strong> 对 volatile 变量的写操作必须在对该变量的读操作前执行，保证时刻读取到这个变量的最新值。</li>
</ul>
</li>
<li>线程和中断：
<ul>
<li><strong>线程启动规则：</strong> <code>Thread#start()</code> 方法一定先于该线程中执行的操作。</li>
<li><strong>线程结束规则：</strong> 线程的所有操作先于线程的终结。</li>
<li><strong>中断规则：</strong> 假设有线程 A，其他线程 interrupt A 的操作先于检测 A 线程是否中断的操作，即对一个线程的 <code>interrupt()</code> 操作和 <code>interrupted()</code> 等检测中断的操作同时发生，那么 <code>interrupt()</code> 先执行。</li>
</ul>
</li>
<li>对象生命周期相关：
<ul>
<li><strong>终结器规则：</strong> 对象的构造函数执行先于 <code>finalize()</code> 方法。</li>
</ul>
</li>
</ul>
<p>规则制定了在一些特殊情况下，不允许编译器、指令器对你写的代码进行指令重排，必须保证你的代码的有序性
<p>指令重排 -> happens-before -> volatile起到避免指令重排

<h4> volatile底层是如何基于内存屏障保证可见性和有序性的？</h4>
<p> 内存模型 -> 原子性、可见性、有序性 - > volatile+可见性 -> volatile+有序性（指令重排 + happens-before） -> voaltile+原子性 -> volatile底层的原理（内存屏障级别的原理）

<h5>lock指令：volatile保证可见性</h5>
<p>对volatile修饰的变量，执行写操作的话，JVM会发送一条lock前缀指令给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行嗅探，自己本地缓存中的数据是否被别人修改
<p>如果发现别人修改了某个缓存的数据，那么CPU就会将自己本地缓存的数据过期掉，然后这个CPU上执行的线程在读取那个变量的时候，就会从主内存重新加载最新的数据了
<p>lock前缀指令 + MESI缓存一致性协议

<h5> volatile 的有序实现</h5>
<p>对于volatile修改变量的读写操作，都会加入内存屏障
<p>每个volatile写操作前面，加StoreStore屏障，禁止上面的普通写和他重排；每个volatile写操作后面，加StoreLoad屏障，禁止跟下面的volatile读/写重排
<p>每个volatile读操作后面，加LoadLoad屏障，禁止下面的普通读和voaltile读重排；每个volatile读操作后面，加LoadStore屏障，禁止下面的普通写和volatile读重排

<p>synchronized、volatile，底层都对应着一套复杂的cpu级别的硬件原理，大量的内存屏障的原理；lock API，concurrenthashmap，都是各种复杂的jdk级别的源码，技术深度是很深入的


<h3>3 Spring</h3>

<h4>Spring的 IOC 机制</h4>
<p>Spring IOC框架，控制反转，依赖注入
<p>tomcat在启动的时候，直接会启动spring容器
<p>spring ioc，spring容器，根据xml配置，或者是你的注解，去实例化你的一些bean对象，然后根据xml配置或者注解，去对bean对象之间的引用关系，去进行依赖注入，某个bean依赖了另外一个bean
<p>底层的核心技术，反射，他会通过反射的技术，直接根据你的类去自己构建对应的对象出来，用的就是反射技术
<p>spring ioc，系统的类与类之间彻底的解耦合

<h4> pring的AOP机制的理解</h4>
<p> spring核心框架里面，最关键的两个机制，就是ioc和aop，根据xml配置或者注解，去实例化我们所有的bean，管理bean之间的依赖注入，让类与类之间解耦，维护代码的时候可以更加的轻松便利
<p>他有几个概念，可以做一个切面，语法、用法、术语和概念
<p>做一个切面，如何定义呢？MyServiceXXXX的这种类，在这些类的所有方法中，都去织入一些代码，在所有这些方法刚开始运行的时候，都先去开启一个事务，在所有这些方法运行完毕之后，去根据是否抛出异常来判断一下，如果抛出异常，就回滚事务，如果没有异常，就提交事务 => AOP

<p>spring在运行的时候，动态代理技术，AOP的核心技术，就是动态代理
<p>他会给你的那些类生成动态代理,把你本身的类注入到代理类
<p>事务，mysql，数据库里都提供一个事务机制，我们如果开启一个事务，在这个事务里执行多条增删改的sql语句，这个过程中，如果任何一个sql语句失败了，会导致这个事务的回滚，把其他sql做的数据更改都恢复回去
<p>在一个事务里的所有sql，要么一起成功，要么一起失败，事务功能可以保证我们的数据的一致性，在业务逻辑组件里去加入这个事务

<h4>cglib动态代理？他跟jdk动态代理的区别</h4>
<p> 其实就是动态的创建一个代理类出来，创建这个代理类的实例对象，在这个里面引用你真正自己写的类，所有的方法的调用，都是先走代理类的对象，他负责做一些代码上的增强，再去调用你写的那个类
<p> spring里使用aop，比如说你对一批类和他们的方法做了一个切面，定义好了要在这些类的方法里增强的代码，spring必然要对那些类生成动态代理，在动态代理中去执行你定义的一些增强代码
<p> 如果你的类是实现了某个接口的，spring aop会使用jdk动态代理，生成一个跟你实现同样接口的一个代理类，构造一个实例对象出来，jdk动态代理，他其实是在你的类有接口的时候，就会来使用
<p> 很多时候我们可能某个类是没有实现接口的，spring aop会改用cglib来生成动态代理，他是生成你的类的一个子类，他可以动态生成字节码，覆盖你的一些方法，在方法里加入增强的代码

<p> AspectJ和spring aop
<p> AspectJ是一个代码生成工具
<p> AspectJ有自己的类装载器，支持在类装载时织入切面，即所谓的LTW机制
<p> AspectJ同样也支持运行时织入，运行时织入是基于动态代理的机制。（默认机制）
<p>spring aop是aop实现方案的一种，它支持在运行期基于动态代理的方式将aspect织入目标代码中来实现aop。
<p>但是spring aop的切入点支持有限，而且对于static方法和final方  法都无法支持aop（因为此类方法无法生成代理类）；另外spring aop只支持对于ioc容器管理的bean，其他的普通java类无法支持aop。同时spring整合了aspectj，使得在spring  体系中可以使用aspectj语法来实现aop

<h4>Spring中的Bean是线程安全的吗？</h4>
<p>Spring容器中的bean可以分为5个范围：

<li>（1）singleton：默认，每个容器中只有一个bean的实例
<li>（2）prototype：为每一个bean请求提供一个实例
<p>一般来说下面几种作用域，在开发的时候一般都不会用，99.99%的时候都是用singleton单例作用域
<p>（3）request：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收
<p>（4）session：与request范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效
<p>（5）global-session
<p>答案是否定的，绝对不可能是线程安全的，spring bean默认来说，singleton，都是线程不安全的，java web系统，一般来说很少在spring bean里放一些实例变量，一般来说他们都是多个组件互相调用，最终去访问数据库的
<p> 如果bean 有变量,多个请求是会不安全的


<h4>Spring的事务实现原理是什么？能聊聊你对事务传播机制的理解吗？</h4>
<p>事务的实现原理，事务传播机制，如果说你加了一个@Transactional注解，此时就spring会使用AOP思想，对你的这个方法在执行之前，先去开启事务，执行完毕之后，根据你方法是否报错，来决定回滚还是提交事务


<li>① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。
<li>② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。‘
<li>③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。
<li>④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。
<li>⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
<li>⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。
<li>⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。
<p>出去面试，事务传播机制
<p>嵌套事务，外层的事务如果回滚，会导致内层的事务也回滚；但是内层的事务如果回滚，仅仅是回滚自己的代码

<h4> Spring Boot的核心架构 </h4>
<p>spring框架，mybatis，spring mvc，去做一些开发，打包部署到线上的tomcat里去，tomcat启动了，他就会接收http请求，转发给spring mvc框架，调用controller -> service -> dao -> mybatis（sql语句）
<p>java web开发的时候，在这里整合进来redis、elasticsearch、还有很多其他的一些东西，rabbitmq、zookeeper，等等，诸如此类的一些东西
<p>国外的spring开源社区，就发起了一个项目，spring boot，我们基于spring boot直接进行开发，里面还是使用spring + spring mvc + mybatis一些框架，我们可以一定程度上来简化我们之前的开发流程
<img src="/src/main/resources/img-demo/advanced/springboot.jpg">

<h4>能画一张图说说Spring的核心架构吗？</h4>

<p>spring bean生命周期，从创建 -> 使用 -> 销毁

<p>你在系统里用xml或者注解，定义一大堆的bean

<li>（1）实例化Bean：如果要使用一个bean的话
<li>（2）设置对象属性（依赖注入）：他需要去看看，你的这个bean依赖了谁，把你依赖的bean也创建出来，给你进行一个注入，比如说通过构造函数，setter
<li>（3）处理Aware接口：如果这个Bean已经实现了ApplicationContextAware接口，spring容器就会调用我们的bean的setApplicationContext(ApplicationContext)方法，传入Spring上下文，把spring容器给传递给这个bean
<li>（4）BeanPostProcessor：如果我们想在bean实例构建好了之后，此时在这个时间带你，我们想要对Bean进行一些自定义的处理，那么可以让Bean实现了BeanPostProcessor接口，那将会调用postProcessBeforeInitialization(Object obj, String s)方法。
<li>（5）InitializingBean 与 init-method：如果Bean在Spring配置文件中配置了 init-method 属性，则会自动调用其配置的初始化方法。
<li>（6）如果这个Bean实现了BeanPostProcessor接口，将会调用postProcessAfterInitialization(Object obj, String s)方法
<li>（7）DisposableBean：当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法；
<li>（8）destroy-method：
<p>最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。


<h4>Spring中都使用了哪些设计模式</h4>
<p>工厂模式，单例模式，代理模式
<p>工厂模式，spring ioc核心的设计模式的思想提现，他自己就是一个大的工厂，把所有的bean实例都给放在了spring容器里（大工厂），如果你要使用bean，就找spring容器就可以了，你自己不用创建对象了
<p>如果说你要对一些类的方法切入一些增强的代码，会创建一些动态代理的对象，让你对那些目标对象的访问，先经过动态代理对象，动态代理对象先做一些增强的代码，调用你的目标对象
<p>spring默认来说，对每个bean走的都是一个单例模式，确保说你的一个类在系统运行期间只有一个实例对象，只有一个bean，用到了一个单例模式的思想，保证了每个bean都是单例的
<p>在设计模式里，就是一个代理模式的体现和运用，让动态代理的对象，去代理了你的目标对象，在这个过程中做一些增强的访问，你可以把面试突击的内容作为一个抛砖引玉的作用，去更加深入的学习一些技术
<p>模版模式，在各种BeanFactory以及ApplicationContext实现中也都用到了
<p>策略模式，加载资源文件的方式，使用了不同的方法，比如：ClassPathResourece，FileSystemResource，ServletContextResource，UrlResource但他们都有共同的借口Resource；在Aop的实现中，采用了两种不同的方式，JDK动态代理和CGLIB代理

<h4> Spring Web MVC的核心架构</h4>
<li>（1）tomcat的工作线程将请求转交给spring mvc框架的DispatcherServlet
<li>（2）DispatcherServlet查找@Controller注解的controller，我们一般会给controller加上你@RequestMapping的注解，标注说哪些controller用来处理哪些请求，此时根据请求的uri，去定位到哪个controller来进行处理
<li>（3）根据@RequestMapping去查找，使用这个controller内的哪个方法来进行请求的处理，对每个方法一般也会加@RequestMapping的注解
<li>（4）他会直接调用我们的controller里面的某个方法来进行请求的处理
<li>（5）我们的controller的方法会有一个返回值，以前的时候，一般来说还是走jsp、模板技术，我们会把前端页面放在后端的工程里面，返回一个页面模板的名字，spring mvc的框架使用模板技术，对html页面做一个渲染；返回一个json串，前后端分离，可能前端发送一个请求过来，我们只要返回json数据
<li>（6）再把渲染以后的html页面返回给浏览器去进行显示；前端负责把html页面渲染给浏览器就可以了



<h4>说说Spring Cloud的核心架构吗？</h4>
<img  src="/src/main/resources/img-demo/advanced/springbootcloud.jpg">
<p>eureka、ribbon、feign、zuul、hystrix、链路追踪、其他组件，服务于分布式系统的，hystrix主要用于服务之间调用的熔断、隔离、降级

<p> 细节后面加

<h3> 4 JVM 初级 </h3>
<p>jvm笔记有详细  但不是很详细
<h4>JVM中有哪几块内存区域？Java 8之后对内存分代做了什么改进？</h4>
<p>执行我们的一些对象的方法，执行代码的时候肯定会有很多的线程，tomcat里就有很多自己的工作线程，去执行我们写的代码，每个工作线程都会有自己的一块数据结构，栈内存，这个里面是存放一些东西
<p>java 8以后的内存分代的改进，永久代里放了一些常量池+类信息，常量池 -> 堆里面，类信息 -> metaspace（元区域）

<h4>JVM是如何运行起来的？我们的对象是如何分配的？</h4>

<p>比如说我们有一个类里面包含了一个main方法，你去执行这个main方法，此时会自动一个jvm进程，他会默认就会有一个main线程，这个main线程就负责执行这个main方法的代码，进而创建各种对象
<p>tomcat，类都会加载到jvm里去，spring容器而言都会对我们的类进行实例化成bean，有工作线程会来执行我们的bean实例对象里的方法和代码，进而也会创建其他的各种对象，实现业务逻辑

<h4>JVM在哪些情况下会触发垃圾回收</h4>
<p>jvm的内存其实是有限制的，不可能是无限的，昂贵的资源，2核4G的机器，堆内存也就2GB左右，4核8G的机器，堆内存可能也就4G左右，栈内存也需要空间，metaspace区域放类信息也需要空间
<p>在jvm里必然是有一个内存分代模型，年轻代和老年代
<p>比如说给年轻代一共是2GB内存，给老年代是2GB内存，默认情况下eden和2个s的比例：8:1:1，eden是1.6GB，S是0.2GB
<p>如果说eden区域满了，此时必然触发垃圾回收，young gc，ygc，谁是可以回收的垃圾对象呢？就是没有人引用的对象就是垃圾对象

<h4> JVM的年轻代垃圾回收算法？对象什么时候转移到老年代？</h4>
<p>如果说你让代码一边运行，一边有变动，一边判断哪些对象是可以回收的，这个是不现实的，垃圾回收的时候有一个概念，叫做stop the world，停止你的jvm里的工作线程的运行，然后扫描所有的对象，判断哪些可以回收，哪些不可以回收的
<p>年轻代，大部分情况下，对象生存周期是很短的，可能在0.01ms之内，线程执行了3个方法，创建了几个对象，0.01ms之后就方法都执行结束了，此时那几个对象就会在0.01ms之内变成垃圾，可以回收的
<p>复制算法，一次young gc，年轻代的垃圾回收
<p>三种场景，
<li> 1   在新生代 Survivor被回收次数默认为15，会进入老年代
<li> 2  大对象会直接进入老年代
<li> 3  当Survivor区相当年龄的对象 大小超过Survivor的一般  年龄大于或等于该年龄的对象进去老年代

<h4>老年代的垃圾回收算法？常用的垃圾回收器都有什么？</h4>
<p>GC 算法原理（垃圾收集算法）
<li>基础：标记 - 清除算法
<li>解决效率问题：复制算法
<li>解决空间碎片问题：标记 - 整理算法
<li>进化：分代收集算法
<p>GC 算法的真正实现：7 个垃圾收集器
<li>新生代：Serial、ParNew、Parallel Scavenge
<li>老年代：Serial Old、Parallel Old、CMS
<li>全能：G1

<p>老年代对象越来越多，是不是会发现说，老年代的内存空间也会满的，可以不可以使用类似年轻代的复制算法，不合适的，因为老年代里的对象，很多都是被长期引用的，spring容器管理的各种bean
<p>对老年代而言，他里面垃圾对象可能是没有那么多的，标记-清理，找出来那些垃圾对象，然后直接把垃圾对象在老年代里清理掉，标记-整理，把老年代里的存活对象标记出来，移动到一起，存活对象压缩到一片内存空间里去
<p>剩余的空间都是垃圾对象整个给清理掉，剩余的都是连续的可用的内存空间，解决了内存碎片的一个问题
<p>parnew+cms的组合，g1直接分代回收，新版本，慢慢的就是主推g1垃圾回收器了，以后会淘汰掉parnew+cms的组合，jdk 8~jdk 9比较居多一些，parnew+cms的组合比较多一些，是这么一个情况
<p>分成好几个阶段，初始标记，并发标记，并发清理，等等，老年代垃圾回收是比较慢的，一般起码比年轻代垃圾回收慢个10倍以上，cms的垃圾回收算法，刚开始用标记-清理，标记出来垃圾对象，清理掉一些垃圾对象，整理，把一些存活的对象压缩到一起，避免内存碎片的产生
<p>执行一个比较慢的垃圾回收，还要stop the world，需要100mb，此时就会让系统停顿100ms，不能处理任何请求，尽可能的让垃圾回收和工作线程的运行，并发着来执行

<h4>生产环境中的Tomat是如何设置JVM参数的？如何检查JVM运行情况？</h4>
<p>你确实必须得去看一下你当前生产系统的jvm参数都是如何设置的，如果说你是tomcat部署的java web系统，jvm进程对应的tomcat自己，你的系统仅仅是在tomcat的jvm进程来执行
<p>tomcat的一个配置脚本，catalina脚本里去找一下，jvm专栏都有说明的，里面是有对应的tomcat启动的一些jvm参数的设置
<p>比如通过java命令直接启动你的一个main方法跑起来的系统，就是你自己启动的时候，java命令可以带上一些jvm参数
<p>对你自己系统的jvm参数有一个了解，内存区域大小的分配，每个线程的栈大小，metaspace大小，堆内存的大小，年轻代和老年代分别的大小，eden和survivor区域的大小分别是多少，如果没有设置，会有一些默认值
<p>jvm专栏里，在中间有一些地方，他是讲了一些命令的，可以查看jvm的启动默认参数
<p>垃圾回收器，年轻代是用了什么，老年代，每种垃圾回收器是否有对应的一些特殊的参数有设置，那些特殊的参数分别都是用来干什么的
<p>为什么要这么设置呢？当前线上系统运行的时候，jvm的表现如何？
<p>预估完毕之后，根据预估的情况，可以去设置一些jvm参数
<p>进行压测，在压测的时候，其实就需要去观察jvm运行的情况，jstat工具去分析jvm运行的情况，他的年轻代里的eden区域的对象增长的情况，ygc的频率，每次ygc过后有多少对象存活，s能否放的下，老年代对象增长速率，老年代多久会触发一次fgc
<p>就可以根据压测的情况去进行一定的jvm参数的调优，一个系统的QPS，一个是系统的接口的性能，压测到一定程度的时候 ，机器的cpu、内存、io、磁盘的一些负载情况，jvm的表现
<p>可能需要对一些代码进行优化，比如优化性能，或者减轻一点cpu负担，减轻io和磁盘负担，发现jvm的gc过于频繁，内存泄漏，此时就需要对jvm的各个内存区域的大小以及一些参数进行调优
<p>跑到线上实际生产环境里去，运行的过程中，也需要基于一些监控工具，或者是jstat，除了观察系统的QPS和性能，接口可用性，调用成功率，机器的负载，jvm的表现，gc的频率，gc耗时，内存的消耗


<h4>你在实际项目中是否做过JVM GC优化，怎么做的？</h4>
<p> 主要还是上面那个

<h4> 发生OOM之后，应该如何排查和处理线上系统的OOM问题？</h4>
<p>找他自动导出的内存快照，分析，XX对象，直接去定位代码，修改代码
<p>你一定要把案例的业务、背景和思想给吸收了，就得融入到自己的业务里去，我负责的业务系统，在什么样的情况下，可能说会出现一大批的对象卡在内存里，无法回收，导致我系统没法放更多的对象了
<p>产生OOM，内存泄漏的问题，少数场景在互联网公司，超高并发下的oom问题，瞬时大量存活对象占据内存， 导致没法创建更多的对象了
<p>你也得去思考，甚至去模拟一下，最好可以模拟出来，oom不是你自己的代码，可能是你依赖的第三方的组件，netty导致的，结合自己的项目去一步一步的分析，oom问题的产生，和解决的过程

<h3>5 网络协议</h3>
<h4> 你能聊聊TCP/IP四层网络模型吗？OSI七层网络模型也说一下！</h4>
<p>设想一下，各个电脑厂商，比如IBM、苹果啥的，都弄自己的协议，结果就苹果电脑和苹果电脑自己可以通信，和IBM电脑就不可以通信，这不是尴尬么。所以搞一个国际通行的协议，大家都按照这个来，所有电脑都可以通信，不是很好么。
<p> 
<p>此时就必须搞一个标准的网络模型出来，大家都按照这个来走，大家都要遵守统一的规范。这就是所谓OSI七层模型，他们分别是：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。那么在这个基础上，又简化出了TCP/IP四层模型，数据链路层、网络层、传输层、应用层
<a href="https://mp.weixin.qq.com/s/MV1-UeiOzwKm_xWlNSFUvA">详细博客</a>
<img  src="/src/main/resources/img-demo/advanced/osi.jpg">
<p> 博客总结
<li>1. 物理层,网线,光纤等，物理层负责传输0和1的电路信号。，计算机的最最底层，就是0/1，电信号
<li>2. 数据链路层,以太网协议，以太网。一组电信号是一个数据包，叫一个帧（frame），每个帧分成两个部分，标头（head）和数据（data）,
<p>一个局域网内的每台机器都有自己的ARP cache，这个ARP就是用来在一个局域网内让各个设备都知道每个设备的ip地址和mac地址的对应关系的
<li>3. 网络层  太网协议通过子网广播,mac地址比较的方式找到对应机器 ,ip+子网掩码 的&运算,判断是不是一个子网  .路由器配置了两块网卡，每个网卡可以连到一个局域网内
<p>ip地址 -> mac地址 -> 交换机 -> 路由器 -> ip地址 -> mac地址 -> 交换机的方式来通过路由器进行通信。
<li>4）传输层  多程序通过端口完成多程序数据传输,udp和tcp都是传输层的协议，作用就是在数据包里加入端口号，可以通过端口号进行点对点的通信了。udp协议是不可靠的，发出去人家收到没有就不知道了；tcp协议是可靠的，要求三次握手，而且要求人家接收到数据必须回复你。
<li>5）应用层 通过传输层的tcp协议可以传输数据 ,这个应用层，我们就假设综合了会话层、表示层和应用层了，3层合成1层。负责解析和处理
<p>DNS服务器，DNS服务器告诉你www.baidu.com对应的ip地址的。

<h4>浏览器请求www.baidu.com的全过程大概是怎么样的</h4>
<li>1 这个时候找DNS服务器，DNS服务器解析域名之后，返回一个ip地址
<li>2 拿到网关ip地址的mac地址的，现在我们从应用层出发，通过浏览器访问一个网站，是走应用层的http协议的，把浏览器发出的请求打包成数据包
<p>比如常见的可以放一个json这就构成了一个http请求报文浏览器请求一个地址，先按照应用层的http协议，封装一个应用层数据包，数据包里就放了http请求报文，这个时候会将这个http请求报文打包成一个数据包，仅仅只是数据包的数据部分，此时是数据包是没有头的。上面根据http协议搞一个http请求报文，然后搞一个数据包出来，就是网络模型中到的应用层干的事儿了。
<li>3 接着就是跑传输层来了，这个层是tcp协议，这个tcp协议会让你设置端口，发送方的端口随机选一个，接收方的端口一般是默认的80端口。
<p>这个时候，会把应用层数据包给封装到tcp数据包中去，而且会加一个tcp头，这个tcp数据包是对应一个tcp头的，这个tcp头里就放了端口号信息
<li>4 接着跑到网络层来了，走ip协议，这个时候会把tcp头和tcp数据包，放到ip数据包里去，然后再搞一个ip头，ip头里本机和目标机器的ip地址。
 <pre>
 这里本机ip地址是192.168.31.37，
 目标机器是172.194.26.108。</pre>
<p>因为，通过ip协议，可以判断说，两个ip地址不是在一个子网内的，所以此时只能将数据包先通过以太网协议广播到网关上去，通过网关再给他发送出去

<li>5 接着是数据链路层，这块走以太网协议，这里是把ip头和ip数据包封到以太网数据包里去，然后再加一个以太网数据包的头，头里放了本机网卡mac地址，和网关的mac地址。但是以太网数据包的限制是1500个字节，但是假设这个时候ip数据包都5000个字节了，那么需要将ip数据包切割一下。
<p>这个时候一个以太网数据包要切割为4个数据包，每个数据包包含了以太网头、ip头和切割后的ip数据包，4个数据包的大小分别是1500，1500,1500，560。ip头里包含了每个数据包的序号。

<li>6 百度服务器接收到4个以太网数据包以后，根据ip头的序号，把4个以太网数据包里的ip数据包给拼起来，就还原成一个完整的ip数据包了。接着就从ip数据包里面拿出来tcp数据包，再从tcp数据包里取出来http数据包，读取出来http数据包里的各种协议内容，接着就是做一些处理，然后再把响应结果封装成htp响应报文，封装在http数据包里，再一样的过程，封装tcp数据包，封装ip数据包，封装以太网数据包，接着通过网关给发回去

<h4>TCP三次握手流程图？为啥是三次而不是二次或者四次呢？</h4>
<p>tcp三次握手过程
<p>通过传输层的tcp协议建立网络连接的时候，其实走的是三次握手的过程
<li>建立三次握手的时候，TCP报头用到了下面几个东西，ACK、SYN、FIN。
<li>第一次握手，客户端发送连接请求报文，此时SYN=1、ACK=0，这就是说这是个连接请求，seq = x，接着客户端处于SYN_SENT状态，等待服务器响应。
<li>第二次握手，服务端收到SYN=1的请求报文，需要返回一个确认报文，ack = x + 1，SYN=1，ACK = 1，seq = y，发送给客户端，自己处于SYN_RECV状态。
<li>第三次握手，客户端收到了报文，将ack = y + 1，ACK = 1，seq = x + 1
<p>其实三次握手说白了，就是来回来去三次请求，每次请求带上一堆TCP报文头，根据报文头是否正确，就是越好的协议来建立连接。简单说就是这样。

 <p>（2）为啥不是2次或者4次握手呢？
<p>假设两次握手就ok了，要是客户端第一次握手过去，结果卡在某个地方了，没到服务端；完了客户端再次重试发送了第一次握手过去，服务端收到了，ok了，大家来回来去，三次握手建立了连接。
<p>结果，尴尬的是，后来那个卡在哪儿的老的第一次握手发到了服务器，服务器直接就返回一个第二次握手，这个时候服务器开辟了资源准备客户端发送数据啥的，结果呢？客户端根本就不会理睬这个发回去的二次握手，因为之前都通信过了。
<p>但是如果是三次握手，那个二次握手发回去，客户端发现根本不对，就会发送个复位的报文过去，让服务器撤销开辟的资源，别等着了。
<p>因为3次握手就够了，不需要4次或者5次浪费资源了

<h5>（3）tcp断开连接的4次挥手</h5>
<li>第一次挥手，客户端发送报文，FIN=1，seq=u，此时进入FIN-WAIT-1状态
<li>第二次挥手，服务端收到报文，这时候进入CLOSE_WATI状态，返回一个报文，ACK=1，ack=u+1，seq=v。客户端收到这个报文之后，直接进入FIN-WAIT-2状态，此时客户端到服务端的连接就释放了。
<li>第三次挥手，服务端发送连接释放报文，FIN=1，ack=u+1，seq=w，服务端进入LAST-ACK状态
<li>第四次挥手，客户端收到连接释放报文之后，发应答报文，ACK=1，ack=w+1，seq=u+1，进入TIME_WAIT状态，等待一会儿客户端进入CLOSED状态，服务端收到报文之后就进入CLOSED状态。

<h4>HTTP协议的工作原理</h4>
<p>http请求封装到应用层数据包，封装在tcp数据包，封装在ip数据包，封装在以太网数据包，如果过大，可能会拆成几个包，走以太网协议+交换机 -> 广播 -> 网关 -> 多个网关 -> 目标的机器 -> 一层一层拆包 -> http请求报文 -> 传递给tomcat -> spring mvc -> http响应 -> 一样的路径会去
<li>http 1.0要指定keep-alive来开启持久连接，默认是短连接，就是浏览器每次请求都要重新建立一次tcp连接，完事儿了就释放tcp连接。早期的网页都很low，没啥东西，就一点文字，就用这个没问题。但是现在，一个网页打开之后，还要加载大量的图片、css、js，这就坑爹了，发送多次请求
<li>http 1.1默认支持长连接，就是说，浏览器打开一个网页之后，底层的tcp连接就保持着，不会立马断开，之后加载css、js之类的请求，都会基于这个tcp连接来走。http 1.1还支持host头，也就可以支持虚拟主机；而且对断点续传有支持。
<p>浏览器，第一次请求去一个网站的一个页面的时候，就会打开一个tcp连接，接着就在一段时间内都不关闭了，然后接下来这个网页加载css、js、图片大量的请求全部走同一个tcp连接，频繁的发送请求获取响应，最后过了一段时间，这些事儿都完了，然后才会去释放那一个tcp连接。大幅度的提升复杂网页的打开的速度，性能。
<li>http 2.0，支持多路复用，基于一个tcp连接并行发送多个请求以及接收响应，解决了http 1.1对同一时间同一个域名的请求有限制的问题。二进制分帧，将传输数据拆分为更小的帧（数据包），frame（数据包，帧），提高了性能，实现低延迟高吞吐

<h4>HTTPS的工作原理？为啥用HTTPS就可以加密通信？</h4>
<p>https的工作原理大概是这样的：
<li>（1）浏览器把自己支持的加密规则发送给网站
<li>（2）网站从这套加密规则里选出来一套加密算法和hash算法，然后把自己的身份信息用证书的方式发回给浏览器，证书里有网站地址、加密公钥、证书颁发机构
<li>（3）浏览器验证证书的合法性，然后浏览器地址栏上会出现一把小锁；浏览器接着生成一串随机数密码，然后用证书里的公钥进行加密，这块走的非对称加密；用约定好的hash算法生成握手消息的hash值，然后用密码对消息进行加密，然后把所有东西都发给网站，这块走的是对称加密
<li>（4）网站，从消息里面可以取出来公钥加密后的随机密码，用本地的私钥对消息解密取出来密码，然后用密码解密浏览器发来的握手消息，计算消息的hash值，并验证与浏览器发送过来的hash值是否一致，最后用密码加密一段握手消息，发给浏览器
<li>（5）浏览器解密握手消息，然后计算消息的hash值，如果跟网站发来的hash一样，握手就结束，之后所有的数据都会由之前浏览器生成的随机密码，然后用对称加密来进行进行加密。

<h4>http的长连接的工作原理到底是啥？</h4>
<p>http本身没什么所谓的长连接短连接之说，其实说白了都是http下层的tcp连接是长连接还是短连接，tcp连接保持长连接，那么多个http请求和响应都可以通过一个链接来走。其实http 1.1之后，默认都是走长连接了，就是底层都是一个网页一个tcp连接，一个网页的所有图片、css、js的资源加载，都走底层一个tcp连接，来多次http请求即可。
<p>http 1.0的时候，底层的tcp是短连接，一个网页发起的请求，每个请求都是先tcp三次握手，然后发送请求，获取响应，然后tcp四次挥手断开连接；每个请求，都会先连接再断开。短连接，建立连接之后，发送个请求，直接连接就给断开了
<p>http 1.1，tcp长连接，tcp三次握手，建立了连接，无论有多少次请求都是走一个tcp连接的，走了n多次请求之后，然后tcp连接被释放掉了

<h3> 6 MySQL</h3>
<h4>MySQL、MyISAM和InnoDB存储引擎的区别是啥？ </h4>
<li>（1）myisam
<p>myisam，不支持事务，不支持外键约束，索引文件和数据文件分开，这样在内存里可以缓存更多的索引，对查询的性能会更好，适用于那种少量的插入，大量查询的场景。
<p>比如说最经典的就是报表系统，比如大数据的报表系统，给大家画个图聊聊一半都是怎么玩儿的，常见的就是走hadoop生态来搞，hdfs来存储数据，然后基于hive来进行数仓建模，每次hive跑出来的数据都用sqoop从hive中导出到mysql中去。然后基于mysql的在线查询，就接上j2ee写个简单的web系统，每个报表开发一套代码，写sql查数据，组织数据，按照前端要求的格式返回数据，展现出来一个报表。

<li>（2）innodb

<p>mysql 5.5之后的默认存储引擎。
<p>主要特点就是支持事务，走聚簇索引，强制要求有主键，支持外键约束，高并发、大数据量、高可用等相关成熟的数据库架构，分库分表、读写分离、主备切换，全部都可以基于innodb存储引擎来玩儿，如果真聊到这儿，其实大家就可以带一带，说你们用innodb存储引擎怎么玩儿分库分表支撑大数据量、高并发的，怎么用读写分离支撑高可用和高并发读的。

<h4>MySQL的索引实现原理 </h4>
<h5> b-树</h5>
<img  src="https://mmbiz.qpic.cn/mmbiz_png/1J6IbIcPCLax49ouyduhuZLJqXQ3OxsFrfNfx8dzOxGzic4EkLd7QvseOiaXcbT3WLUSm9k3qcslmNHadNlgLgFw/640">

<h5>b+树</h5>
<img src="https://mmbiz.qpic.cn/mmbiz_png/1J6IbIcPCLax49ouyduhuZLJqXQ3OxsFJpibGcWnX1teXpwWoxicOZJtYqicVibeCKV3fVdy2yroibNP3QvgcCxjx1Q/640">

<p>b+树跟b-树不太一样的地方在于：
<p>每个节点的指针上限为2d而不是2d+1。
<p>内节点不存储data，只存储key；叶子节点不存储指针。

<p>一般数据库的索引都对b+树进行了优化，加了顺序访问的指针，如网上弄的一个图，这样在查找范围的时候，就很方便，比如查找18~49之间的数据
<img src="https://mmbiz.qpic.cn/mmbiz_png/1J6IbIcPCLax49ouyduhuZLJqXQ3OxsFknVKMq7DuzkvoJ3FzAqyKj6JEd2vd8dGPod5pj0pAqq7nXYPj07baA/640">

<h5> myism存储引擎的索引实现</h5>
<p>myisam存储的索引实现，在myisam存储引擎的索引中，每个叶子节点的data存放的是数据行的物理地址，比如0x07之类的东西，然后我们可以画一个数据表出来，一行一行的，每行对应一个物理地址。

<h5>innodb存储引擎的索引</h5>
innodb的数据文件本身就是个索引文件，就是主键key，然后叶子节点的data就是那个数据的所在行。
<img  src="https://mmbiz.qpic.cn/mmbiz_png/1J6IbIcPCLax49ouyduhuZLJqXQ3OxsFknVKMq7DuzkvoJ3FzAqyKj6JEd2vd8dGPod5pj0pAqq7nXYPj07baA/640">
<p>innodb存储引擎，要求必须有主键，会根据主键建立一个默认索引，叫做聚簇索引，innodb的数据文件本身同时也是个索引文件，索引存储结构大致如下：
<p>15，data：0x07，完整的一行数据，（15,张三,22）
<p>22，data：完整的一行数据，（22,李四,30）

<h5> 自建索引</h5>
<p>innodb存储引擎下，如果对某个非主键的字段创建个索引，那么最后那个叶子节点的值就是主键的值，因为可以用主键的值到聚簇索引里根据主键值再次查找到数据，即所谓的回表，例如：
<pre>select * from table where name = ‘张三’</pre>
<p>先到name的索引里去找，找到张三对应的叶子节点，叶子节点的data就是那一行的主键，id=15，然后再根据id=15，到数据文件里面的聚簇索引（根据主键组织的索引）根据id=15去定位出来id=15这一行的完整的数据

<h5> 索引值</h5>
<p>为啥innodb下不要用UUID生成的超长字符串作为主键？因为这么玩儿会导致所有的索引的data都是那个主键值，最终导致索引会变得过大，浪费很多磁盘空间。
<p>还有一个道理，一般innodb表里，建议统一用auto_increment自增值作为主键值，因为这样可以保持聚簇索引直接加记录就可以，如果用那种不是单调递增的主键值，可能会导致b+树分裂后重新组织，会浪费时间

<h5>索引的使用规则</h5>
<p>最左前缀匹配原则，这个东西是跟联合索引（复合索引）相关联的，就是说，你很多时候不是对一个一个的字段分别搞一个一个的索引，而是针对几个索引建立一个联合索引的。
<p>创建一个联合索引：shop_id、product_id、gmt_create
<p>一般来说，你有一个表（product）：shop_id、product_id、gmt_create，你的SQL语句要根据这3个字段来查询，所以你一般来说不是就建立3个索引，一般来说会针对平时要查询的几个字段，建立一个联合索引
<p>后面在java系统里写的SQL，都必须符合最左前缀匹配原则，确保你所有的sql都可以使用上这个联合索引，通过索引来查询
<li>（1）全列匹配
<p>这个就是说，你的一个sql里，正好where条件里就用了这3个字段，那么就一定可以用到这个联合索引的：
<p>select * from product where shop_id=1 and product_id=1 and gmt_create=’2018-01-01 10:00:00’
<li>（2）最左前缀匹配
<p>这个就是说，如果你的sql里，正好就用到了联合索引最左边的一个或者几个列表，那么也可以用上这个索引，在索引里查找的时候就用最左边的几个列就行了：
<p>select * from product where shop_id=1 and product_id=1，这个是没问题的，可以用上这个索引的
<li>（3）最左前缀匹配了，但是中间某个值没匹配
<p>这个是说，如果你的sql里，就用了联合索引的第一个列和第三个列，那么会按照第一个列值在索引里找，找完以后对结果集扫描一遍根据第三个列来过滤，第三个列是不走索引去搜索的，就是有一个额外的过滤的工作，但是还能用到索引，所以也还好，例如：
<p>select * from product where shop_id=1 and gmt_create=’2018-01-01 10:00:00’
<p>就是先根据shop_id=1在索引里找，找到比如100行记录，然后对这100行记录再次扫描一遍，过滤出来gmt_create=’2018-01-01 10:00:00’的行
<p>这个我们在线上系统经常遇到这种情况，就是根据联合索引的前一两个列按索引查，然后后面跟一堆复杂的条件，还有函数啥的，但是只要对索引查找结果过滤就好了，根据线上实践，单表几百万数据量的时候，性能也还不错的，简单SQL也就几ms，复杂SQL也就几百ms。可以接受的。
<li>（4）没有最左前缀匹配
<p>那就不行了，那就在搞笑了，一定不会用索引，所以这个错误千万别犯
<p>select * from product where product_id=1，这个肯定不行
<li>（5）前缀匹配
<p>这个就是说，如果你不是等值的，比如=，>=，<=的操作，而是like操作，那么必须要是like ‘XX%’这种才可以用上索引，比如说
<p>select * from product where shop_id=1 and product_id=1 and gmt_create like ‘2018%’
<li>（6）范围列匹配
<p>如果你是范围查询，比如>=，<=，between操作，你只能是符合最左前缀的规则才可以范围，范围之后的列就不用索引了
<p>select * from product where shop_id>=1 and product_id=1
<p>这里就在联合索引中根据shop_id来查询了
<li>（7）包含函数
<p>如果你对某个列用了函数，比如substring之类的东西，那么那一列不用索引
<p>select * from product where shop_id=1 and 函数(product_id) = 2
<p>上面就根据shop_id在联合索引中查询

<h5>索引的缺点以及使用注意</h5>
<p>索引是有缺点的，比如常见的就是会增加磁盘消耗，因为要占用磁盘文件，同时高并发的时候频繁插入和修改索引，会导致性能损耗的。
<p>我们给的建议，尽量创建少的索引，比如说一个表一两个索引，两三个索引还可以接受,高并发场景下还可以。十来个，20个索引 就不太好
<p>索引字段是 sex,status 这些字段,建立索引是没有意义的
<p>还有一种特殊的索引叫做前缀索引，就是说，某个字段是字符串，很长，如果你要建立索引，最好就对这个字符串的前缀来创建，比如前10个字符这样子，要用前多少位的字符串创建前缀索引，就对不同长度的前缀看看选择性就好了，一般前缀长度越长选择性的值越高。

<h4>事务的几个特性是啥？有哪几种隔离级别？</h4>
<h5>事务的ACID</h5>
<li>（1）Atomic：原子性，就是一堆SQL，要么一起成功，要么都别执行，不允许某个SQL成功了，某个SQL失败了，这就是扯淡，不是原子性。
<li>（2）Consistency：一致性，这个是针对数据一致性来说的，就是一组SQL执行之前，数据必须是准确的，执行之后，数据也必须是准确的。别搞了半天，执行完了SQL，结果SQL对应的数据修改没给你执行，那不是坑爹么。
<li>（3）Isolation：隔离性，这个就是说多个事务在跑的时候不能互相干扰，别事务A操作个数据，弄到一半儿还没弄好呢，结果事务B来改了这个数据，导致事务A的操作出错了，那不就搞笑了。
<li>（4）Durability：持久性，事务成功了，就必须永久对数据的修改是有效的，别过了一会儿数据自己没了

<h5> 事务隔离级别 </h5>
<li>（1）读未提交，Read Uncommitted：这个很坑爹，就是说某个事务还没提交的时候，修改的数据，就让别的事务给读到了，这就恶心了，很容易导致出错的。这个也叫做脏读。
<li>（2）读已提交，Read Committed（不可重复读）：这个比上面那个稍微好一点，但是一样有问题

<p>就是说事务A在跑的时候， 先查询了一个数据是值1，然后过了段时间，事务B把那个数据给修改了一下还提交了，此时事务A再次查询这个数据就成了值2了，这是读了人家事务提交的数据啊，所以是读已提交。
<p>这个也叫做不可重复读
<li>（3）可重复读，Read Repeatable：这个比上面那个再好点儿，就是说事务A在执行过程中，对某个数据的值，无论读多少次都是值1；哪怕这个过程中事务B修改了数据的值还提交了，但是事务A读到的还是自己事务开始时这个数据的值
<li>（4）幻读：不可重复读和可重复读都是针对两个事务同时对某条数据在修改，但是幻读针对的是插入

<h5>MVCC</h5>
<p>MVCC机制来实现的，就是多版本并发控制，multi-version concurrency control。
<p>当我们使用innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。
<p>事务id，在mysql内部是全局唯一递增的，事务id=1，事务id=2，事务id=3

<p> mvcc总结下:
<li>对于可重复读，查询只承认在事务启动前就已经提交完成的数据；
<li>对于读已提交，查询只承认在语句启动前就已经提交完成的数据；
<p> 在可重复读中   读
<li>版本未提交，不可见；
<li>版本已提交，但是是在视图创建后提交的，不可见；
<li>版本已提交，而且是在视图创建前提交的，可见。
<p> 在可重复读中   update (绕点,因为在读的基础上加了当前读)
<p>可以理解为先执行 DELETE 后执行 INSERT。 现在快照里先在原快照的删除id标记,然后加一条当前创建id的快照
<p> 当前读 总是读取已经提交完成的最新版本,这是他的规则,当他读最新值的,另一个事物可能没提交 ,没提交的修改操作会上锁,这时的流程就变成了,等待另一个事物锁解放,然后读已提交的最新数据

<h4>MySQL数据库锁的实现原理吗？如果死锁了咋办？</h4>
<li>mysql 锁
<p>MyISAM和MEMORY存储引擎采用的是表级锁（table-level locking）；
<p>InnoDB存储引擎既支持行级锁（ row-level locking），也支持表级锁，但默认情况下是采用行级锁。

<li> 表级锁： 开销小，加锁快；不会出现死锁(因为MyISAM会一次性获得SQL所需的全部锁)；锁定粒度大，发生锁冲突的概率最高,并发度最低。
<li> 行级锁： 开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。
<li> 页锁：开销和加锁速度介于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般
<p>1.主要是针对锁粒度划分的，一般分为：行锁、表锁、库锁

<li>（1）行锁：访问数据库的时候，锁定整个行数据，防止并发错误。
<li>（2）表锁：访问数据库的时候，锁定整个表数据，防止并发错误。

<p>2.行锁 和 表锁 的区别：
<li>表锁： 开销小，加锁快，不会出现死锁；锁定力度大，发生锁冲突概率高，并发度最低
<li>行锁： 开销大，加锁慢，会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高

##### 悲观锁和乐观锁
<li>（1）悲观锁：顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。

<p>传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。

<li>（2）乐观锁： 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。

##### 死锁
<p> 一般是因为锁资源相互持有,找到死锁sql,修改代码逻辑


#### MySQL的SQL调优一般都有哪些手段？你们一般怎么做？
<p>如果某个线上SQL跑的慢，十有八九就是因为那个SQL没有用索引，所以这个时候，第一步就是去看MySQL的执行计划，看看那个SQL有没有用到索引，如果没有，那么就改写一下SQL让他用上索引，或者是额外加个索引。
<p>explain select * from table，就ok了
 <pre>
table | type | possible_keys | key | key_len | ref | rows | Extra
table：哪个表
type：这个很重要，是说类型，all（全表扫描），const（读常量，最多一条记录匹配），eq_ref（走主键，一般就最多一条记录匹配），index（扫描全部索引），range（扫描部分索引）
possible_keys：显示可能使用的索引
key：实际使用的索引
key_len：使用索引的长度
ref：联合索引的哪一列被用了
rows：一共扫描和返回了多少行
extra：using filesort（需要额外进行排序），using temporary（mysql构建了临时表，比如排序的时候），using where（就是对索引扫出来的数据再次根据where来过滤出了结果）</pre>

### 7 网络编程
#### Socket的工作原理？Socket跟TCP IP之间是啥关系？
<p>socket就是在传输层里把tcp/ip协议给封装了一下，我们程序员一般都是面向socket来编程的，比如java原生就支持socket网络编程的。
<p>大体来说这个步骤，就是我们搞一个ServerSocket无限等待别人来连接你，然后某个机器要跟你连接，就在本地创建一个socket去连接你，然后建立连接之后，在服务器上，ServerSocket也会创建出来一个socket的。通过客户端的socket跟服务端的socket进行通信，我给你写数据，你读数据，你给我写数据，我读数据，就这个过程。
<p>当然这个底层，比如建立连接和释放连接，都是基于tcp三次握手和四次挥手的规范来搞的，包括基于tcp协议传输数据，其实就跟我们之前说的一样，都是封装个tcp数据包，里面有tcp报头，整了端口号啥的，然后封装在ip数据包里，最后封在以太网数据包里传递。

#### 进程间是如何通信的？线程间又如何切换呢？
<p> 进程间的通信有很多种方式，比如说：管道（pipe）、命名管道（fifo）、消息队列，共享内存（System V）
<li>（1）管道（pipe）
<p>unix操作系统里面，有一个fork操作，可以创建进程的子进程，或者说是复制一个进程完全一样的子进程，共享代码空间，但是各自有独立的数据空间，不过子进程的数据空间是拷贝父进程的数据空间的。
<p>管道机制要求的是两个进程之间是有血缘关系的，就比如fork出来的父子进程。
<p>linux操作系统里，管道用来缓存要在进程间传输的数据，管道是一个固定大小的缓冲区，是4kb。管道中的数据一旦被读取出来，就不在管道里了。
<p>但是如果管道满了，那么写管道的操作就阻塞了，直到别人读了管道的数据；反之如果管道是空的，那么读操作就阻塞了。就这个意思。管道一边连着一个进程的输出，一边连着一个进程的输入，然后就一个进程写数据，另外一个进程读数据，两个进程都没了，管道也就没了。管道是半双工的，就是数据只能流向一个方向，比如说你架设一个管道，只能一个进程写，另外一个进程读。
<p>linux里面对管道的实现，是用了两个文件，指向了一个VFS（虚拟文件系统）的索引节点inode，然后VFS索引节点指向一个物理页面，接着一个进程通过自己关联的那个文件写数据，另外一个进程通过自己关联的那个文件读数据。

<li>（2）命名管道（fifo）
<p>管道的通信，要求必须是父子关系的进程间通信，就受到了限制，所以可以用命名管理来解决这个问题。
<p>之前的管道，是没有名字的，所以必须是有父子关系的进程才能使用。但是这个命名管道是有名字的。这个命名管道，相当于是一个有名字的文件，是有路径的，所以没有血缘关系的进程多可以通过这个命名管道来通信，名字在文件系统上，数据在内存里。其他的跟管道一样，一个进程写，一个进程读，也是半双工的，数据只能单向流动。
<li>（3）消息队列
<p>linux的消息队列可以认为是个链表结构，linux内核有一个msgque链表，这个链表里每个指针指向一个msgid_ds结构，这个结构就描述了一个消息队列。然后进程之间就通过这个消息队列通信就可以，一样是写入数据和消费数据。消息队列的好处就是对每个消息可以指定类型，消费的时候就消费指定类型的消息就行了，功能更多一些。这种方式其实用的不多的。
<li>（4）共享内存
<p>一块物理内存被映射到两个进程的进程地址空间，所以进程之间互相都可以立即看到对方在共享内存里做出的修改，但是因为是共享内存，所以需要锁来保证同步。这个说对了很复杂，我在这里就不多说了，我觉得如果被人问到这个问题，短期内突击的话，回答到这个程度就行了，就是知道有哪些方式。如果你要深入理解各种机制，那是要好好学习linux的各种东西了。
<li>（5）线程间如何切换
<p>一个进程的多个线程间切换的时候就涉及到了上下文切换，这个东西说复杂了就很复杂，但是简单来说，就是有一个时间片算法，cpu给每个线程一个时间片来执行，时间片结束之后，就保存这个线程的状态，然后切换到下一个线程去执行，这就是所谓多线程并发执行的原理，就是多个线程来回来去切换，每个线程就一个时间片里执行。太复杂的我也不讲了，大家就记住一个线程上下文切换指的是什么就行了。

#### BIO、NIO、AIO分别都是啥？有什么区别？
<p> 几种io模式，以及同步、异步、阻塞和非阻塞几种io的概念

#####  BIO
      
<p> 这个其实就是最传统的网络通信模型，就是BIO，同步阻塞式IO。就是服务端创建一个ServerSocket，然后客户端用一个Socket去连接那个ServerSocket，然后ServerSocket接收到一个Socket的连接请求就创建一个Socket和一个线程去跟那个Socket进行通信。
<p> 然后客户端和服务端的socket，就进行同步阻塞式的通信，客户端socket发送一个请求，服务端socket进行处理后返回响应，响应必须是等处理完后才会返回，在这之前啥事儿也干不了，这可不就是同步么。
<p> 这种方式最大的坑在于，每次一个客户端接入，都是要在服务端创建一个线程来服务这个客户端的，这会导致大量的客户端的时候，服务端的线程数量可能达到几千甚至几万，几十万，这会导致服务器端程序的负载过高，最后崩溃死掉。
<p> 要么你就是搞一个线程池，固定线程数量来处理请求，但是高并发请求的时候，还是可能会导致各种排队和延时，因为没那么多线程来处理

##### NIO

<p>JDK 1.4中引入了NIO，这是一种同步非阻塞的IO，基于Reactor模型。
<p>NIO中有一些概念：
<p>比如Buffer，缓冲区的概念，一般都是将数据写入Buffer中，然后从Buffer中读取数据，有IntBuffer、LongBuffer、CharBuffer等很多种针对基础数据类型的Buffer。
<p>还有Channel，NIO中都是通过Channel来进行数据读写的。
<p>包括Selector，这是多路复用器，selector会不断轮询注册的channel，如果某个channel上发生了读写事件，selector就会将这些channel获取出来，我们通过SelectionKey获取有读写事件的channel，就可以进行IO操作。一个Selector就通过一个线程，就可以轮询成千上万的channel，这就意味着你的服务端可以接入成千上万的客户端。
<p>这块其实相当于就是一个线程处理大量的客户端的请求，通过一个线程轮询大量的channel，每次就获取一批有事件的channel，然后对每个请求启动一个线程处理即可。
<p>这里的核心就是非阻塞，就那个selector一个线程就可以不停轮询channel，所有客户端请求都不会阻塞，直接就会进来，大不了就是等待一下排着队而已。
<p>这里的核心就是因为，一个客户端不是时时刻刻都要发送请求的，没必要死耗着一个线程不放吧，所以NIO的优化思想就是一个请求一个线程。只有某个客户端发送了一个请求的时候，才会启动一个线程来处理。
<p>所以为啥是非阻塞呢？因为无论多少客户端都可以接入服务端，客户端接入并不会耗费一个线程，只会创建一个连接然后注册到selector上去罢了，一个selector线程不断的轮询所有的socket连接，发现有事件了就通知你，然后你就启动一个线程处理一个请求即可，但是这个处理的过程中，你还是要先读取数据，处理，再返回的，这是个同步的过程。
<p>所以NIO是同步非阻塞的。
<p>工作线程，从channel里读数据，是同步的，是工作线程自己去干这个事儿，卡在那儿，专门干读数据的这个活儿，数据没读完，你就卡死在这儿了；然后往channel里写数据，也是你自己去干这个事儿，卡死在这儿了，数据没写完，你就卡在这儿了


 ##### AIO

<p>AIO是基于Proactor模型的，就是异步非阻塞模型。
<p>每个连接发送过来的请求，都会绑定一个buffer，然后通知操作系统去异步完成读，此时你的程序是会去干别的事儿的，等操作系统完成数据读取之后，就会回调你的接口，给你操作系统异步读完的数据。
<p>然后你对这个数据处理一下，接着将结果往回写。
<p>写的时候也是给操作系统一个buffer，让操作系统自己获取数据去完成写操作，写完以后再回来通知你。
<p>工作线程，读取数据的时候，是说，你提供给操作系统一个buffer，空的，然后你就可以干别的事儿了，你就把读数据的事儿，交给操作系统去干，操作系统内核，读数据将数据放入buffer中，完事儿了，来回调你的一个接口，告诉你说，ok，buffer交给你了，这个数据我给你读好了
<p>写数据的时候也是一样的的，把放了数据的buffer交给操作系统的内核去处理，你就可以去干别的事儿了，操作系统完成了数据的写之后，级会来回调你，告诉你说，ok，你交给我的数据，我都给你写回到客户端去了

##### 同步阻塞、同步非阻塞、异步非阻塞
<li>BIO的这个同步阻塞，不是完全针对的网络通信模型去说的，针对的是磁盘文件的IO读写，FileInputStream，BIO，卡在那儿，直到你读写完成了才可以
<li>NIO为啥是同步非阻塞？就是说通过NIO的FileChannel发起个文件IO操作，其实发起之后就返回了，你可以干别的事儿，这就是非阻塞，但是接下来你还得不断的去轮询操作系统，看IO操作完事儿了没有。
<li>AIO为啥是异步非阻塞？就是说通过AIO发起个文件IO操作之后，你立马就返回可以干别的事儿了，接下来你也不用管了，操作系统自己干完了IO之后，告诉你说ok了。同步就是自己还得主动去轮询操作系统，异步就是操作系统反过来通知你。

### 8 线上
#### 8.1 线上服务器CPU 100%了！该怎么排查、定位和解决？
<li>（1）定位耗费cpu的进程
<p>top -c，就可以显示进程列表，然后输入P，按照cpu使用率排序，你会看到类似下面的东西
<pre>
PID           USER       PR   NI    VIRT         RES  SHR          S      %CPU      %MEM    TIME+ COMMAND
43987       root           20     0       28.2g        4.5g 68m S       99.0          24.0          44333.4   java -Xms。。。
</pre>
<p>大概类似上面这样，能看到哪个进程，CPU负载最高，还有启动这个进程的命令，比如一般就是java啥啥的。

<li>（2）定位耗费cpu的线程
<p>top -Hp 43987，就是输入那个进程id就好了，然后输入P，按照cpu使用率排序，你会看到类似下面的东西
<p>大概类似上面那样，你就可以看到这个进程里的哪个线程耗费cpu最高

<li>（3）定位哪段代码导致的cpu过高
<p>printf “%x\n” 16872，把线程pid转换成16进制，比如41e8
<p>jstack 43987 | grep ‘0x41e8’ -C5 --color

#### 8.2 线上机器的一个进程用kill命令杀不死该怎么办？磁盘空间快满了又该怎么处理？

##### 线上进程kill不掉怎么办
<p>ps aux，看看STAT那一栏，如果是Z，那么就是zombie状态的僵尸进程
<p>ps -ef | grep 僵尸进程id，可以找到父进程id
<p>然后先kill掉父进程即可


#### 8.3 服务器存储空间快满了（95%），还有一个小时存储就满了，在不影响服务正常运行的情况下，该如何解决？

<p>df -h，先看看磁盘使用的情况
<p>然后就是到你的系统部署的地方，一般就是tomcat下的日志、spring boot的日志，去看看，如果过多，就删除掉一些日志就行了，自己注意让tomcat或者nginx之类的日志输出，按天切割，这样你还可以写个shell脚本，crontab定时，定期删除7天以前的日志
<p>要是不行，那就：find / -size +100M |xargs ls -lh，找找大于100m的文件，但是如果有大量的小文件，那么这样是不行的
<p>或者是用：du -h >fs_du.log，看看各个目录占用的磁盘空间大小，看看是不是哪个目录有大量的小文件
<p>其实面试官无非就是看看是不是知道常见的命令罢了，如果不是。那那个面试官就得再提示多一些细节，到底要考察你什么。但是简单问一个磁盘占用排查，就是常见这几个命令罢了

### 9 深入volatile和synchronized

#### 9.1 原子性：Java规范规定所有变量写操作都是原子的
<p>java语言规范里面，int i = 0，resource = loadedResoures，flag = true，各种变量的简单的赋值操作，规定都是原子的
<p>包括引用类型的变量的赋值写操作，也是原子的
<p>你赋值的时候，要保证没有人先赋值过，没有人修改过，你才能赋值，AtomicReference的CAS操作来实现了

#### 9.2 32位Java虚拟机中的long和double变量写操作为何不是原子的
<p>原子性这块，特例，32位虚拟机里的long/double类型的变量的简单赋值写操作，不是原子的，long i = 30，double c = 45.0，在32位虚拟机里就不是原子的，因为long和double是64位的
<pre>
0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000
 </pre>

<p>如果多个线程同时并发的执行long i = 30，long是64位的，就会导致有的线程在修改i的高32位，有的线程在修改i的低32位，多线程并发给long类型的变量进行赋值操作，在32位的虚拟机下，是有问题的
<p>就可能会导致多线程给long i = 30赋值之后，导致i的值不是30，可能是-3333344429，乱码一样的数字，就是因为高低32位赋值错了，就导致二进制数字转换为十进制之后是一个很奇怪的数字

#### 9.3 volatile原来还可以保证long和double变量写操作的原子性
<p>volatile对原子性保障的语义，在java里很有限的，几乎可以忽略不计。32位的java虚拟机里面，对long/double变量的赋值写是不原子的，此时如果对变量加上了volatile，就可以保证在32位java虚拟机里面，对long/double变量的赋值写是原子的了
<p>int i = 0，原子性，volatile，java语言规范就规定了，原子性的



#### 9.4 可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器
##### 从硬件的级别来考虑一下可见性的问题 (可以看下深入简出计算器组成里的 总线嗅探机制和 MESI 协议)
<p>每个处理器都有自己的寄存器（register），所以多个处理器各自运行一个线程的时候，可能导致某个变量给放到寄存器里去，接着就
<p>会导致各个线程没法看到其他处理器寄存器里的变量的值修改了
<p>可见性的第一个问题，首先，就有可能在寄存器的级别，导致变量副本的更新，无法让其他处理器看到
<p>然后一个处理器运行的线程对变量的写操作都是针对写缓冲来的（store buffer）并不是直接更新主内存，所以很可能导致一个线程更
<p>新了变量，但是仅仅是在写缓冲区里罢了，没有更新到主内存里去
<p>这个时候，其他处理器的线程是没法读到他的写缓冲区的变量值的，所以此时就是会有可见性的问题，这是第二个可见性发生的场景
<p>然后即使这个时候一个处理器的线程更新了写缓冲区之后，将更新同步到了自己的高速缓存里（cache，或者是主内存），然后还把这
<p>个更新通知给了其他的处理器，但是其他处理器可能就是把这个更新放到无效队列里去，没有更新他的高速缓存
<p>此时其他处理器的线程从高速缓存里读数据的时候，读到的还是过时的旧值

##### 9.5 可见性发生的问题

<p>如果要实现可见性的话，其中一个方法就是通过MESI协议，这个MESI协议实际上有很多种不同的时间，因为他不过就是一个协议罢
<p>了，具体的实现机制要靠具体底层的系统如何实现
<p>根据具体底层硬件的不同，MESI协议的实现是有区别的
<p>比如说MESI协议有一种实现，就是一个处理器将另外一个处理器的高速缓存中的更新后的数据拿到自己的高速缓存中来更新一下，这样大家的缓存不就实现同步了，然后各个处理器的线程看到的数据就一样了
<p>为了实现MESI协议，有两个配套的专业机制要给大家说一下：flush处理器缓存、refresh处理器缓存。
<p>flush处理器缓存，他的意思就是把自己更新的值刷新到高速缓存里去（或者是主内存），因为必须要刷到高速缓存（或者是主内存）
<p>里，才有可能在后续通过一些特殊的机制让其他的处理器从自己的高速缓存（或者是主内存）里读取到更新的值
<p>除了flush以外，他还会发送一个消息到总线（bus），通知其他处理器，某个变量的值被他给修改了
<p>refresh处理器缓存，他的意思就是说，处理器中的线程在读取一个变量的值的时候，如果发现其他处理器的线程更新了变量的值，必须从其他处理器的高速缓存（或者是主内存）里，读取这个最新的值，更新到自己的高速缓存中
<p>所以说，为了保证可见性，在底层是通过MESI协议、flush处理器缓存和refresh处理器缓存，这一整套机制来保障的要记住，flush和refresh，这两个操作，flush是强制刷新数据到高速缓存（主内存），不要仅仅停留在写缓冲器里面；refresh，是从总线嗅探发现某个变量被修改，
<p>必须强制从其他处理器的高速缓存（或者主内存）加载变量的最新值到自己的高速缓存里去内存屏障的使用，在底层硬件级别的原理，其实就是在执行flush和refresh，MESI协议是如何与内存屏障搭配使用的（flush、refresh）
<p>volatile boolean isRunning = true;
<p>isRunning = false; => 写volatile变量，就会通过执行一个内存屏障，在底层会触发flush处理器缓存的操作；while(isRunning) {}，读volatile变量，也会通过执行一个内存屏障，在底层触发refresh操作
<p>之前给大家讲过那个volatile关键字的作用，对一个变量加了volatile修饰之后，对这个变量的写操作，会执行flush处理器缓存，把数据
<p>刷到高速缓存（或者是主内存）中，然后对这个变量的读操作，会执行refresh处理器缓存，从其他处理器的高速缓存（或者是主内存）中，读取最新的值
<p>当然跟我们之前说的有一点点不一样，因为之前说的是写volatile变量的时候，一个是强制刷主内存，一个是过期掉其他处理器的高速缓存中的数据；读volatile变量的时候，会发现高速缓存中的值过期，然后强制从主内存加载最新值
<p>他其实本质都是让一个线程写了volatie变量之后，另外一个变量立马可以读到volatile变量的值，只不过MESI协议的底层具体实现，根据cpu等硬件的不同，有多种不同的实现方式罢了

#### 9.6 有序性：Java程序运行过程中发生指令重排的几个地方

<p>写好的代码在实际执行的时候那个顺序可能在很多环节都会被人给重排序，一旦重排序之后，在多线程并发的场景下，就有可能会出现一些问题
<li>（1）自己写的源代码中的执行顺序：这个是我们自己写的代码，一般来说就是按照我们自己脑子里想的样子来写
<li>（2）编译后的代码的执行顺序：java里有两种编译器，一个是静态编译器（javac），一个是动态编译器（JIT）。javac负责把.java文件中的源代码编译为.class文件中的字节码，这个一般是程序写好之后进行编译的。JIT负责把.class文件中的字节码编译为JVM所在操作系统支持的机器码，一般在程序运行过程中进行编译。
<p>在这个编译的过程中，编译器是很有可能调整代码的执行顺序的，为了提高代码的执行效率，很可能会调整代码的执行顺序，JIT编译器对指令重排的还是挺多的
<li>（3）处理器的执行顺序：哪怕你给处理器一个代码的执行顺序，但是处理器还是可能会重排代码，更换一种执行顺序，JIT编译好的指令的时候，还是可能会调整顺序
<li>（4）内存重排序：有可能你这个处理器在实际执行指令的过程中，在高速缓存和写缓冲器、无效队列等等，硬件层面的组件，也可能会导致你的指令的执行看起来的顺序跟想象的不太一样
<p>在遵守一定的规则的前提下，有好几个层面的代码和指令都可能出现重排序

#### 9.7 JIT编译器对创建对象的指令重排以及double check单例实践

JIT动态编译的时候，有可能会造成一个非常经典的指令重排
<pre>
public class MyObject {
private Resource resource;
    public MyObject() {
       // 从配置文件里加载数据构造Resource对象
        this.resource = loadResource(); 
    }
    public void execute() {
        this.resource.execute();
    }
}
// 线程1:
MyObject myObj = new MyObject(); => 这个是我们自己写的一行代码
// 线程2：
myObj.execute();
// 步骤1：以MyObject类作为原型，
// 给他的对象实例分配一块内存空间，
//objRef就是指向了分配好的内存空间的地址的引用，指针
objRef = allocate(MyObject.class);
// 步骤2：就是针对分配好内存空间的一个对象实例，执行他的构造函数，对这个对象实例进行初始化的操作，执行我们自己写的构造函数里的一些代码，对各个实例变量赋值，初始化的逻辑
invokeConstructor(objRef);
// 步骤3：上两个步骤搞定之后，一个对象实例就搞定了，此时就是把objRef指针指向的内存地址，赋值给我们自己的引用类型的变量，myObj就可以作为一个类似指针的概念指向了MyObject对象实例的内存地址
myObj = objRef;
 </pre>
<p>有可能JIT动态编译为了加速程序的执行速度，因为步骤2是在初始化一个对象实例，这个步骤是有可能很耗时的，比如说你可能会在里面执行一些网络的通信，磁盘文件的读写，都有可能
<p>JIT动态编译，指令重排，为了加速程序的执行性能和效率，可能会重排为，步骤1 -> 步骤3 -> 步骤2
<p>线程1，刚刚执行完了步骤1和步骤3，步骤2还没执行，此时myObj已经不是null了，但是MyObject对象实例内部的resource是null
<p>线程2，直接调用myObj.execute()方法， 此时内部会调用resource.execute()方法，但是此时resource是null，直接导致空指针
<p>double check单例模式里面，就是可能会出现这样的JIT指令重排，如果你不加volatile关键字，会导致一些问题的发生，volatile是避免说步骤1、步骤3、步骤2，必须全部执行完毕了，此时才能试用myObj对象实例

#### 9.8 现代处理器为了提升性能的指令乱序和猜测执行的机制！

#### <b>---------乱序执行和猜测执行  看计算机组成22-25</b>
<p>指令乱序机制
<p>指令不一定说是拿到了一个指令立马可以执行的，比如有的指令是要进行网络通信、磁盘读写，获取锁，很多种，有的指令不是立马就绪可以执行的，为了提升效率，在现代处理器里面都是走的指令的乱序执行机制
<p>把编译好的指令一条一条读取到处理器里，但是哪个指令先就绪可以执行，就先执行，不是按照代码顺序来的。每个指令的结果放到一个重排序处理器中，重排序处理器把各个指令的结果按照代码顺序应用到主内存或者写缓冲器里

#### 9.9 高速缓存和写缓冲器的内存重排序造成的视觉假象

<p>处理器会将数据写入写缓冲器，这个过程是store；从高速缓存里读数据，这个过程是load。写缓冲器和高速缓存执行load和store的过程，都是按照处理器指示的顺序来的，处理器的重排处理器也是按照程序顺序来load和store的
<p>但是有个问题，就是在其他的处理器看到的一个视觉假象而言，有可能会出现看到的load和store是重排序的，也就是内存重排序
<p>处理器的乱序执行和推测执行，都是指令重排序，这次说的是内存重排序，因为都是发生在内存层面的写缓冲器和高速缓存中的
<p>这个内存重排序，有4种可能性：
<li>（1）LoadLoad重排序：一个处理器先执行一个L1读操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再L1
<li>（2）StoreStore重排序：一个处理器先执行一个W1写操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再W1
<li>（3）LoadStore重排序：一个处理器先执行一个L1读操作，再执行一个W2写操作；但是另外一个处理器看到的是先W2再L1
<li>（4）StoreLoad重排序：一个处理器先执行一个W1写操作，再执行一个L2读操作；但是另外一个处理器看到的是先L2再W1
<p>比如说写缓冲器为了提升性能，有可能先后到来W1和W2操作了之后，他先执行了W2操作，再执行了W1操作。那这个时候其他处理器看到的可不就是先W2再W1了，这就是StoreStore重排序

 
   <pre>
共享变量：
Resource resource = null;

Boolean resourceLoaded = false;

处理器0：

resource = loadResoureFromDisk();

resourceLoaded = true;

处理器1：
while(!resourceLoaded) {
    try {
        Thread.sleep(1000);
    } catch(Exception) {
    }
}
resource.execute();   </pre>

<p>类似上面的代码，很可能处理器0先写了resource，再写了resourceLoaded。结果呢，写缓冲器进行了内存重排序，先落地了resourceLoaded = true了，此时resource还是null。此时处理器1就会看到resourceLoaded = true，就会对resource对象执行execute()方法，此时就会有空指针异常的问题
<p>反正类似的情况，高速缓存和写缓冲器都可以自己对Load和Store操作的结果落地到内存进行各种不同的重排序，进而造成上述4种内存重排序问题的发生


#### 9.10 synchronized锁同时对原子性、可见性以及有序性的保证

<p>原子性、可见性、有序性，三块东西，都重新从比较细节和底层的层面
<p>原子性，基本的赋值写操作都是可以保证原子性的，复杂的操作是无法保证原子性的
<p>可见性，MESI协议、flush、refresh，配合起来，才可以解决可见性
<p>有序性，三个层次，最后一个层次有4种重排（LoadLoad、StoreStore、LoadStore、StoreLoad）
<p>synchronized关键字，同时可以保证原子性、可见性以及有序性的
<p>原子性的层面而言，他加了以后，有一个加锁和释放锁的机制，加锁了之后，同一段代码就只有他可以执行了
<p>可见性，可以保证可见性的，他会通过加入一些内存屏障，他在同步代码块对变量做的写操作，都会在释放锁的时候，全部强制执行flush操作，在进入同步代码块的时候，对变量的读操作，全部会强制执行refresh的操作
<p>更新的数据，别的线程只要进入代码块，就一定可以读到的
<p>有序性，synchronized关键字，他会通过加各种各样的内存屏障，来保证说，解决LoadLoad、StoreStore等等重排序

#### 9.11 深入分析synchronized是如何通过加锁保证原子性的？

<p> 可以看下 synchronized 笔记
<p>synchronized底层的原理，monitor，没有特别细化，给大家详细说明一下synchronized实现原子性的核心原理
<p>之前给大家简单说过synchronized加锁的原理，说白了，就是在进入加锁代码块的时候加一个monitorenter的指令，然后针对锁对象关联的monitor累加加锁计数器，同时标识自己这个线程加了锁
<p>通过monitor里的加锁计数器可以实现可重入的加锁
<p>在出锁代码块的时候，加一个monitorexit的指令，然后递减锁计数器，如果锁计数为0，就会标志当前线程不持有锁，释放锁
<p>然后wait和notify关键字的实现也是依托于monitor实现的，有线程执行wait之后，自己会加入一个waitset中等待唤醒获取锁，notifyall操作会从monitor的waitset中唤醒所有的线程，让他们竞争获取锁
<p>这边来深入分析一下那个加锁的底层原理
<pre>
MyObject lock = new MyObject();
    synchronized(lock) {
} </pre>
<p>java对象都是分为对象头和实例变量两块的，其中实例变量就是大家平时看到的对象里的那些变量数据。然后对象头包含了两块东西，一个是Mark Word（包含hashCode、锁数据、GC数据，等等），另一个是Class Metadata Address（包含了指向类的元数据的指针）
<p>在Mark Word里就有一个指针，是指向了这个对象实例关联的monitor的地址，这个monitor是c++实现的，不是java实现的。这个monitor实际上是c++实现的一个ObjectMonitor对象，里面包含了一个_owner指针，指向了持有锁的线程。
<p>ObjectMonitor里还有一个entrylist，想要加锁的线程全部先进入这个entrylist等待获取机会尝试加锁，实际有机会加锁的线程，就会设置_owner指针指向自己，然后对_count计数器累加1次
<p>各个线程尝试竞争进行加锁，此时竞争加锁是在JDK 1.6以后优化成了基于CAS来进行加锁，理解为跟之前的Lock API的加锁机制是类似的，CAS操作，操作_count计数器，比如说将_count值尝试从0变为1
<p>如果成功了，那么加锁成功了；如果失败了，那么加锁失败了
<p>然后释放锁的时候，先是对_count计数器递减1，如果为0了就会设置_owner为null，不再指向自己，代表自己彻底释放锁
<p>如果获取锁的线程执行wait，就会将计数器递减，同时_owner设置为null，然后自己进入waitset中等待唤醒，别人获取了锁执行notify的时候就会唤醒waitset中的线程竞争尝试获取锁
<p>有人会问，那尝试加锁这个过程，也就是对_count计数器累加操作，是怎么执行的？如何保证多线程并发的原子性呢？
<p>很简单，JDk 1.6之后，对synchronized内的加锁机制做了大量的优化，这里就是优化为CAS加锁的
<p>把ReentrantLock底层的源码都读懂了，AQS的机制都读懂了之后，那么synchronized底层的实现差不多的，synchronized的ObjectMonitor的地位就跟ReentrantLock里的AQS是差不多的

#### 9.12 synchronized是如何使用内存屏障保证可见性和有序性的？

 <pre>
    int b = 0;
    int c = 0;
    synchronized(this) { -> monitorenter
        Load内存屏障
        Acquire内存屏障
        int a = b;
        c = 1; => synchronized代码块里面还是可能会发生指令重排
        Release内存屏障
    } -> monitorexit   </pre>
<p>Store内存屏障
<p>java的并发技术底层很多都对应了内存屏障的使用，包括synchronized，他底层也是依托于各种不同的内存屏障来保证可见性和有序性的
<p>按照可见性来划分的话，内存屏障可以分为Load屏障和Store屏障。
<p>Load屏障的作用是执行refresh处理器缓存的操作，说白了就是对别的处理器更新过的变量，从其他处理器的高速缓存（或者主内存）加载数据到自己的高速缓存来，确保自己看到的是最新的数据。
<p>Store屏障的作用是执行flush处理器缓存的操作，说白了就是把自己当前处理器更新的变量的值，都刷新到高速缓存（或者主内存）里去
<p>在monitorexit指令之后，会有一个Store屏障，让线程把自己在同步代码块里修改的变量的值都执行flush处理器缓存的操作，刷到高速缓存（或者主内存）里去；然后在monitorenter指令之后会加一个Load屏障，执行refresh处理器缓存的操作，把别的处理器修改过的最新值加载到自己高速缓存里来
<p>所以说通过Load屏障和Store屏障，就可以让synchronized保证可见性。
<p>按照有序性保障来划分的话，还可以分为Acquire屏障和Release屏障。
<p>在monitorenter指令之后，Load屏障之后，会加一个Acquire屏障，这个屏障的作用是禁止读操作和读写操作之间发生指令重排序。
<p>在monitorexit指令之前，会加一个Release屏障，这个屏障的作用是禁止写操作和读写操作之间发生重排序。
<p>所以说，通过 Acquire屏障和Release屏障，就可以让synchronzied保证有序性，只有synchronized内部的指令可以重排序，但是绝对不会跟外部的指令发生重排序。
<p>synchronized：
<li>（1）原子性：加锁和释放锁，ObjectMonitor
<li>（2）可见性：加了Load屏障和Store屏障，释放锁flush数据，加锁会refresh数据
<li>（3）有序性：Acquire屏障和Release屏障，保证同步代码块内部的指令可以重排，但是同步代码块内部的指令和外面的指令是不能重排的

#### 9.13 volatile关键字对原子性、可见性以及有序性的保证

<p>  volatile对原子性的保证真的是非常的有限，其实主要就是32位jvm中的long/double类型变量的赋值操作是不具备原子性的，加上
<p> volatile就可以保证原子性了
 <pre>
    volatile boolean isRunning = true;
     线程1：
     Release屏障
     isRunning = false;
         Store屏障 => 对于之前的讲解，更进了一步，原理，没有过多的牵扯到内存屏障的一些东西，可见性和有序性，
         主要都是基于各种内存屏障来实现的线程2：
     Load屏障
     while(isRunning) {
     Acquire屏障
     // 代码逻辑
     } </pre>
<p> 在volatile变量写操作的前面会加入一个Release屏障，然后在之后会加入一个Store屏障，这样就可以保证volatile写跟Release屏障之前的任何读写操作都不会指令重排，然后Store屏障保证了，写完数据之后，立马会执行flush处理器缓存的操作
<p> 在volatile变量读操作的前面会加入一个Load屏障，这样就可以保证对这个变量的读取时，如果被别的处理器修改过了，必须得从其他处理器的高速缓存（或者主内存）中加载到自己本地高速缓存里，保证读到的是最新数据；
<p> 在之后会加入一个Acquire屏障，禁止volatile读操作之后的任何读写操作会跟volatile读指令重排序
<p> 跟之前讲解的volatie读写内存屏障的知识对比一下，其实你看一下是类似的意思的
<p> 那个Acquire屏障其实就是LoadLoad屏障 + LoadStore屏障，Release屏障其实就是StoreLoad屏障 + StoreStore屏障
<p> 好像有点不太一样，对吧？
<p> 其实不要对内存屏障这个东西太较真，因为说句实话，不同版本的JVM，不同的底层硬件，都可能会导致加的内存屏障有一些区别，所以这个本来就没完全一致的。你只要知道内存屏障是如何保证volatile的可见性和有序性的就可以了
<p> 看各种并发相关的书和文章，对内存屏障到底是加的什么屏障，莫衷一是，没有任何一个官方权威的说法，因为这个内存屏障太底层了，底层到了涉及到了硬件，硬件不同对内存屏障的实现是不一样的
<p> 内存屏障这个东西，大概来说，其实就是大概的给你说一下这个意思，尤其是Release屏障，Store屏障和Load屏障还好理解一些，比较简单，Acqurie屏障，莫衷一是，我也没法给你一个官方的定论
<p> 具体底层的硬件实现
<p> 两点：volatile读写前后会加屏障，避免跟前后的读写操作发生指令重排
<p> volatile和synchronized保证可见性和有序性，原来都是通过各种内存屏障来实现的，因为加了内存屏障，就会有一些特殊的指令和实 现，就可以保证可见性和有序性了，有序性在几个阶段的指令重排的问题内存屏障对应的底层的一些基本的硬件级别的原理，也都讲清楚了


#### 硬件的原理
#### 9.14 高速缓存的数据结构：拉链散列表、缓存条目以及地址解码
<a href="https://imgchr.com/i/Yqwb90"><img src="https://s1.ax1x.com/2020/05/21/Yqwb90.jpg" alt="Yqwb90.jpg" border="0" /></a>
<p>处理器高速缓存的底层数据结构实际是一个拉链散列表的结构，就是有很多个bucket，每个bucket挂了很多的cache entry，每个cache entry由三个部分组成：tag、cache line和flag，其中的cache line就是缓存的数据
<p>tag指向了这个缓存数据在主内存中的数据的地址，flag标识了缓存行的状态，另外要注意的一点是，cache line中可以包含多个变量的值
<p>处理器会操作一些变量，怎么在高速缓存里定位到这个变量呢？
<p>那么处理器在读写高速缓存的时候，实际上会根据变量名执行一个内存地址解码的操作，解析出来3个东西，index、tag和offset。index用于定位到拉链散列表中的某个bucket，tag是用于定位cache entry，offset是用于定位一个变量在cache line中的位置
<p>如果说可以成功定位到一个高速缓存中的数据，而且flag还标志着有效，则缓存命中；否则不满足上述条件，就是缓存未命中。如果是读数据未命中的话，会从主内存重新加载数据到高速缓存中，现在处理器一般都有三级高速缓存，L1、L2、L3，越靠前面的缓存读写速度越快

#### 9.15 结合硬件级别的缓存数据结构深入分析缓存一致性协议
<p> 看计算机组成缓存那章
<p>因为有高速缓存的存在，所以就导致各个处理器可能对一个变量会在自己的高速缓存里有自己的副本，这样一个处理器修改了变量值，别的处理器是看不到的，所以就是为了这个问题引入了缓存一致性协议（MESI协议）
<p>MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个处理器可以执行，其实也会通过排他锁的机制保证就一个处理器能写
<p>之前说过那个cache entry的flag代表了缓存数据的状态，MESI协议中划分为：

<li>（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用
<li>（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已
<li>（3）exclusive：独占的，标记为E，这个意思就是当前处理器对这个数据独占了，只有他可以有这个副本，其他的处理器都不能包含这个副本
<li>（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache entry，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的
<p>MESI协议规定了一组消息，就说各个处理器在操作内存数据的时候，都会往总线发送消息，而且各个处理器还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个处理器的协作
<p>下面来详细的图解MESI协议的工作原理，处理器0读取某个变量的数据时，首先会根据index、tag和offset从高速缓存的拉链散列表读取数据，如果发现状态为I，也就是无效的，此时就会发送read消息到总线
<p>接着主内存会返回对应的数据给处理器0，处理器0就会把数据放到高速缓存里，同时cache entry的flag状态是S
<p>在处理器0对一个数据进行更新的时候，如果数据状态是S，则此时就需要发送一个invalidate消息到总线，尝试让其他的处理器的高速缓存的cache entry全部变为I，以获得数据的独占锁。
<p>其他的处理器1会从总线嗅探到invalidate消息，此时就会把自己的cache entry设置为I，也就是过期掉自己本地的缓存，然后就是返回invalidate ack消息到总线，传递回处理器0，处理器0必须收到所有处理器返回的ack消息
<p>接着处理器0就会将cache entry先设置为E，独占这条数据，在独占期间，别的处理器就不能修改数据了，因为别的处理器此时发出invalidate消息，这个处理器0是不会返回invalidate ack消息的，除非他先修改完再说
<p>接着处理器0就是修改这条数据，接着将数据设置为M，也有可能是把数据此时强制写回到主内存中，具体看底层硬件实现
<p>然后其他处理器此时这条数据的状态都是I了，那如果要读的话，全部都需要重新发送read消息，从主内存（或者是其他处理器）来加载，这个具体怎么实现要看底层的硬件了，都有可能的

#### 9.16 采用写缓冲器和无效队列优化MESI协议的实现性能

<p>MESI协议如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了
<p>所以为了解决这个问题，硬件层面引入了写缓冲器和无效队列
<p>写缓冲器的作用是，一个处理器写数据的时候，直接把数据写入缓冲器，同时发送invalidate消息，然后就认为写操作完成了，接着就干别的事儿了，不会阻塞在这里。接着这个处理器如果之后收到其他处理器的ack消息之后
<p>才会把写缓冲器中的写结果拿出来，通过对cache entry设置为E加独占锁，同时修改数据，然后设置为M
<p>其实写缓冲器的作用，就是处理器写数据的时候直接写入缓冲器，不需要同步阻塞等待其他处理器的invalidate ack返回，这就大大提升了硬件层面的执行效率了
<p>包括查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发
<p>引入无效队列，就是说其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可
<p>通过引入写缓冲器和无效队列，一个处理器要写数据的话，这个性能其实很高的，他直接写数据到写缓冲器，发送一个validate消息出去，就立马返回，执行别的操作了；其他处理器收到invalidate消息之后直接放入无效队列，立马就返回invalidate ack

#### 9.17 硬件层面的MESI协议为何会引发有序性和可见性的问题？
<p>可见性：写缓冲器和无效队列导致的，写数据不一定立马写入自己的高速缓存（或者主内存），是因为可能写入了写缓冲器；读数据不一定立马从别人的高速缓存（或者主内存）刷新最新的值过来，invalidate消息在无效队列里面
<p>有序性：
<p>（1）StoreLoad重排序
<pre>int a = 0;
int c = 1;
线程1：
a = 1;
int b = c;  </pre>
<p>这个很简单吧，第一个是Store，第二个是Load。但是可能处理器对store操作先写入了写缓冲器，此时这个写操作相当于没执行，然后就执行了第二行代码，第二行代码的b是局部变量，那这个操作等于是读取a的值，是load操作
<p>这就导致好像第二行代码的load先执行了，第一行代码的store后执行
<p>第一个store操作写到写缓冲器里去了，导致其他的线程是读不到的，看不到的，好像是第一个写操作没执行一样；第二个load操作成功的执行了
<p>StoreLoad重排，Store先执行，Load后执行；Load先执行，Store后执行（2）StoreStore重排序
<pre>resource = loadResource();
loaded = true;
</pre>
<p>两个写操作，但是可能第一个写操作写入了写缓冲器，然后第二个写操作是直接修改的高速缓存，这个时候不就导致了两个写操作顺序颠倒了？
<p>诸如此类的重排序，都可能会因为MESI的机制发生
<p>可见性问题也是一样的，写入写缓冲器之后，没刷入高速缓存，导致别人读不到；读数据的时候，可能invalidate消息在无效队列里，
<p>导致没法立马感知到过期的缓存，立马加载最新的数据

#### 9.18 内存屏障在硬件层面的实现原理以及如何解决各种问题
<p>可见性问题：
<p>Store屏障 + Load屏障
<p>如果加了Store屏障之后，就会强制性要求你对一个写操作必须阻塞等待到其他的处理器返回invalidate ack之后，对数据加锁，然后修改数据到高速缓存中，必须在写数据之后，强制执行flush操作
<p>他的效果，要求一个写操作必须刷到高速缓存（或者主内存），不能停留在写缓冲里
<p>如果加了Load屏障之后，在从高速缓存中读取数据的时候，如果发现无效队列里有一个invalidate消息，此时会立马强制根据那个invalidate消息把自己本地高速缓存的数据，设置为I（过期），然后就可以强制从其他处理器的高速缓存中加载最新的值了
<p>这就是refresh操作
<p>为了解决有序性问题
<p>内存屏障，Acquire屏障，Release屏障，但是都是由基础的StoreStore屏障,StoreLoad屏障，可以避免指令重排序的效果
<p>StoreStore屏障，会强制让写数据的操作全部按照顺序写入写缓冲器里，他不会让你第一个写到写缓冲器里去，第二个写直接修改高速缓存了
<p>resource = loadResource();
<p>StoreStore屏障
<p>loaded = true;
<p>StoreLoad屏障，他会强制先将写缓冲器里的数据写入高速缓存中，接着读数据的时候强制清空无效队列，对里面的validate消息全部过期掉高速缓存中的条目，然后强制从主内存里重新加载数据
<p>a = 1; // 强制要求必须直接写入高速缓存，不能停留在写缓冲器里，清空写缓冲器里的这条数据
<p>int b = c;


#### 9.19 在复杂的硬件模型之上的Java内存模型是如何大幅简化的？
<p>java内存模型是对底层的硬件模型，cpu缓存模型，做了大幅度的简化，提供一个抽象和统一的模型给java程序员易于理解，很多时候
<p>如果要理解一些技术的本质，还是要深入到底层去研究的
<p>volatile，原子性，可见性，有序性，加了一些内存屏障可以避免前后各种读写指令重排
<p>synchronized，原子性，可见性，有序性，没有提到
<p>CAS，cas指令到硬件级别，实现了一个原子性的cas操作

#### 9.20 如何从内存屏障、硬件层面的原理来震慑面试官
<p>volatile、synchronized
<p>原子性这块，直接把底层的一些东西喷出来
<p>硬件层面的原理 -> MESI协议在硬件层面运行的原理 -> 这套原理为何会导致可见性和有序性的问题 -> 各种内存屏障是如何在硬件层面解决可见性和有序性的问题 -> volatile和synchroized是如何加各种内存屏障来分别保证可见性和有序性的


#### 9.21 Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁
<p>synchronized说是锁，但是他的底层加锁的方式 可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁
<p>这些东西本身你只要了解一个概念就可以了，JDK 1.6开始对synchronized关键字做过哪些优化，有哪些加锁的方式，效果是什么，作用是什么，在实际的开发和使用中，根本就不需要你去过多的care一些东西
<p>        synchronized(this) {
<p>        }
<li>（1）锁消除
<p>锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令
<p>这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁
<li>（2）锁粗化
<pre>synchronized(this) {
     }
     synchronized(this) {
     }
     synchronized(this) {
     }</pre>
<p>这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁
<li>（3）偏向锁
<p>这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS
<p>性能会提升很多
<p>但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好
<p>可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小
<p>如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好
<p>（4）轻量级锁
<p>如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁
<p>如果是自己加的锁，那就执行代码就好了
<p>如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁
<li>（5）适应性锁
<p>这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒
<p>也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大

<p>所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了
<p>这样可以大幅度减少线程上下文的切换，而这种自旋等待获取锁的方式，就是所谓自旋锁，就是不断的自旋尝试获取锁
<p>如果一个线程持有锁的时间很长，那么其他线程获取不到锁，就会暂停，发生上下文切换，让其他线程来执行，这种自己暂停获取锁的方式，就是所谓的重量级锁
<p>这个根据不同情况自动调整的过程，就是适应锁的意思

####   9.22 再来看看CAS是如何基于MESI协议在底层硬件层面实现加锁的？
<p>无法发出指令来执行一个原子性的cas，先查出数据，比较一下，如果一样，就写数据。MESI协议有关系
<p>总结:cas主要基于mesi协议中的e，也就是对该变量在硬件级别上加一个独占锁，从而来实现原子性的比较替换。

<p>volatile、synchronized、CAS、ThreadLocal、ReentrantReadWriteLock、锁优化、锁生产故障 


### 10  安全问题

<p>（1）     能不能聊聊平时我们开发的系统，有可能被黑客以哪些方式来攻击呢？
<p>（2）     XSS攻击方式背后的原理是什么，SQL注入背后的原理是什么，等等，各种攻击方式背后的原理是什么？
<p>（3）     针对常见的黑客攻击方式，你平时开发系统的时候都有哪些方案可以去保护你的系统安全，避免被黑客攻破
<p>（4）     平时你们微服务架构里，网关系统用的是什么？在网关层面如何防止黑客攻击？
<p>（5）     哪怕网关不是你负责的，你负责的一些系统的接口，如何保证你设计的接口的安全性呢？
<p>（6）     缓存穿透，假如说有黑客攻击你，每次使用的缓存的Key是不同的，传统的缓存穿透的方案无法防御，此时怎么办呢？
<p>（7）     加密算法，公钥密钥是怎么回事，如何进行加密的网络通信？数据加密？
<p>（8）     除了公钥密钥以外，你们有没有完整的一套系统安全性防御机制呢，防火墙，网站安全漏洞扫描，密钥存储是如何做的

#### 10.1 XSS网络攻击的原理
<p>XSS攻击和SQL注入就是大部分的黑客进行网络攻击的手段，此外还有很多别的攻击方式，比如说CSRF、Session劫持之类的，今天先
<p>说说XSS网络攻击的原理
<p>XSS的全称是Cross Site Script，就是跨站点脚本攻击，意思就是说，黑客恶意篡改你的网页的前端代码，在里面注入一些他自己的
<p>html+javascript的脚本和代码，然后你比如在访问那个网站的网页的时候，他注入的那些恶意脚本就会运行了
<p>恶意脚本运行的时候就会控制你的浏览器，这个时候他的脚本就可以做很多很多的事情了
<p>第一种XSS攻击是反射型攻击，他主要是想办法让你点击一个URL链接，在这个URL链接里就嵌入他自己的恶意脚本，你点击那个URL
<p>链接之后，那个URL指向的是黑客自己的服务器上的一段恶意脚本
<p>他可能给你展示的是一个什么什么图片，或者是一个flash的动图，或者是一个小视频的东西，诱惑性，引诱你去点击
<p>然后恶意脚本被返回到你的浏览器里就会运行，然后就可以控制你的浏览器里的行为了，这个控制行为就很恐怖了，他可以干很多的事
<p>儿，比如说脚本可以自动让你关注某个用户ID，然后控制你自动发布一个带有病毒的微博，这是比较简单的
<p>实际上来说，一段恶意的js脚本，几乎可以说是无恶不作的，因为他一旦控制了你的浏览器就可以得到大量的东西，大家都知道浏览器
<p>里包含了你的一些cookie，有的浏览器可能还存储了你的密码，通过知道你的cookie，就可以利用cookie伪造你的用户登录的session
<p>状态，去以你这个用户的名义干一些事儿
<p>另外一种XSS攻击是叫做持久型攻击，这个意思就是说，举个例子，比如是个什么论坛、或者社交网站之类的系统，不是你可以发布一
<p>些帖子啊，或者是评论啥的内容么，此时黑客就可以在里面写一段恶意脚本
<p>然后把恶意脚本混杂在评论内容里提交到你的网站的数据库里去
<p>然后后面比如其他用户在社交网站里浏览到了你的这个评论，评论内容会被返回到浏览器里去，此时评论内容是包含恶意js脚本的，马
<p>上恶意脚本运行，又可以干坏事儿了，干的坏事儿就跟之前是一样的
<p>如果要防止XSS攻击，一般来说手段有如下两种：
<p>包含恶意URL链接的图片、视频、动图、flash动画，平时自己注意一下，少点，尽量使用正规的网站
<p>消毒机制，这就是说，如果黑客在一些评论之类的内容里混入恶意脚本，那么你的代码里必须对内容进行消毒，就是进行一些转义，比
<p>如说把>转义为&gt之类的，这样就可以把恶意脚本里的html标签、js代码之类的东西，都给转义掉，让这些恶意脚本失效
<p><html> -> &lthtml&gt，这种东西在浏览器里是不会运行的
<p><html><script>// 包含恶意脚本</script></html>
<p>这样的话，转义以后的脚本被其他用户看到的时候也不会在浏览器里运行了
<p>HttpOnly方式，这个意思是说如果你在浏览器里存放cookie的时候，可以设置一个HttpOnly属性，比如说存放用户加密认证信息的
<p>cookie，这样的话，在浏览器里运行的js脚本是被禁止访问这些HttpOnly cookie的，他就无法窃取你在浏览器里存储的cookie了

####  10.2 SQL注入攻击背后的原理

<p>但是如果要给你搞SQL注入，其实也不是那么容易的，因为必须要知道你的数据库表结构才行，一般获取数据库表结构的方式就下面几种：
<p>（1）如果你使用的是开源软件，比如开源的博客系统，论坛系统，或者别的什么系统，那么人家自然知道你的表结构了，这种情况是比较少见的
<p>（2）错误回显，不知道大家有没有经历过这种，就是你有时候把系统跑在web服务器里，然后程序报错了，结果直接在浏览器页面上显示出来了你的异常堆栈信息，包括有错误的SQL语句，这就尴尬了，通过这个，黑客直接就知道你的表结构了
<p>（3）根据你的请求参数的名称，大致推测你的数据库表结构，这个一般不太现实我就经常在一些不大不小的站点会见过，5年前开始，我会在一些网站上发布我录制的一些课程，有一些有一定知名度的不大不小的站点，我就经常亲眼见过，站点可能有bug，我一点击什么东西，他系统内部直接就报错了
<p>执行SQL语句的时候报错了，是不是会有异常堆栈，在controller层面没有进行try catch，他在controller层面就直接把你的异常给抛出来了，被mvc框架捕获到，mvc框架就直接把这段异常堆栈信息返回给浏览器了
<p>在浏览器里，我居然经常见到一个站点内部的SQL语句报错的异常，直接可以看到SQL语句的语法，通过SQL语句，就可以反过来推测
<p>出来你的表结构，此时就可以观察你的系统有哪些http接口，然后可以通过postman那种工具，去构造一个请求发送过去执行
<p>在参数里可以拼接进去一个恶意的SQL语句进行注入
<p>所以要防止SQL注入，一个是别让人家知道你的数据表结构，关闭web服务器的错误回显，显示一个400，500之类的就可以了，
<p>另外一个，就是要用预编译的方法，现在mybatis、hibernate都是支持预编译的
<p>放到底层的JDBC里，PreparedStatement，对SQL进行预编译，如果你给SQL的某个参数传入进去的是一个恶意SQL语句，人家预编译过后，会让你的恶意SQL语句是无法执行的，所以千万不要直接自己用字符串去拼接SQL语句
<p>insert into xxx_table(xx,xxx,xx) values(?,?,?)，对这个SQL进行预编译，然后给他里面把各个参数设置进去，此时参数里如果带有恶意SQL是不会作为SQL去执行的
<p>mybatis
<p>对这个方法比如传递进去了一个map或者是对象，mybatis，根据你的占位符的变量名字，从你的Map里或者是对象里提取出来一个一个的参数的值，进行预编译SQL的参数值的设置
<p>insert into xxx_table(xxx,xx,xx) values(#{xx},#{xx},#{xxx})
<p>这个预编译，就是说把黑客在参数里混进来来的SQL语句当做一个参数，而绝对不会作为独立的SQL语句去执行，这就避免了SQL注入攻击了
<p>所以说，平时开发系统，我们一定要注意这两件事情，包括关闭web服务器错误回显，包括mybatis之类的用预编译，不要直接拼接SQL语句

#### 10.3 CSRF攻击吗？你知道他背后的原理是什么吗？

<p>Cross Site Request Forgery，垮站点请求伪造
<p>这个就是黑客想办法去伪造成你这个用户去发送请求到某个系统上去，然后查询你的数据，转账交易之类的，伪装成你，也有很多办
<p>法，比如利用XSS搞一个恶意脚本让你执行，然后盗取你的浏览器里的cookie
<p>利用你的cookie伪装成你登录的状态，然后去执行一些请求
<p>利用XSS跨站点脚本攻击，获取cookie，然后再利用postman发送垮站点伪造请求
<p>防御CSRF的方法主要是以下几种：
<li> （1）防止cookie被窃取：最最根本的，其实还是说防止cookie被窃取，可以给你的网站的cookie设置HttpOnly属性，禁止被别人的script脚本窃取，那么别人就无法伪造用户登录请求了
<li>（2）随机token：每次返回一个页面给你的时候，都生成一个随机token附加在页面的隐藏元素里，同时在你的redis里可以存以下，然后页面发送请求的时候附加随机token，验证通过才能执行请求，你要是自己用postman构造请求就不知道随机token是什么了
<li>（3）验证码：页面提交必须搞一个验证码，那种图形的，现在比较流行的还有拖动一个拼图什么的，必须验证码通过了才能执行你的请求，避免黑客直接伪造请求发送过来，这个其实是比较常见的，最好是在用户进行支付交易的时候，要求必须在页面上拖拽一个拼图验证码
<li>（4）Referer请求头：这个是http请求里有一个referer请求头，带有这个请求的来源，你可以验证一下这个请求是不是从自己的页面里来的，如果是的话才执行，否则就不要执行了

#### 10.4 如果你们的系统允许用户上传文件，可能会遭到什么样的黑客攻击？

<p>很多时候如果我们的网站允许别人上传文件，那么文件可能是可执行的脚本，可能是病毒或者木马文件，其实这个是非常危险的，如果是脚本的话，可能会在服务器执行，搞很多破坏，比如黑客黑掉你的服务器，勒索你给他比特币之类的
<p>比如把自己的文件后缀改成.jpg、.txt之类的东西，来上传，其实本质病毒文件
<p>病毒脚本是非常的可怕的，因为原则上来说，只要黑客掌握底层的一些技术，就可以利用病毒脚本干各种各样的事情，比如连接你的数据库之类的
<p>对于文件上传这块，核心的就是要进行白名单校验，限制上传文件的类型，只能是我们指定的，而且要限制文件的大小，还要对文件重命名，限制文件类型不能简单的根据后缀来判断，可能后缀被篡改了，要根据文件二进制数据的开头几个字节代表的magic number来判断文件的类型
<p>读取这个文件的二进制数据流，读取开头的几个字节，提取这个文件的魔数，根据魔数的值去判断他是什么类型的
<p><pre>
<p>FFD8FF：JEPG
<p>89504E47：PNG   </pre>
<p>类似这样，以此类推
<p>比如说你的网站要求用户只能上传word类型，png类型，此时你就限制仅仅这几种文件是可以上传的，其他的类型的文件都不让上传
<p>网上可以查到完整的magic number列表，根据那个限制一下，哪些文件类型可以上传，这样就避免说有那种木马、病毒之类的可执行文件被上传了
<p>而且还要限制，不允许用户上传大文件，文件超过一定大小就不让上传了
<p>然后对上传好的文件进行重命名
<p>而且最好对文件进行一定的压缩，这样可以破坏原来的文件结构，避免文件在服务器执行，利用imagemagick这种开源包，可以很方便进行文件缩放

#### 10.5 让所有工程师闻声色变的DDoS攻击到底是什么东西？
<p>DDoS，distributed denial of service，分布式拒绝服务攻击，最可怕的黑客攻击，可以把你的网站、APP、系统给搞瘫痪了
<p>DoS攻击，就是说黑客知道你的服务器地址了，然后你的系统假设每秒就抗下1000请求，黑客就以每秒1000请求访问你，你的服务器线程资源全部打满，正常用户根本无法发送请求，你的网站就宕机了
<p>甚至他以每秒1万请求攻击你的服务器呢，那就的系统机器就挂了
<p>如何防御DDoS攻击？自己其实挺难的，这其实是非常专业的一种攻击手段，通常我们可以采购云厂商的安全服务，比如DDoS高防IP，可以把攻击流量都导入到云厂商的高防IP的服务器上去，他们有专业的技术方案和算法来防御

#### 10.6 基于SYN Flood模式的DDoS攻击，背后的原理是什么呢？

<p>TCP三次握手
<li>1、客户端发送一个SYN请求，指明客户端的端口号以及TCP连接的初始序列号
<li>2、的服务器收到SYN后，返回一个SYN+ACK，表示请求被接收，TCP序列号加1
<li>3、客户端收到服务器的SYN+ACK后，返回一个ACK给服务器，TCP序列号加1，连接建立完毕，接着可以通信了
<p>如果服务器没有收到第三步的ACK，会重试返回SYN+ACK给客户端，同时处于SYN_RECV状态，把客户端放入等待列表。重试会3~5次，每隔30重试一次，遍历等待列表，再次重试发送SYN+ACK
<p>只要返回SYN+ACK给客户端，就会为客户端预留一部分资源，重试期间都保留，等待跟客户端建立连接；所以如果说太多的客户端来建立连接，资源耗尽，那么就无法建立新的TCP连接了
<p>所以黑客就会伪造大量的不同ip地址去发送SYN请求给一台服务器建立TCP连接，每次都是卡在服务器返回SYN+ACK，但是黑客是不会最终返回ACK的，所以导致服务器可能为了黑客建立了大量的半连接放在等待列表里，占用了大量的资源，还得不停的去重试
<p>一旦服务器的资源耗尽，那么正常的请求过来，是无非建立TCP连接的要知道，HTTP底层就是基于TCP实现的，一旦你无法建立TCP连接，那么这台服务器也自然接受不了任何HTTP请求

#### 10.7 基于DNS Query Flood和HTTP Flood的DDoS攻击
<p>这个DNS Query Flood攻击，顾名思义，就是去攻击DNS服务器，也就是伪造大量的域名解析请求发送给DNS服务器，然后DNS服务器必然没有，接着必然会去找上级DNS服务器，一直到根域名服务器
<p>这么搞必然导致DNS服务器的资源别耗尽，其他正常人浏览网页也要解析域名的，此时就没法访问DNS服务器了
<p>13.68.131.42
<p>cc攻击，HTTP flood，其实就是之前说过的那种，直接就是在互联网上找到大量的HTTP代理，说白了，其实本身就有很多公司提供
<p>HTTP代理服务，自己搜一下就知道了，就是有很多代理服务器，你可以控制那些HTTP代理服务器去给目标服务器发送大量的HTTP请求
<p>然后目标服务器直接肯定就挂了
<p>Nginx、Tomcat、Jetty，机器上都是会部署Web服务器，都是一个进程，启动多个线程，来并发的处理各种HTTP请求
<p>控制大量的肉鸡，控制大量的HTTP代理，TCP SYN Flood、DNS Query Flood、HTTP Flood，搞瘫痪你的服务器，或者DNS服务器，DDoS攻击，利用大量的机器进行分布式的海量请求发送，你的网站的服务不可用

#### 10.8 分布式架构中，Zuul网关是如何防止网络攻击的？

<p>XSS、CRSF、SQL注入、DDoS
<p>XSS核心是设置cookie的http only属性，过滤脚本，CRSF也是设置cookie的http only属性，根据referer请求头来过滤，设置表单随机参数，SQL注入就是过滤恶意参数，DDoS攻击主要就是限流
<p>zuul网关里加一个过滤器，过滤器里去过滤一些特殊请求的脚本，根据referer请求头过滤，对请求的随机token进行校验，甚至可以对参数进行校验，如果参数里包含SQL说明要注入，全部过滤，对ip地址可以进行基于redis的访问计数，比如说一个ip地址一秒内连续访问5次，那么就直接禁止访问
<img src="https://s1.ax1x.com/2020/05/25/tCckVg.jpg" alt="tCckVg.jpg" border="0" />

### 11 怎么深挖网络与IO的面试连环炮的？
<li>1、Netty的架构原理图能画一下吗，他是如何体现Reactor架构思想的？
<li>2、能说说你对Netty堆外内存的理解吗？什么情况下会使用堆外内存？
<li>3、你遇到过堆外内存溢出或者堆外内存泄漏的场景吗？怎么解决的？
<li>4、能聊聊你对零拷贝技术原理的理解吗？他到底是如何提升性能的？
<li>5、你知道哪些开源的中间件系统用了零拷贝技术吗？为什么那些系统要使用零拷贝技术？
<li>6、你了解过在操作系统层面，系统区内存和用户区内存的关系是什么吗？
<li>7、说说你对序列化机制的理解，了解过Protobuf吗？他是用来干什么的知道吗？

#### 11.1 Netty的架构原理图能画一下吗，他是如何体现Reactor架构思想的？
<p>里面的技术的点特别的多，Netty是一款非常优秀的，高性能的，一个网络通信的框架，他底层的话呢，把NIO进行了重度的封装，如果大家连NIO，BIO，区别，最好自己先去看看NIO，以及NIO的一些示例代码
<p>JDK提供了网络编程的框架，NIO这套东西
<p>netty平时常用的一些场景，主要是一些中间件系统，比如分布式存储系统，分布式消息系统，比较典型的，RocketMQ，底层做网络通信这一块就是基于Netty来的，分布式服务框架，服务之间进行远程通信，也可以基于Netty来

#### 11.2 堆外内存的理解？堆外内存的优势在哪里？

<p>如何用堆外内存？
<p>ByteBuffer buffer = ByteBuffer.allocateDirect(1024); // 传入的是你要申请的堆外内存的大小
<p>// 你可以直接把你的数据写入到内外内存DirectByteBuffer里去
<p>// 把这块数据通过Socket发送，就是直接发送就可以了，不需要走一个拷贝
<p>堆外内存的优势？堆内的数据，要网络IO写出去，要先拷贝到堆外内存，再写入到socket里发送出去；如果直接数据分配在堆外内存，
<p>是不需要有一次额外的拷贝的，性能是比较高的
<p>读写文件也是同理的，都可以节约数据拷贝次数
<li>1、如果堆外内存足够，就直接预留一部分内存
<li>2、如果堆外内存不足，则将已经被 JVM 垃圾回收的 DirectBuffer 对象的堆外内存释放
<li>3、如果进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则进行 System.gc()
<li>4、如果 9 次尝试后依旧没有足够的可用堆外内存，则抛异常。

#### 回收和溢出
<p> -XX:MaxDirectMemorySize：通过JVM参数是可以设置你最大可以使用的堆外内存的大小的，比如说设置堆外内存最大可以使用1GB，此时已经使用了950MB空间了，然后呢，你此时要申请一块80MB的堆外内存
<p>会发现说，堆外内存已经不够了，此时不能直接分配堆外内存了
<p>DirectByteBuffer，这个对象是JVM堆内存里的一个对象，但是这个DirectByteBuffer里面包含指针，引用了一块堆外的内存
<li>1、如果堆外内存足够，就直接预留一部分内存
<li>2、如果堆外内存不足，则将已经被 JVM 垃圾回收的 DirectBuffer 对象的堆外内存释放
<li>3、如果进行一次堆外内存资源回收后，还不够进行本次堆外内存分配的话，则进行 System.gc()
<li>4、如果 9 次尝试后依旧没有足够的可用堆外内存，则抛异常
<li>5、实际分配内存

<p>jvm专栏，或者是对jvm的垃圾回收有一定的理解的话
<p>jvm一般分为young gc和full gc，无论是发生哪种gc，都可能会回收掉一些没有GC roots变量引用的DirectByteBuffer对象，回收掉了之后，就会主动释放他们引用的那些堆外内存，是这样子的
<p>DirectByteBuffer回收，就会回收关联的堆外内存，或者是内部有一个cleaner对象，可以用反射获取他，然后调用他的clean方法来主动释放内存
<p>如果依靠jvm gc机制，可能DirectByteBuffer躲过N次minor gc进入了老年代，然后老年代迟迟没有放满，因此迟迟没有回收，此时可能会导致DirectByteBuffer对象一直在引用堆外内存
<p>这样当你要分配更多的堆外内存时，无法腾出来更多的内存，就会有堆外内存溢出了
<p>堆内内存的OOM一样，out of memory，内存耗尽，实在是没有空闲的内存空间给你来使用了，因为所有的内存此时都别别人在使用，你要申请一块新的内存空间，实在是没有了，所以就OOM
<p>堆外内存的溢出，也是一样的

#### 11.3 如果不使用零拷贝技术，普通的IO操作在OS层面是如何执行的？
 <pre>
File file = new File("xxx.txt");
RandomAccessFile raf = new RandomAccessFile(file, "rw");
byte[] arr = new byte[(int) file.length()];
raf.read(arr);
Socket socket = new ServerSocket(8080).accept();
socket.getOutputStream().write(arr);   </pre>
<p>使用read读取数据的时候，会有一次用户态到内核态的切换，也就是说从用户角度切换到了内核角度去执行，这个时候基于DMA引擎把磁盘上的数据拷贝到内核缓冲里去；接着会从内核态切换到用户态，基于CPU把内核缓冲里的数据拷贝到用户缓冲区里去
<p>接着我们调用了Socket的输出流的write方法，此时会从用户态切换到内核态，同时基于CPU把用户缓冲区里的数据拷贝到Socket缓冲区里去，接着会有一个异步化的过程，基于DMA引擎从Socket缓冲区里把数据拷贝到网络协议引擎里发送出去都完成之后，从内核态切换回用户态
<p>所以说，从本地磁盘读取数据，到通过网络发送出去，用户态和内核态之间，要发生4次切换，这是其一；其二，数据从磁盘拿出来过后，一共要经过4次拷贝；所以说，这4次切换和4次拷贝，让普通的IO操作都性能较低

#### 11.4 mmap？内存映射技术为什么可以提升IO性能？
<p>把一个磁盘文件映射到内存里来，然后把映射到内存里来的数据通过socket发送出去
<p>有一种mmap技术，也就是内存映射，直接将磁盘文件数据映射到内核缓冲区，这个映射的过程是基于DMA引擎拷贝的，同时用户缓冲区是跟内核缓冲区共享一块映射数据的，建立共享映射之后，就不需要从内核缓冲区拷贝到用户缓冲区了
<p>光是这一点，就可以避免一次拷贝了，但是这个过程中还是会用户态切换到内核态去进行映射拷贝，接着再次从内核态切换到用户态，建立用户缓冲区和内核缓冲区的映射
<p>接着把数据通过Socket发送出去，还是要再次切换到内核态
<p>接着直接把内核缓冲区里的数据拷贝到Socket缓冲区里去，然后再拷贝到网络协议引擎里，发送出去就可以了，最后切换回用户态减少一次拷贝，但是并不减少切换次数，一共是4次切换，3次拷贝
<p>mmap技术是主要在RocketMQ里来使用的，RocketMQ底层主要就是基于mmap技术来提升了磁盘文件的读写，性能

#### 11.5 零拷贝技术到底是什么，他是如何提升IO性能的？
<p>linux提供了sendfile，也就是零拷贝技术
<p>在你的代码里面，如果说你基于零拷贝技术来读取磁盘文件，同时把读取到的数据通过Socket发送出去的话，流程如下，Kafka源码，
<p>transferFrom和transferTo两个方法，从磁盘上读取文件，把数据通过网络发送出去
<p>这个零拷贝技术，就是先从用户态切换到内核态，在内核态的状态下，把磁盘上的数据拷贝到内核缓冲区，同时从内核缓冲区拷贝一些
<p>offset和length到Socket缓冲区；接着从内核态切换到用户态，从内核缓冲区直接把数据拷贝到网络协议引擎里去
<p>同时从Socket缓冲区里拷贝一些offset和length到网络协议引擎里去，但是这个offset和length的量很少，几乎可以忽略
<p>只要2次切换，2次拷贝，就可以了
<p>kafka、tomcat，都是用的零拷贝技术，rocketmq用的是mmap技术，mmap还是要多2次切换和1次拷贝的，在Java代码中如何进行mmap和零拷贝，大家可以去看一看网上的一些资料
<p>用户态和内核态，用户态空间，内核态空间


### 12 部分分布式架构面试连环炮

<li>你们的分布式系统是如何进行链路监控的？说说链路追踪系统架构原理？
<li> 对分布式系统进行核心链路追踪的时候，链路id是怎么管理的？
<li>聊过两阶段提交了，那么分布式事务三阶段提交的思想能说一下吗？
<li>唯一id生成机制中的snowflake算法的时钟回拨问题如何解决？
<li>实施灰度发布的时候，网关是可以灰度了，可是Dubbo服务如何进行灰度呢？
<li>除了常见服务注册中心之外，你觉得Redis能作为服务注册中心吗？

#### 12.1 分布式系统是如何进行链路监控的？都监控什么？

<p>追踪了有什么用，调用链路，链路性能监控，链路故障排查
<p>Google的Dapper，阿里的鹰眼，大众点评的CAT，Twitter的Zipkin，LINE的pinpoint，国产的skywalking，很多，国内一般用CAT和zipkin比较多
<p>其实核心架构就是做一个框架，然后每一次服务调用都要经过这个框架，框架采集调用链路的数据存储起来，然后有可视化界面展示出来每个调用链路，性能，故障，这些东西
<p>订单服务收到这个请求是12:00:00，商品服务收到这个请求是12:00:01，库存服务收到这个请求是12:00:10

#### 12.2 分布式系统进行核心链路追踪的时候，链路id是怎么管理的？
<p>每一个请求入口，traceid，每一次服务调用，spanid，上游服务id，parenetid，调用时间，timestamp，有正向的，还有反向的，把请求发出，请求接收，业务处理，各种时间都记录下来，计算网络耗时和业务处理耗时
<p>底层的服务框架，接收到每一层请求调用的时候都要交给链路追踪系统的客户端框架来处理一下，traceid，代表了一次请求
<p>链路追踪数据：
<pre>
traceid=1，spanid=1，parentid=0，received_timestamp=12:00:00 300，send_timestamp=12:00:00 302  // 请求 1  id 1  父节点 0
traceid=1，spanid=2，parentid=1，received_timestamp=12:00:00 303，send_timestamp=12:00:00 305  // 请求 1  id 2  父节点 1
traceid=1，spanid=3，parentid=2，received_timestamp=12:00:00 306   // 请求 1  id 3 父节点 2  (没有请求下级)
traceid=1，spanid=3，parentid=2，send_timestamp=12:00:05 502     // 请求 1  id 3 父节点 2   (完成返回)
traceid=1，spanid=2，parentid=1，received_timestamp=12:00:05 503  // 请求 1  id 2 父节点 1  (自己完成时间)  </pre>


#### 12.3 两阶段提交，那么分布式事务三阶段提交的思想能说一下吗？
<p>两个阶段的执行

<li>1.请求阶段（commit-request phase，或称表决阶段，voting phase）
<p>在请求阶段，协调者将通知事务参与者准备提交或取消事务，然后进入表决过程。
<p>在表决过程中，参与者将告知协调者自己的决策：同意（事务参与者本地作业执行成功）或取消（本地作业执行故障）。

<li>2.提交阶段（commit phase）
<p>在该阶段，协调者将基于第一个阶段的投票结果进行决策：提交或取消。
<p>当且仅当所有的参与者同意提交事务协调者才通知所有的参与者提交事务，否则协调者将通知所有的参与者取消事务。
<p>参与者在接收到协调者发来的消息后将执行响应的操作。

<li>（3）两阶段提交的缺点

<p>1.同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
<p>
<p>2.单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）
<p>
<p>3.数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

<p>（4）两阶段提交无法解决的问题

<p>当协调者出错，同时参与者也出错时，两阶段无法保证事务执行的完整性。
<p>考虑协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。
<p>那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

<p>（1）三个阶段的执行
<li>1.CanCommit阶段
<p>3PC的CanCommit阶段其实和2PC的准备阶段很像。
<p>协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

<li>2.PreCommit阶段
<p>Coordinator根据Cohort的反应情况来决定是否可以继续事务的PreCommit操作。
<p>根据响应情况，有以下两种可能。
<p>A.假如Coordinator从所有的Cohort获得的反馈都是Yes响应，那么就会进行事务的预执行：
<p>发送预提交请求。Coordinator向Cohort发送PreCommit请求，并进入Prepared阶段。
<p>事务预提交。Cohort接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
<p>响应反馈。如果Cohort成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。

<p>B.假如有任何一个Cohort向Coordinator发送了No响应，或者等待超时之后，Coordinator都没有接到Cohort的响应，那么就中断事务：
<p>发送中断请求。Coordinator向所有Cohort发送abort请求。
<p>中断事务。Cohort收到来自Coordinator的abort请求之后（或超时之后，仍未收到Cohort的请求），执行事务的中断。

<p>3.DoCommit阶段

<p>该阶段进行真正的事务提交，也可以分为以下两种情况:
<p>执行提交
<p>
<p>A.发送提交请求。Coordinator接收到Cohort发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有Cohort发送doCommit请求。
<p>B.事务提交。Cohort接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
<p>C.响应反馈。事务提交完之后，向Coordinator发送ACK响应。
<p>D.完成事务。Coordinator接收到所有Cohort的ACK响应之后，完成事务。
<p>
<p>中断事务
<p>Coordinator没有接收到Cohort发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

<p>（2）三阶段提交协议和两阶段提交协议的不同

<li>对于协调者(Coordinator)和参与者(Cohort)都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到cohort的消息则默认失败）。
<li>在2PC的准备阶段和提交阶段之间，插入预提交阶段，使3PC拥有CanCommit、PreCommit、DoCommit三个阶段。
<li>PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。

<p>（2）三阶段提交协议的缺点

<li>如果进入PreCommit后，Coordinator发出的是abort请求，假设只有一个Cohort收到并进行了abort操作，
<li>而其他对于系统状态未知的Cohort会根据3PC选择继续Commit，此时系统状态发生不一致性。

<p> 他们两共同的缺点:在进入最后提交阶段, 多个事物提交是可能有的成功,有的失败的,
<p>解决:记录事物,手动补偿


#### 12.4 唯一id生成机制中的snowflake算法的时钟回拨问题如何解决？
<p>snowflake
<p>一串数字，用很多的二进制里的bit位，去代表不同的东西
<p>40多位都是当前机器上的时间，中间有几位是代表的是机器id，自增长的id
<p>分布式业务系统，分布在很多台机器上，这很多台机器都要生成唯一的id，不能重复，此时可以调用某台机器上的snowflake算法生成的唯一id
<p>12:00:00 500 28 1~20
<p>12:00:04 300 28 1~20
<p>12:00:05 308，当前你的snowflake算法部署的机器发生了本地时钟的回拨，时间回拨到了12:00:00 500这个时间
<p>判断是否发生了时钟回拨，当前时间比我上一次生成id的时间要小，此时就是发生了时钟回拨问题，12:00:00 500 28 1~20，snowflake算法生成的不重复的id，此时会导致生成的id是重复的，这就比较坑了
<p>比较简单容易理解的思路，当前的机器的可能会跟一台基准时间服务器进行时间校准，导致你的机器的时间本来跑的稍微快了一点，此时跟基准时间服务器进行了校准，你的时间回拨回去了，倒退回去了
<p>你在内存里把过去1个小时之内生成的每一毫秒的每台机器生成的id都在内存里保存最大的那个id
<p>12:00:00 500 28 20
<p>12:00:04 300 28 8
<p>如果发生了时钟回拨，此时你看看时钟汇报到了之前的哪一毫秒里去，直接接着在那一毫秒里的最大的id继续自增就可以了，12:00:00 500 28 21


#### 12.5 实施灰度发布的时候，网关是可以灰度了，可是Dubbo服务如何进行灰度呢？
<p> dubbo 灰度主要靠,注册中心的权重,发布新版本,少量请求,完成
<p>eruka 原生不支持,spring cloud alibaba技术栈，nacos，nacos是企业级的服务注册中心，功能还是比较强大的，灰度发布这块都是可以做到的


#### 12.6 除了常见服务注册中心之外，你觉得Redis能作为服务注册中心吗？
<li>redis里的hash数据结构，类似map的数据结构
<p> 通过hash 存值注册,设置过期时间,注册的服务监听,如果过期重新刷新值完成心跳检测
 

<li>其他数据结构的高可用注册中心,约定好，你的服务注册的key都是：service_ORDER，service_PRODUCT，scan扫描指定的前缀开头的所有的key，一次性把注册表里的几十个到几百个服务都给扫描出来，获取到一个完整的注册表
<p>基于Redis还可以做分布式服务注册中心，Redis Cluster，做一个集群


### 13 ZooKeeper

#### 13.1 我们一般到底用ZooKeeper来干什么事儿
<p>分布式架构中，分布式锁，Redis分布式锁，ZooKeeper分布式锁
<p>分布式锁：运用于分布式的Java业务系统中
<p>元数据管理：Kafka、Canal，本身都是分布式架构，分布式集群在运行，本身他需要一个地方集中式的存储和管理分布式集群的核心元数据，所以他们都选择把核心元数据放在zookeeper中的
<p>分布式协调：如果有人对zk中的数据做了变更，然后zk会反过来去通知其他监听这个数据的人，告诉别人这个数据变更了，kafka有多个broker，多个broker会竞争成为一个controller的角色
<p>如果作为controller的broker挂掉了，此时他在zk里注册的一个节点会消失，其他broker瞬间会被zk反向通知这个事情，继续竞争成为新的controller
<p>这个就是非常经典的一个分布式协调的场景，有一个数据，一个broker注册了一个数据，其他broker监听这个数据
<p>Master选举 -> HA架构
<p>HDFS，NameNode HA架构，部署主备两个NameNode，只有一个人可以通过zk选举成为Master，另外一个backup
<p>Canal，HA
<p>ZooKeeper，分布式协调系统，封装了分布式架构中所有核心和主流的需求和功能，分布式锁、分布式集群的集中式元数据存储、Master选举、分布式协调和通知

#### 13.2 有哪些开源的分布式系统中使用了ZooKeeper？
<p>Canal、Kafka、HDFS，学习过的这些技术都用了ZooKeeper，元数据管理，Master选举
<p>ZooKeeper，他主要是提供哪些功能，满足哪些需求，使用在哪些场景下，最后一句话总结，ZooKeeper到底是为什么而生的，定位是什么？
<p>三类系统
<p>第一类：分布式Java业务系统，分布式电商平台，大部分的Java开发的互联网平台，或者是传统架构系统，都是分布式Java业务系统，Dubbo、Spring Cloud把系统拆分成很多的服务或者是子系统，大家协调工作，完成最终的功能
<p>ZooKeeper，用的比较少，分布式锁的功能，而且很多人会选择用Redis分布式锁
<p>第二类：开源的分布式系统
<p>Dubbo，HBase，HDFS，Kafka，Canal，Storm，Solr
<p>分布式集群的集中式元数据存储、Master选举实现HA架构、分布式协调和通知
<p>Dubbo：ZooKeeper作为注册中心，分布式集群的集中式元数据存储
<p>HBase：分布式集群的集中式元数据存储
<p>HDFS：Master选举实现HA架构
<p>Kafka：分布式集群的集中式元数据存储，分布式协调和通知
<p>Canal：分布式集群的集中式元数据存储，Master选举实现HA架构
<p>第三类：自研的分布式系统
<p>HDFS，面向的超大文件，切割成一个一个的小块儿，分布式存储在一个大的集群里
<p>分布式海量小文件系统：NameNode的HA架构，仿照HDFS的NameNode的HA架构，做主备两个NameNode，进行数据同步，然后自动基于zk进行热切换
<p>在很多，如果你自己研发类似的一些分布式系统，都可以考虑，你是否需要一个地方集中式存储分布式集群的元数据？是否需要一个东西辅助你进行Master选举实现HA架构？进行分布式协调通知？
<p>如果你在自研分布式系统的时候，有类似的需求，那么就可以考虑引入ZooKeeper来满足你的需求


#### 13.3 为什么我们在分布式系统架构中需要使用 ZooKeeper 集群？

<p>ZooKeeper，功能和定位，满足的需求
<p>使用ZooKeeper去满足自己需求的项目都有哪些
<p>分布式集群的集中式元数据存储，Master选举实现HA架构，分布式协调和通知
<p>我们写一个类似ZK的系统，单机版本，就是部署在一台机器上面，里面提供了一些功能，比如说允许你在里面存储一些元数据，支持你进行Master选举，支持你分布式协调和通知，也可以做到
<p>单机版本的系统，万一挂掉了怎么办？
<p>集群部署，部署一个集群出来，多台机器，保证高可用性，挂掉一台机器，都可以继续运行下去
<p>3台机器
<p>我现在要进行元数据的存储，我向机器01写了一条数据，机器01应该怎么把数据同步给其他的机器02和机器03呢？
<p>自己写一个类似ZK的系统？不可能单机版本吧？肯定得集群部署保证高可用吧？一旦集群了之后，数据一致性怎么保证？多麻烦！
<p>你的分布式架构中有需求，干脆就直接用工业级，久经考验的zookeeper就可以了，bug很少，功能很全面，运用在很多工业级的大规模的分布式系统中，HDFS、Kafka、HBase

#### 13.4 ZooKeeper为了满足分布式系统的需求要有哪些特点？
<p>ZooKeeper肯定是一套系统，这个系统可以存储元数据，支持Master选举，可以进行分布式协调和通知
<p>集群部署：不可能单机版本
<p>顺序一致性：所有请求全部有序
<p>原子性：要么全部机器都成功，要么全部机器都别成功
<p>数据一致性：无论连接到哪台ZK上去，看到的都是一样的数据，不能有数据不一致
<p>高可用：如果某台机器宕机，要保证数据绝对不能丢失
<p>实时性：一旦数据发生变更，其他人要实时感知到


#### 13.5  为了满足分布式系统的需求，ZooKeeper的架构设计有哪些特点？

<p>为了实现需要的一些特性，ZooKeeper的架构设计需要有哪些特点？
<p>集群化部署：3~5台机器组成一个集群，每台机器都在内存保存了zk的全部数据，机器之间互相通信同步数据，客户端连接任何一台机器都可以
<p>树形结构的数据模型：znode，树形结构，数据模型简单，纯内存保存
<p>数据结构就跟我们的文件系统是类似的，是有层级关系的树形的文件系统的数据结构
<p>znode可以认为是一个节点而已
<p>create /usr/local/uid
<p>create /usr/local/test_file
<p>uid：可以写入一些数据的值，比如说hello world
<p>test_file：也可以写入一些数据的值
<p>顺序写：集群中只有一台机器可以写，所有机器都可以读，所有写请求都会分配一个zk集群全局的唯一递增编号，zxid，保证各种客户端发起的写请求都是有顺序的
<p>数据一致性：任何一台zk机器收到了写请求之后都会同步给其他机器，保证数据的强一致，你连接到任何一台zk机器看到的数据都是一致的
<p>高性能：每台zk机器都在内存维护数据，所以zk集群绝对是高并发高性能的，如果你让zk部署在高配置物理机上，一个3台机器的zk集群抗下每秒几万请求没有问题
<p>高可用：哪怕集群中挂掉不超过一半的机器，都能保证可用，数据不会丢失，3台机器可以挂1台，5台机器可以挂2台
<p>高并发：高性能决定的，只要基于纯内存数据结构来处理，并发能力是很高的，只有一台机器进行写，但是高配置的物理机，比如16核32G，写入几万QPS，读，所有机器都可以读，3台机器的话，起码可以支撑十几万QPS


#### 13.6  ZooKeeper集群的三种角色：Leader、Follower、Observer
<p>通常来说ZooKeeper集群里有三种角色的机器
<p>
<p>集群启动自动选举一个Leader出来，只有Leader是可以写的，Follower是只能同步数据和提供数据的读取，Leader挂了，Follower可以继续选举出来Leader，Observer也只能读但是Observer不参与选举
<p>Follower 接收到写请求,会转发到Leader,Leader去写入,然后同步数据
<p>Observer这个东西，再往后讲点，分析清楚了别的东西以后，Observer我们会回过头来讲解的

#### 13.7 客户端与ZooKeeper之间的长连接和会话是什么？
<p>zk集群启动之后，自己分配好角色，然后客户端就会跟zk建立连接，是TCP长连接
<p>也就建立了一个会话，就是session，可以通过心跳感知到会话是否存在，有一个sessionTimeout，意思就是如果连接断开了，只要客户端在指定时间内重新连接zk一台机器，就能继续保持session，否则session就超时了

#### 13.8 ZooKeeper的数据模型：znode和节点类型

<p>核心数据模型就是znode树，平时我们往zk写数据就是创建树形结构的znode，里面可以写入值，就这数据模型，都在zk内存里存放
<p>有两种节点，持久节点和临时节点，持久节点就是哪怕客户端断开连接，一直存在
<p>临时节点，就是只要客户端断开连接，节点就没了
<p>还有顺序节点，就是创建节点的时候自增加全局递增的序号
<p>大家去看一下，之前Java架构的分布式锁里，有一个zk锁的源码分析，curator框架，zk分布式锁的实现，在里面就是基于zk的临时顺序节点来实现的，加锁的时候，是创建一个临时顺序节点
<p>zk会自动给你的临时节点加上一个后缀，全局递增的，编号
<p>如果你客户端断开连接了，就自动销毁这个你加的锁，此时人家会感知到，就会尝试去加锁
<p>如果你是做元数据存储，肯定是持久节点
<p>如果你是做一些分布式协调和通知，很多时候是用临时节点，就是说，比如我创建一个临时节点，别人来监听这个节点的变化，如果我
<p>断开连接了，临时节点消失，此时人家会感知到，就会来做点别的事情
<p>顺序节点，在分布式锁里用的比较经典
<p>每个znode还有一个Stat用来存放数据版本，version（znode的版本），cversion（znode子节点的版本），aversion（znode的ACL权限控制版本）


#### 13.9 ZooKeeper最核心的一个机制：Watcher监听回调
<p>ZooKeeper最核心的机制，就是你一个客户端可以对znode进行Watcher监听，然后znode改变的时候回调通知你的这个客户端，这个是非常有用的一个功能，在分布式系统的协调中是很有必要的
<p>支持写和查：只能实现元数据存储，Master选举，部分功能
<p>分布式系统的协调需求：分布式架构中的系统A监听一个数据的变化，如果分布式架构中的系统B更新了那个数据/节点，zk反过来通知系统A这个数据的变化
<p>/usr/local/uid
<p>使用zk很简单，内存数据模型（不同节点类型）；写数据，主动读取数据；监听数据变化，更新数据，反向通知数据变化
<p>实现分布式集群的集中式的元数据存储、分布式锁、Master选举、分布式协调监听

#### 13.10 一个关键的问题：zk到底通过什么协议在集群间进行数据一致性同步？

<p>在整个zk的架构和工作原理中，有一个非常关键的环节，就是zk集群的数据同步是用什么协议做的？其实用的是特别设计的ZAB协议，ZooKeeper Atomic Broadcast，就是ZooKeeper原子广播协议
<p>原子性
<p>通过这个协议来进行zk集群间的数据同步，保证数据的强一致性
<p>zk，就说他的一致性的问题，很多同学可能不理解zk的一致性

#### 13.11 ZAB的核心思想介绍：主从同步机制和崩溃恢复机制

<p>协议的本质而言，划分集群角色，使用主从架构，Leader和Follower两种角色
<p>只有Leader可以接受写操作，Leader和Follower都可以读，Leader收到事务请求，转换为事务Proposal（提议）同步给所有的
<p>Follower，超过半数的Follower都说收到事务proposal了,Follower发送Ack消息，Leader再给所有的Follower发一个Commit消息，让所有Follower提交一个事务
<p>而且如果Leader崩溃了，要重新选举Leader保证继续运行
<p>角色划分，2PC（两阶段），过半写机制

#### 13.12 从zk集群启动到数据同步再到崩溃恢复的ZAB协议流程

<p>zk集群启动的时候，进入恢复模式，选举一个leader出来，然后leader等待集群中过半的follower跟他进行数据同步，只要过半follower完成数据同步，接着就退出恢复模式，可以对外提供服务了
<p>只要有超过一半的机器，认可你是leader，你就可以被选举为leader
<p>3台机器组成了一个zk集群，启动的时候，只要有2台机器认可一个人是Leader，那么他就可以成为leader了
<p>3台可以容忍不超过一半的机器宕机，1台
<p>剩余的2台机器，只要2台机器都认可其中某台机器时leader，2台 > 一半，就可以选举出来一个leader了
<p>zk的leader选举算法，我们可以在后面的zk核心源码剖析的时候
<p>1台机器时没有办法自己选举自己的
<p>5台机器，3台机器认可某个人是leader；可以允许2台机器宕机，3台机器，leader选举，只要是5台机器，一半2.5，3台机器都认可某个人是leader，此时3 > 2.5，过半，leader是可以选举出来的
<p>2台机器，小于一半，没有办法选举新的leader出来了
<p>当然还没完成同步的follower会自己去跟leader进行数据同步的
<p>此时会进入消息广播模式
<p>只有leader可以接受写请求，但是客户端可以随便连接leader或者follower，如果客户端连接到follower，follower会把写请求转发给leader
<p>leader收到写请求，就把请求同步给所有的follower，过半follower都说收到了，就再发commit给所有的follower，让大家提交这个请求事务
<p>如果突然leader宕机了，会进入恢复模式，重新选举一个leader，只要过半的机器都承认你是leader，就可以选举出来一个leader，所以zk很重要的一点是主要宕机的机器数量小于一半，他就可以正常工作
<p>因为主要有过半的机器存活下来，就可以选举新的leader
<p>新leader重新等待过半follower跟他同步，完了重新进入消息广播模式
<p>集群启动：恢复模式，leader选举（过半机器选举机制） + 数据同步
<p>消息写入：消息广播模式，leader采用2PC模式的过半写机制，给follower进行同步
<p>崩溃恢复：恢复模式，leader/follower宕机，只要剩余机器超过一半，集群宕机不超过一半的机器，就可以选举新的leader，数据同步

#### 13.13 采用了2PC两阶段提交思想的ZAB消息广播流程

<p>每一个消息广播的时候，都是2PC思想走的，先是发起事务Proposal的广播，就是事务提议，仅仅只是个提议而已，各个follower返回ack，过半follower都ack了，就直接发起commit消息到全部follower上去，让大家提交
<p>发起一个事务proposal之前，leader会分配一个全局唯一递增的事务id，zxid，通过这个可以严格保证顺序
<p>leader会为每个follower创建一个队列，里面放入要发送给follower的事务proposal，这是保证了一个同步的顺序性
<p>每个follower收到一个事务proposal之后，就需要立即写入本地磁盘日志中，写入成功之后就可以保证数据不会丢失了，然后返回一个ack给leader，然后过半follower都返回了ack，leader推送commit消息给全部follower
<p>leader自己也会进行commit操作
<p>commit之后，就意味这个数据可以被读取到了

#### 13.14 ZooKeeper到底是强一致性还是最终一致性？

<p>强一致性：只要写入一条数据，立马无论从zk哪台机器上都可以立马读到这条数据，强一致性，你的写入操作卡住，直到leader和全部follower都进行了commit之后，才能让写入操作返回，认为写入成功了
<p>此时只要写入成功，无论你从哪个zk机器查询，都是能查到的，强一致性
<p>明显，ZAB协议机制，zk一定不是强一致性
<p>最终一致性：写入一条数据，方法返回，告诉你写入成功了，此时有可能你立马去其他zk机器上查是查不到的，短暂时间是不一致的，但是过一会儿，最终一定会让其他机器同步这条数据，最终一定是可以查到的
<p>研究了ZooKeeper的ZAB协议之后，你会发现，其实过半follower对事务proposal返回ack，就会发送commit给所有follower了，只要follower或者leader进行了commit，这个数据就会被客户端读取到了
<p>那么有没有可能，此时有的follower已经commit了，但是有的follower还没有commit？绝对会的，所以有可能其实某个客户端连接到follower01，可以读取到刚commit的数据，但是有的客户端连接到follower02在这个时间还没法读取到
<p>所以zk不是强一致的，不是说leader必须保证一条数据被全部follower都commit了才会让你读取到数据，而是过程中可能你会在不同的follower上读取到不一致的数据，但是最终一定会全部commit后一致，让你读到一致的数据的
<p>zk官方给自己的定义：顺序一致性
<p>因此zk是最终一致性的，但是其实他比最终一致性更好一点，出去要说是顺序一致性的，因为leader一定会保证所有的proposal同步到follower上都是按照顺序来走的，起码顺序不会乱
<p>但是全部follower的数据一致确实是最终才能实现一致的
<p>如果要求强一致性，可以手动调用zk的sync()操作

#### 13.15 ZAB协议下一种可能存在的数据一致性问题
<p>Leader收到了过半的follower的ack，接着leader自己commit了，还没来得及发送commit给所有follower自己就挂了，这个时候相当于leader的数据跟所有follower是不一致的，你得保证全部follower最终都得commit
<p>
<p>另外一个，leader可能会自己收到了一个请求，结果没来得及发送proposal给所有follower之前就宕机了，此时这个Leader上的请求应该是要被丢弃掉的
<p>
<p>所以在leader崩溃的时候，就会选举一个拥有事务id最大的机器作为leader，他得检查事务日志，如果发现自己磁盘日志里有一个proposal，但是还没提交，说明肯定是之前的leader没来得及发送commit就挂了
<p>
<p>此时他就得作为leader为这个proposal发送commit到其他所有的follower中去，这个就保证了之前老leader提交的事务已经会最终同步提交到所有follower里去
<p>
<p>然后对于第二种情况，如果老leader自己磁盘日志里有一个事务proposal，他启动之后跟新leader进行同步，发现这个事务proposal其实是不应该存在的，就直接丢弃掉就可以了

#### 13.16 崩溃恢复时选举出来的Leader是如何跟其他Follower进行同步的？

<p>新选举出来一个leader之后，本身人家会挑选已经收到的事务zxid里最大的那个follower作为新的leader。
<p>5个机器，1leader + 4个follower
<p>1个leader把proposal发送给4个follower，其中3个folower（过半）都收到了proposal返回ack了，第四个follower没收到proposal
<p>此时leader执行commit之后自己挂了，commit没法送给其他的follower，commit刚发送给一个follower
<p>剩余的4个follower，只要3个人投票一个人当leader，就是leader
<p>
<p>假设那3个收到proposal的follower都投票第四台没有收到proposal的follower当心的leader？这条数据一定永久性丢失了
<p>
<p>选择一个拥有事务zxid最大的机器作为新Leader
<p>
<p>其他的follower就会跟他进行同步，他给每个follower准备一个队列，然后把所有的proposal都发送给follower，只要过半follower都ack了，就会发送commit给那个follower
<p>
<p>所谓的commit操作，就是把这条数据加入内存中的znode树形数据结构里去，然后就对外可以看到了，也会去通知一些监听这个znode的人
<p>
<p>如果一个follower跟leader完全同步了，就会加入leader的同步follower列表中去，然后过半follower都同步完毕了，就可以对外继续提供服务了


#### 13.17 对于需要丢弃的消息是如何在ZAB协议中进行处理的？
<p>每一条事务的zxid是64位的，高32位是leader的epoch，就认为是leader的版本吧；低32位才是自增长的zxid
<p>老leader发送出去的proposal，高32位是1，低32位是11358
<p>
<p>如果一个leader自己刚把一个proposal写入本地磁盘日志，就宕机了，没来得及发送给全部的follower，此时新leader选举出来，他会的epoch会自增长一位
<p>
<p>proposal，高32位是2，低32位是继续自增长的zxid
<p>
<p>然后老leader恢复了连接到集群是follower了，此时发现自己比新leader多出来一条proposal，但是自己的epoch比新leader的epoch低了，所以就会丢弃掉这条数据
<p>
<p>启动的时候，过半机器选举leader，数据同步
<p>
<p>对外提供服务的时候，2PC + 过半写机制，顺序一致性（最终的一致性）
<p>
<p>崩溃恢复，剩余机器过半，重新选举leader，有数据不一致的情况，针对两种情况自行进行处理，保证数据是一致的（磁盘日志文件、zxid的高32位）

#### 13.18 现在再来看看ZooKeeper的Observer节点是用来干什么的？
<p>Observer节点是不参与leader选举的，他也不参与ZAB协议同步时候的过半follower ack的那个环节，他只是单纯的接收数据，同步数据，可能数据存在一定的不一致的问题，但是是只读的
<p>
<p>leader在进行数据同步的时候，observer是不参与到过半写机制里去
<p>
<p>所以大家思考一个问题了
<p>
<p>zk集群无论多少台机器，只能是一个leader进行写，单机写入最多每秒上万QPS，这是没法扩展的，所以zk是适合写少的场景
<p>
<p>但是读呢？follower起码有2个或者4个，读你起码可以有每秒几万QPS，没问题，那如果读请求更多呢？此时你可以引入Observer节点，他就只是同步数据，提供读服务，可以无限的扩展机器

#### 13.19 ZooKeeper为什么只能是小集群部署？为什么适合读多写少场景？
<p>Eureka，peer-to-peer架构，master-slave
<p>
<p>小集群部署，每个节点收到的注册、心跳所有的信息，都必须向其他节点都进行同步，有很大的问题，他在进行同步的时候，采取的是完全的一个异步同步的机制，不管什么2PC，异步慢慢同步就可以了
<p>时效性是很差的，eureka，这个技术不适合大公司，大厂的场景去使用
<p>
<p>现在第二个问题，为什么zk的leader和follower只能是三五台机器，小集群部署？因为你想，假设你有1个leader + 20个follower，21台机器，你觉得靠谱吗？不靠谱，因为follower要参与到ZAB的写请求过半ack里去
<p>
<p>如果你有20个follower，一个写请求出去，要起码等待10台以上的Follower返回ack，才能发送commit，才能告诉你写请求成功了，性能是极差的
<p>所以zk的这个ZAB协议就决定了一般其实就是1个leader + 2个follower的小集群就够了，写请求是无法扩展的，读请求如果量大，可以加observer机器，最终就是适合读多写少的场景
<p>主要就是用于分布式系统的一些协调工作
<p>这也就让大家知道了，很多互联网公司里，不少系统乱用zk，以为zk可以承载高并发写，结果每秒几万写请求下去，zk的leader机器直接可能就挂掉了，扛不住那么大的请求量，zk一旦挂掉，连带的kafka等系统会全部挂掉
<p>
<p>zk适合读多写少的，zk集群挂掉了
<p>
<p>leader写入压力过大， 最终导致集群挂掉了，对一个公司的技术平台是有重大打击的，hbase、kafka之类的一些技术都是强依赖zk的，dubbo + zk去做服务框架的话，有上万甚至几十瓦的服务实例的时候
<p>大量的服务的上线、注册、心跳的压力，达到了每秒几万，甚至上十万，zk的单个leader写入是扛不住那么大的压力的
<p>一般适合写比较少
<p>读比较多，observer节点去线性扩展他的高并发读的能力


#### 13.20 对ZooKeeper特性的总结

<p>集群模式部署
<p>
<p>一般奇数节点，因为你5台机器可以挂2台，6台机器也是挂2台，不能超过一半的机器挂掉，所以5台和6台效果一致，那奇数节点可以减少机器开销，小集群部署，读多写少
<p>
<p>主从架构：Leader、Follower、Observer（一般刚开始没必要用）
<p>
<p>内存数据模型：znode，多种节点类型
<p>
<p>客户端跟zk进行长连接，TCP，心跳，维持session
<p>
<p>zxid，高32位，低32位
<p>
<p>ZAB协议，2PC，过半ack + 磁盘日志写，commit + 写内存数据结构
<p>
<p>支持Watcher机制，监听回调通知
<p>
<p>顺序一致性：消息按顺序同步，但是最终才会一致，不是强一致
<p>
<p>高性能，2PC中的过半写机制，纯内存的数据结构，znode
<p>
<p>高可用，follower宕机没影响，leader宕机有数据不一致问题，新选举的leader会自动处理，正常运行，但是在恢复模式期间，可能有一小段时间是没法写入zk的
<p>
<p>高并发，单机leader写，Observer可以线性扩展读QPS

### 14  系统设计优化

<li>（1）说说高并发场景下的数据库连接池应该如何进行优化？
<li>（2）如果压测的时候发现系统的TPS不达标，此时应该如何优化系统？
<li>（3）说说你对NoSQL的理解以及他的优缺点分别都是什么？
<li>（4）假设让你来负责微信朋友圈这样的社交系统，应该如何设计？
<li>（5）微信朋友圈是如何对好友显示权限进行控制的？
<li>（6）如何设计高并发的朋友圈点赞系统架构？
<li>（7）如何保证刚发的微信朋友圈被人点赞后，自己能及时的看到？
<li>（8）在你的朋友圈点赞系统，如何防止用户连续点击几次进行重复点赞？
<li>（9）设想你负责一个系统，此时某个核心服务如果挂掉，该怎么处理？
<li>（10）如果你们公司的Nginx故障挂掉了，应该如何做才能保证他的高可用性？
<li>（11）如果让你来设计12306售票系统，应该如何设计？
<li>（12）如果让你来设计一个电商场景下的秒杀系统，应该如何设计？

#### 14.1     说说高并发场景下的数据库连接池应该如何进行优化？

<p>（1）maxWait
<p>表示从池里获取连接的等待时间，万一你暂时没有可用的连接，就可能要等待别的连接用完释放，你再去使用，通常建议设置在1000以上，就是等待1s以上，比如你可以设置1200，因为有的时候要等待建立新的TCP连接，最多在1s内，那你就得等一会儿
<p>如果这个参数默认设置为0，意思就是无限的等待获取连接，在高并发场景下，可能瞬间连接池耗尽，大量的请求都卡死在这里等待获取连接，进而导致你tomcat里没有可用的线程，服务就是一个假死的样子
<p>你还会拖累调用你的其他服务，其他服务都卡死在调用你的请求上，可能会导致整体系统大量服务的雪崩
<p>你设置一个靠谱点的参数，那么起码大量线程获取不到连接，1s左右快速就失败了，这个时候还不至于说拖死整个服务，也不至于说拖死其他调用你的服务，还不至于会发生服务雪崩的问题
<p>
<p>（2）connectionProperties
<p>里面可以放connectionTimeout和socketTimeout，分别代表建立TCP连接的超时时间，以及发送请求后等待响应的超时时间，推荐connectionTimeout设置为1200，socketTimeout设置为3000
<p>之所以必须设置他们俩，是因为高并发场景下，万一遇到网络问题，可能会导致你跟数据库的Socket连接异常无法通信，此时你Socket可能一直卡死等待某个请求的响应，然后其他请求无法获取连接，只能是重启系统重新建立连接才行
<p>所以设置一下超时时间，可以让网络异常之后，连接自动超时断开重连
<p>
<p>（3）maxActive
<p>
<p>最大连接池数量，一般建议是设置个20就够了，如果确实有高并发场景，可以适当增加到3~5倍，但是不要太多，其实一般这个数字在几十到100就很大了，因为这仅仅是你一个服务连接数据库的数量，你数据库整体能承受的连接数量是有限的
<p>而且连接越多不是越好，数据库连接太多了，会导致cpu负载很高，可能反而会导致性能降低的，所以这个参数你一般设置个20，最多加到个几十，其实就差不多了
<p>更多的，你反而应该是优化你每个请求的性能，别让一个请求占用连接太长的时间

#### 14.2 如果压测的时候发现系统的TPS不达标，此时应该如何优化系统？
<p>对系统进行压测，比如每秒压个几百请求到几千请求，甚至上万请求，此时发现死活压不上去，压来压去，你的系统最多每秒就处理几百个请求，根本到不了几千个请求，此时就发现系统的TPS不达标，此时如何优化？
<p>
<p>其实这个时候，如果发现TPS不达标，通常是说明你系统肯定是每个请求处理时间太长了，所以就导致你单位时间内，在有限的线程数量下，能处理的TPS就少了，这个时候往往要先优化性能，再提TPS
<p>
<p>假设你一共有200个线程，结果你每个请求要耗费500ms，每个线程每秒就只能处理2个请求，200个线程每秒只能处理400个请求，比你期望的单机处理500~600个请求，要少了很多
<p>
<p>既然说要优化性能，那就得通过打日志的方式，或者是监控的方式，检查你服务的每个环节的性能开销，通常来说用打日志方式会细化一些，要靠监控把每个细节摸清楚，也挺难的，毕竟很多是代码细节
<p>
<p>把你的系统里一个请求过来，每一次数据库、缓存、ES之类的操作的耗时都记录在日志里面，把你的每个请求执行链路里的每个耗时小环节，都给记录清楚他，比如说你一个请求过来一共500ms，此时你发现就是某个SQL语句一下子耗时了300多ms，其实其他的操作都在正常范围内
<p>
<p>优化一下SQL语句呢？这个SQL语句搞了一个全表扫描，因为写SQL的时候没有考虑到使用索引，所以此时可以建立新的索引，或者是改写SQL语句，让他可以使用到你建立好的索引，SQL语句优化到100ms
<p>
<p>每个请求只要300ms就可以了，每个线程每秒可以处理3个请求，200个线程每秒可以处理600个请求
<p>
<p>你可以检查你核心服务的每个环节的性能，针对性的做优化，把你每个请求的时间降到最低，这样你单位时间内的TPS绝对就提高了，这个很关键
<p>
<p>其次就是增加机器数量，线性扩容了，比如说服务层面，每个服务单机最多抗800请求，那扩容到部署10台机器，就可以抗8000请求，但是你又得考虑你依赖的数据库，MQ，Redis能不能抗下这么多的并发

### 14.3 Nosql
#### 14.3.1 为什么有了HDFS之后，还需要HBase呢？
<p>hdfs，对hdfs是什么，功能，架构，源码，如何二次开发，都有了一定的理解，Java架构部分的jdk集合、并发、IO、网络，自研分布式海量小文件系统的项目，最好去先做一下
<p>
<p>hdfs设计主要是针对什么呢？针对的是大数据，超大文件，比如说你有一个超大文件，里面要放100GB的用户行为的日志，甚至是1TB，甚至1PB，针对一个网站的每天的用户行为的日志，可以都放一个超大文件里去
<p>超大文件很难说放在一台服务器上，所以说此时，可以把超大文件拆散，拆成N多个128MB的小文件，每一个小文件就可以说是这个大文件的一个block
<p>
<p>hdfs解决的主要是一个分布式存储的问题，也就是说你有超大数据集，不可能都放在一个文件里，是不现实的，所以可以拆分为N多个128MB的block小文件，分散存储在多台机器上，对超大数据集实现分布式存储的效果
<p>
<p>每个block小文件还有3副本冗余存储，每个副本在不同的机器上，高可用和高容错，任何一台机器挂掉，不会导致数据丢失的
<p>
<p>hdfs，hadoop distributed filesystem，分布式文件系统，他存放的是文件，文件死的，静态的，最多只能是你不停的往文件的末尾追加数据，他会把你追加到文件末尾的数据其实都是分散在不同的小block里存储在机器上
<p>
<p>目录层级结构，创建文件，管理权限，对文件进行删除，大概就是这样的一些事情了，对大文件里的数据进行读取，对文件进行数据追加，hdfs只能做到如上一些事情
<p>
<p>针对你hdfs上存储的海量数据，10TB的数据，我要进行增删改查，我要往里面插入数据，还要修改数据，还有删除里面某一行数据，还有精准的查询里面某一行数据，得了，hdfs上是大量的block小文件
<p>
<p>虽然说帮你把超大数据集给分布式存储了，现实吗？根本就不现实
<p>
<p>所以呢，当当当当，hbase出马了，由hbase基于hdfs进行超大数据集的分布式存储，让海量数据分布式存储在hdfs上，但是对hdfs里的海量数据进行精准的某一行，或者某几行的数据的增删改查，由hbase来解决了
<p>hadoop nosql database 

#### 14.3.2 到底为什么把 HBase 叫做NoSQL数据库呢？
hdfs可以解决我们的一些问题，超大数据集的分布式存储，hbase作为hadoop nosql database，来解决海量数据的增删改查的问题

到底什么是nosql，跟sql相对应

hbase主要是就是能够帮助你对海量数据进行增删改查，跟sql是相反的，关系型数据库，mysql/oracle，一般来说都是基于SQL语法让你实现复杂的一些SQL语句，还可以支持事务，主要是开发业务系统的

比如说你有一些需求，是要对10TB的数据，对他们进行相对较为简单的增删改查，比如说插入一行数据，查询一行数据，根据一些简单的条件查询某几行数据，删除一行数据，更新一行数据

hbase就可以搞定，专职就是干这个的，分布式nosql数据库就可以了，他是跟关系型数据库相反的，是不支持SQL语句的，nosql，没有SQL语句的支持，没有SQL的数据库，帮你对海量数据做简单增删改查的

nosql数据库，一般都是分布式的，解决海量数据的简单增删改查问题的，如果你要是针对少量数据做简单增删改查，也不需要nosql，其实你用mysql/oracle天然就可以搞定少量数据的增删改查

天生不擅长sql，所以不要强行在上面用sql，就是做一些简单的增删改查就可以了

很多场景是需要对海量数据做基础的增删改查，不需要复杂的sql语法支持，那么天然可以用hbase，海量数据可以存储，分布式的nosql支持

#### 14.3.3 HBase作为一个NoSQL数据库，有哪些架构上的特点？
（1）分布式架构

hbase定位是分布式nosql数据库，把自己的nosql数据库的功能是通过多台机器来实现的，有多个RegionServer，分布式管理数据，分布式执行你的各种nosql数据库的操作

（2）分布式数据存储和自动数据分片

这个功能是极为强大的，比如你搞一个hbase里的表，然后在表里搞很多很多的数据，这个表会分为很多的region，每个region里是一个数据分片，然后这些region数据分片就会分散在多台机器上

假设你的表里的数据太多了，此时region会自动进行分裂，分裂成更多的region，自动分散在更多的机器上，对我们使用是极为方便的

（3）集成hdfs作为分布式文件存储系统


（4）强一致读写

   他不是zk那种最终一致性，是强一致的，你写成功了立马就可以读。这个功能是极为实用的，他是依靠的分布式存储才做到的，zk那种是属于主从同步，你读follower机器是可能读到不一致数据的

（5）高可用

每台机器上部署一个RegionServer，管理一大堆的region数据分片，RegionServer都是支持高可用的，一个RegionServer挂掉不会导致数据丢失，他自动可以由别的机器接管他的工作运行下去

（6）支持mapreduce/spark这种分布式计算引擎

对hbase里的数据进行分布式计算，可以从hbase里分布式抽数据去计算，也可以把计算后的结果写入hbase分布式存储

（7）Java API/thrift API/REST API的支持

当然支持Java API了，咱们的Java业务系统经常会有海量数据NoSQL存储的需求，此时就可以基于Java API来操作hbase里的数据了

（8）支持协处理器，块缓存和布隆过滤器，可以用于优化查询性能

（9）hbase现在最新版本都是支持web界面的方式来对hbase集群进行运维管理的


#### 14.3.4 HBase作为NoSQL数据库，到底适用于哪些场景？

（1）海量数据场景

表来形容，单表在千万以内级别的数据量，基本都是小数据，千万级别的数据量，最多只能说是中等数据量，MySQL搞一下分库分表，搞个两三台服务器，就可以轻松抗住千万级别的数据量的表了，每个表可能也就几万条数据了

基于分库分表的中间件，mycat、sharding-sphere，都可以的，直接做一些路由什么的，就可以轻松搞定几千万级别的数据了，性能也是很高的

假设几千万条数据是过去历史几年下来积累的，每年积累一千万数据，每个月也就100万数据左右，每一天几万数据量，10年才1亿数据，MySQL分库分表的技术方案，抗下小亿级别的数据量都是ok的，一两亿数据

可能你作为这个系统的负责人，在可见的范围内，基本上单表撑死也就几千万到一两亿级别，10年、20年以后了，不用考虑这么多了，其实像这种级别的存量和增量的数据量，用MySQL分库分表就可以轻松搞定了

要做一些跨库跨表的SQL，不太好做，可以自己查询一些数据放到内存来做定制计算也是可以的

什么叫做海量数据？说不好听的，假设你就几百万数据，用MySQL就可以轻松高兴了，要是你有几千万数据呢？基本到MySQL的瓶颈和极限了。要是你有几亿条数据呢？而且每天数据量还在不停的增长呢？

这个时候你就可以使用hbase了，他天生就是分布式的，可以扩容很方便，数据分布式存储，自动数据分片，完善的运维管理，底层集成hdfs做分布式文件系统，增删改查的nosql功能都支持，你几亿到几十亿的数据放里面很适合，而且数据还一直在增长

绝对比mysql分库分表要来的方便的多

（2）只需要简单的增删改查的支持

你对海量的数据仅仅就是简单的增删改查的支持，绝对没有MySQL那种关系型数据库支持的列类型、索引、事务、SQL语法，那么多高阶的特性，你要是不需要索引、SQL和事务，那妥妥的用hbase就可以了

虽然hbase之上有很多开源组件，可以搞二级索引、phoniex可以支持SQL，但是说实话，真的没必要，人家hbase就不是干这个的，麻烦大家别折腾他好吗

你要海量数据下支持事务，可以用分布式数据库，比如TiDB；你要海量数据下支持复杂SQL实时分析，可以用clickhouse，或者是druid之类的

#### 14.3.5 HBase的数据模型是什么样的？

hbase里其实也是建一个一个的表

表里有很多行的数据，但是其实这个表说白了就是一个逻辑模型，物理上根本没那么简单的，一个表的数据当然是拆为很多region分散在不同的机器上的，要是表里数据太多了，region数量还会变多，这样你加更多机器，region可以自动迁移到不同的机器上去

每一行都有一个rowkey，还有很多列，表里的数据行都是按照rowkey排序的，大致可以把rowkey理解为mysql里的主键id，在hbase里每一行数据都有一个rowkey行健来唯一的标识一行数据

所以一般设计rowkey是一门讲究活，后续还会讲如何设计rowkey的，因为一般要把同一类数据的rowkey设计的相似一些，比如说用户id=1的订单，就应该叫做order_1_xx之类的，这样一个用户的订单就会在排序之后靠近在一起
 
````
rowkey                 order:base            order:detail           order:extent

order_1_110          xxx                        xxx
order_1_111          x1(t1); x2(t2)          xxx                        xxx
````

每一行数据都有一些列族，就是column family，每个列族都包含一些列，每个列族都有一系列的存储属性，比如说是否把列族里的列值缓存在内存里，列族里的数据如何进行压缩，类似这种

一个表里有固定的一些列族，每一行都有这些列族，当然有可能你一行数据在某个列族里没存什么东西，是有可能的

然后就是列，每个列就是一个列族+分号+列限定符（column qualifier），比如说列族是order，列可能就是order:base，或者是order:detail

每个表的列族是固定的，但是每一行数据有哪些列是不固定的，插入数据的时候可以动态可以给这行数据设定多个列，每个列都是属于一个列族，就是一个列族+分号+列限定符的形式，就可以确定一个列

时间戳，timestamp，每一行的每个列的值写入的时候就会有一个时间戳，时间戳就代表了这一行这个列的某个版本的值，当然这个timestamp你也可以自己插入的时候指定一个timestamp也是ok的

单元格，也就是cell，其实就是一行的某个列族下的某个列（由列限定符来确定）的某个timestamp版本对应的值，说白了就这么个东西，在hbase里，每一行的每个列的值，是有多个版本的，每个版本都是一个cell

#### 14.3.6 HBase的物理存储格式：为啥说他是列式存储？
````
rowkey                 order:base            order:detail           order:extent

order_1_110          xxx(t3)                   xxx(t4)

order_1_111          x1(t1); x2(t2)          xxx(t5)                   xxx(t6)
 ````
 

hbase，列式存储的一个系统，他不是说按一行一行的格式来进行存储的，按列来进行存储的
 ````
rowkey          timestamp     列                  值

order_1_110   t3                  order:base     xxx
order_1_110   t4                  order:detail    xxx
order_1_111   t1                  order:base     x1
order_1_111   t2                  order:base     x2
order_1_111   t5                  order:detail    xxx
order_1_111   t6                  order:extent   xxx
````

### 14.4  假设让你来负责微信朋友圈这样的社交系统，应该如何设计？

你自己可以发朋友圈，刷朋友，看到你自己发的朋友圈以及你的好友发的朋友圈，你可以对朋友圈进行点赞，进行评论，你可以去设置
权限，你不看某些人的朋友圈，不让某些人看你的朋友圈，拉黑或者删除某个好友再也不用看到他的朋友圈了

首先你发送朋友圈的时候，一般是9张图片配合一些文字，组成了一条朋友前，文字还好说，但是图片就会稍微有点大了，也可以是一
个短视频配合一些文字，点击发送，假设你要同步发送，可能会导致你点击发送按钮之后，弹出一个旋转框，告诉你发送中，持续好几
秒种中，用户体验是比较差的

有一个好一点的办法，可以把这些数据在客户端本地暂存一下，然后直接让你发送成功返回，走一个异步发送状态，然后立马让你自己

在刷朋友圈的时候，可以把你客户端本地的刚发的朋友圈加载出来看到

仅仅是这样，就会变成，发朋友圈成了你自己的自娱自乐，因为你的朋友圈并没有发送出去让你的好朋友看到，可以走一个异步的模
式，把你的朋友圈里的图片或者视频+文字，花费几秒钟的时间传送到你的后台服务器上去存储

之后，你的朋友就可以从后台服务器上加载你的朋友圈里的图片和视频，可以看到了你的那些视频和图片，是不是可以就都直接就近上传到CDN（content delivery network），不是直接到朋友圈系统的后台，这样速度
是很快的；接着就是发送请求到朋友圈后端系统，请求包括图片的地址，你配的文字，发朋友圈的时候可以选择开放给谁看，这些数据
写入到朋友圈发布表里去

然后需要在相册表里写入索引数据，里面存放的是对你的发布表里的数据引用，这样你以后浏览相册的时候，都是根据相册里的索引数
据到发布表里找实际对应的数据的

接着就走一个离线批处理，通过批处理程序把这条朋友圈写入到你所有好友的时间线表里去，你好友的时间线表里就是存放了他刷朋友
圈的时候，可以按照时间线刷到的所有好友的朋友圈

你有3个好朋友，每个好朋友都在时间线表里有一个朋友圈的时间线，按照时间顺序排列了他可以查看的所有朋友圈，包括了他自己发
的朋友圈以及他的好友发的朋友圈允许他看的那些，都会在这里

然后你的好朋友刷朋友圈的时候，就会知道自己的时间线表里有个新的变化，就是有好友发了朋友圈，此时就会提示你一个红色的圆
点，你就开始刷，刷的时候就根据图片url地址，去cdn拉取的

#### 14.4.1 微信朋友圈是如何对好友显示权限进行控制的？
发送朋友圈的时候，可以通过几种方式进行谁可以看你这条朋友圈的权限的控制，你发的时候可以选择屏蔽谁，对哪个标签下的人开放

这条朋友圈的权限到了后台之后，会有一个离线批处理的程序跑起来，对最近发的一波朋友圈都找他们的朋友圈的权限的设置看一下，

此时就会对你允许看到的好友，此时就在他们的时间线里插入这条朋友圈数据，那么这样的话，只有你允许的好友的时间线里才有你这条朋友圈

比如说王五发的朋友圈16931可以允许张三和李四看到，设置了一个标签组，标签名称是老铁三人组，里面就正好有张三和李四
 ````
张三 发表朋友圈的时间戳 朋友圈16931 王五
张三 发表朋友圈的时间戳 朋友圈16384 李四
李四 发表朋友圈的时间戳 朋友圈16931 王五
````
在redis里可以设置张三的朋友圈是有变动的一个状态，在上次拉取朋友圈的时间点之后的一些朋友圈都从时间线表里拉取出来，刷朋
友圈的时候，如果说你的网速要是不太好的话，你会发现这样一个场景

就是你最新的一些朋友发的朋友圈是显示出来了，但是视频和图片都是一片灰色，仅仅能看到他的文字和其他的一些东西，比如说点赞
之类的，图片和视频死活看不到，都是一片灰色，反正我自己网速不好的时候经常看到这样的情况

假设王五之前发了一条朋友圈，设置李四可以看到的，李四之前确实是看到了这条朋友圈的，但是有个问题，王五后来跟李四吵了一
架，关系变得非常的不好，王五就对李四设置了一个朋友圈的权限，就是自己的朋友圈不允许李四看到，甚至可能会直接拉黑/删除李
四这个好友，这个就够狠了

你设置自己的朋友圈对所有朋友都是仅仅三天之内可见
就是说你跟李四之间的朋友圈的权限总设置或者是朋友之间的关系，有了变化，或者是你的自己的朋友圈对外展示的总权限有了变化，
此时每次如果有变动，那么这些设置，包括你对每个朋友的朋友圈权限的设置，跟朋友的关系，自己的朋友圈的总权限，这些设置都会
统统的缓存起来

包括缓存在你自己的客户端本地，也可以缓存在你的朋友的客户端本地
但是你可能随时会拉黑、删除某个人，或者是突然设置对那个人朋友圈不可见，或者是突然你自己设置了朋友圈三天可见什么的，所以
你设置的这些东西，都会被缓存起来，每次你好友刷朋友圈，查看自己的时间线表的时候，都会检查你的某条朋友圈根据你的一些行
为，是否还对他可见

李四会关注王五的各种朋友圈权限和朋友关系的一个变化，一旦说有变化了，可以缓存到自己的本地，下次在客户端里再次刷新朋友圈
的时候，

客户端对于王五的朋友圈会和王五的各种权限设置结合起来判断一下李四能否看到王五的这条朋友圈

一般那些操作很少做的，所以做的时候更新一下缓存就行了  一般问题不大

#### 14.4.2  如何设计高并发的朋友圈点赞系统架构？

我看到了你的朋友圈，此时我就可以对你的朋友圈去进行一个点赞，也可以取消点赞，假设要设计成支撑高并发的点赞系统，应该如何设计？

朋友圈的点赞和评论，是独立的数据，其实比如点赞，都是可以基于redis来做的，每个朋友圈里对应一个set数据结构，里面放谁给你点赞了，这样每条朋友圈的点赞人和点赞数量直接从redis出就可以了，smembers和scard

评论也是可以存表里的，都是以朋友圈为粒度来存储

那么刷朋友圈的时候，比如说你好友和你，另外一个好友都是好友，此时你好友刷到了你的朋友圈，就可以把另外一个好友对你的点赞和评论都拉出来，展示在客户端下面就可以了，这个展示过程可以是动态的

你是王五，你的朋友圈被张三点赞了，李四跟你们也是好朋友，此时李四刷朋友圈看到了王五发的这条朋友圈，此时你可以在后台，对这条朋友圈的set用张三做一个sismember操作，就是判断一下你们俩的所有共同好友，有哪些人对这条朋友圈点赞了

此时就可以看出来这条朋友圈被你们的共同好友多少人点赞了，哪些人点赞了

比如你另外一个好友是否对这条朋友圈点赞了，直接sismember就可以判断出来，这样整个你基于redis，他都是非常高性能的


#### 14.4.3 关于重复点赞问题以及点赞查看时效性的方案设计

redis 的set 是自动去重的,


#### 14.5 ThreadLocal内存泄漏问题
ThreadLocal这个东西为什么会有内存泄漏

我们的每个线程可以通过THreadLocal来存取自己线程专属的一个变量副本，ThreadLocalMap，Key-Value，Key是WeakReference，弱引用，value就是你自己放的变量副本

比如说你的线程长期存活，ThreadLocal里会一直有你这个线程的key-value对，万一说出现一些内存不够的情况，进行了gc，此时就会自动把很多线程在ThreadLocal里存放的key-value对的key，弱引用，都会进行回收

>null -> value

JDK团队都有解决的方案了，你在通过ThreadLocal，set、get、remove，他会自动清理掉map里值为null的key，确保不要有很多的null值引用了你的value造成内存的泄漏问题，这个就是一个他的自己的解决方案

你不要老是让一个长期存活的线程，线程池里的线程，要不然可能是你自己开启的线程在后台长期运行，尽量避免在ThreadLocal长期放入数据，你不使用的时候最好及时的进行remove，自己主动把数据给删除了




