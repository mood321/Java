
> 从0开始带你成为消息中间件实战高手 的笔记



### 01 一个真实电商订单系统的整体架构、业务流程及负载情况

#### 一个订单系统的业务流程

<img src="https://s1.ax1x.com/2020/07/20/U4vBKH.jpg" alt="U4vBKH.jpg" border="0" />


####   一个订单系统的非核心流程
<img src="https://s1.ax1x.com/2020/07/20/U4xISO.jpg" alt="U4xISO.jpg" border="0" />

### 02 授人以渔：能概括一下你们系统的架构设计、业务流程以及负载情况吗？
> 首先你得知道现在你的系统，或者行业里其他普遍的一些系统，都有哪些技术难点和痛点，面临哪些问题。

>  接着要针对这些问题，反过来去学习一个技术，学习完这个技术之后，一定要思考如何将技术代入到真实的环境中，
  去解决对应的问题。
  
### 03 系统面临的现实问题：下订单的同时还要发券、发红包、Push推送，性能太差！

> 系统的压力来自哪

+ 电商这种系统,用户都会集中在晚上几个小时,为什么就不分析了, 他的访问曲线不是平均的,而是集中在一起的

<img src="https://s1.ax1x.com/2020/07/20/U5SORf.jpg" alt="U5SORf.jpg" border="0" />

+ 这种架构的压力都会落在单点上,上图就是mysql, 依据经验 4核8g mysql 最好1000-1500 能2000,16核32SSD 1万应该是极限,可能会出问题

+ 上面业务的问题, 在流程8中 ,支付回调中 ,推送,优惠券等,会有大量业务,太耗时


### 04 授人以渔：你们系统的核心流程性能如何？有没有哪个环节拖慢了速度？
> 普通项目还是业务处理和sql

> rpc 和第三方接口调用 ,同步的

要思考，在当前这样的系统压力下：

+ 系统的核心业务流程性能如何？
+ 核心流程的每个步骤要耗费多长时间？
+ 现在核心流程的性能你满意吗？是否还有优化的空间？
+ 在系统高峰期的时候，机器和数据库负载很高，是否对核心流程的性能有影响？
+ 如果有影响的话，会有多大的影响？

那你就想，你的这个系统做一个SaaS云平台的模式，提供给几万个公司，百万用户使用，不就可以了？你要自己去模拟这个场景。

然后，你按照文中的思路去推算出系统高峰期的负载，以及你的线上系统的机器的压力，到底要部署多少机器去满足这个压力。 


### 05 系统面临的现实问题：订单退款时经常流程失败，无法完成退款！
> 退款流程

<img src="https://s1.ax1x.com/2020/07/20/U5Fwxx.jpg" alt="U5Fwxx.jpg" border="0" />

+ 问题一: 和下单一样,流程过长  

+ 问题二: 如果退款失败怎么办

+ 下单之后不付款怎么办
> 下单之后会占用库存, 不付款 ,别人也买不了这个商品

给这种订单一个超时时间,超过 取消订单

+ 如果 下单之后不付款的单子 巨多,怎么处理
> 不停地去扫描表  效率太差,占用太多资源


### 06 授人以渔：你们系统出现过核心流程链路失败的情况吗？
> low 系统..

假设下 如果失败,可以保存失败操作,开始重试

### 07 系统面临的现实问题：第三方客户系统的对接耦合性太高，经常出问题！
> 还是上面的 订单支付 流程

+ 还是流程中有很多对接的第三方系统 ,比如支付,库仓 发货

<img src="https://s1.ax1x.com/2020/07/20/U5EDs0.jpg" alt="U5EDs0.jpg" border="0" />


+ 积分,发货,优惠券等系统都是和订单系统耦合在一起的

+ 这样订单系统和第三物流系统 也算耦合到一起, 会带来不确认性, 会基于网络,第三方系统大量原因导致 性能差

### 08 授人以渔：你们有没有跟第三方系统对接过，有遇到什么问题吗？
> 同步调第三方系统 真的一言难尽,  会各种坑, 随时保存日志,记得甩锅

### 09 系统面临的现实问题：大数据团队需要订单数据，该怎么办？

#### 大数据到底是干嘛的？
> 通过实时 采集来的数据,分析用户行为和其他结论 

每天如果有100万用户来访问你的APP，积累下来的一些浏览行为、访问行为、交易行为都是各种数据，这个数据量很大，所以你可以称之为“大数据”

大数据团队每天要负责的事情，说白了就是去尽可能的搜集每天100万用户在你的APP上的各种行为数据。

#### 大数据与我们系统的关系
> 订单数据等  他们需要分析每天几十万个订单，从中提取出老板最关心的APP交易数据报表！ 

方法

   +   select 查询
       
        大sql  而且查询次数不低,  数据库的cpu 和io 占用会很高, 其他操作就效率很低了
        

### 10 授人以渔：你们有没有遇到过自己系统的数据，其他团队需要获取的？
> 没有 

假设有, 比如微服务 过来取数据, 加缓存( 可以是缓存,也可以是新表), 异步同步  查询走缓存 

### 11 系统面临的现实问题：秒杀活动时数据库压力太大，该怎么缓解？

> 上面系统的缺点 是同步流程太长,效率太差

 假如在平时晚上的高峰使用期，最顶峰的时候大概是每秒2000左右的请求压力到订单系统上来。
 
 如果用户每秒会发起2000个请求到我们的订单系统的各类接口，包括下单接口、退款接口、查询接口等等，那么你觉得我们的订单系统每秒会执行多少条SQL在订单数据库上？
 
 一般你可以认为平均每个接口会执行2~3次的数据库操作。
 
 一般一个接口根据业务复杂度的不同，有的接口可能处理一个请求要执行五六次数据库操作，有的接口可能是1次数据库操作+两三个其他系统的接口调用（比如库存系统、营销系统）。

 总之，一般来说，业务系统的接口处理逻辑，基本都集中在对自己的数据库的操作以及对其他系统的调用上
 
 
#### 双11之类的大促活动有多恐怖

如果有200万用户参与双11活动，在双11购物最高峰的时候，假设会达到每秒至少1万的QPS。

也就是说，光是系统被请求的QPS就会达到1万以上，那么系统请求数据库的QPS就会达到2万以上。16核32G SSD 数据库性能，是无论如何扛不住每秒2万请求的。 

### 12 授人以渔：你们系统会不会遇到流量洪峰的场景，导致瞬时压力过大？
> 没有

假设有,异步处理, mq 削峰,  失败了就  开重试 ,最大重试次数还是失败  ,取消吧


### 13 阶段性复习：一张思维导图给你梳理高并发订单系统面临的技术痛点！

<img src="https://s1.ax1x.com/2020/07/21/UovGkR.jpg" alt="UovGkR.jpg" border="0" />

### 14 阶段性复习：放大100倍压力，也要找出你系统的技术挑战！

+ 第一，大家先思考一下系统的核心业务流程，当然不是指那种查询之类的操作。所谓核心链路指的是对你的系统进行的数据更新的操作，这才是核心链路，因为查询操作一般来说不涉及复杂的业务逻辑，主要是对数据的展示。
    
    对你的系统的核心链路分析一下，有哪些步骤，这些步骤各自的性能如何，综合起来让你的核心链路的性能如何？在这里是否有改进的空间？



+ 第二，大家可以思考一下，在你的系统中，是否有类似后台线程定时补偿的逻辑？

    比如订单长时间未支付就要自动关闭它，你们系统里有没有那种后台线程，会定时扫描你的数据，对异常数据进行补偿、自动修复等操作的？

    如果有的话，这种数据一般量有多大？如果没有，你可以思考一下，你们系统的核心数据是否需要类似的后台自动扫描机制？


+ 第三，大家可以思考一下，在你的系统里有没有跟第三方系统进行耦合？就是一些核心流程里需要同步调用第三方系统进行查询、更新等操作，第三方系统是否对你的核心链路有性能和稳定性上的影响？

+ 第四，大家可以思考一下，在你的核心链路中，是否存在那种关键步骤可能会失败的情况？万一失败了该怎么办？



+ 第五，大家可以思考一下，平时是否存在其他系统需要获取你们数据的情况？他们是如何获取你们数据的？

    是直接跑SQL从你们数据库里查询？或者是调用你们的接口来获取数据？是否存在这种情况？如果有，对你们有什么影响吗？



+ 第六，你们的系统是否存在流量洪峰的情况，有时候突然之间访问量增大好几倍，是否会对你们的系统产生无法承受的压力？

### 15 解决订单系统诸多问题的核心技术：消息中间件到底是什么？
#### 同步调用
用户发起一个请求，系统A收到请求，接着系统A必须立马去调用系统B，直到系统B返回了，系统A才能返回结果给用户，这种模式其实就是所谓的“同步调用”。

#### 消息中间件的异步

为系统A仅仅是发个消息到MQ，至于系统B什么时候获取消息，有没有获取消息，他是不管的。所以这种情况下，我们说系统A和系统B是异步调用。

#### 作用 

主要的作用有这么几个，包括异步化提升性能，降低系统耦合，流量削峰，等等

MQ进行流量削峰的效果，系统A发送过来的每秒1万请求是一个流量洪峰，然后MQ直接给扛下来了，都存储自己本地磁盘，这个过程就是流量削峰的过程，瞬间把一个洪峰给削下来了，让系统B后续慢慢获取消息来处理。

### 16 授人以渔：结合自己的系统问题思考一下，MQ有什么用处？

假设
1. 上传文件比如file是一个系统 具体业务一个系统 他们直接用mq ,file系统接收到文件存到文件服务器写一个消息给mq然后就返给用户结果 业务系统从mq接受消息进行业务处理 把处理结果写一个消息给mq , file系统接受消息更新文件处理结果， 当用户需要刷新文件上传结果的时候查询数据库就可以了
2. 在和硬件对接比如接受考勤信息也会用mq
3. 日志
4. 异步短信提示等

### 17 领导的要求：你来对 Kafka、RabbitMQ 以及 RocketMQ 进行技术选型调研

+ RabbitMQ  单机吞吐量 万级别, 基本不丢,功能全,web管理页面很方便, 集群模式麻烦 ,基于 erlang 开发

+ Kafka   10 万级，高吞吐 ,基本不丢 ,分布式, 功能不全, 但适合大数据和日志系统

+ RocketMQ 10 万级，支撑高吞吐, 基本不丢 ,分布式, 功能全, 可视化界面

### 18 授人以渔：你们公司主要使用的 MQ 是哪种？为什么要选用它？

 小项目和以前的 很多都是Rabbit ,熟悉,而且能支撑服务需求
 
 但现在Roket 好像用的不少

### 19 新技术引入：给团队分享 RocketMQ 的架构原理和使用方式
+ RocketMQ是如何集群化部署来承载高并发访问的？
+ 如果RocketMQ中要存储海量消息，如何实现分布式存储架构？

#### MQ如何集群化部署来支撑高并发访问？
发送消息到MQ的系统会把消息分散发送给多台不同的机器，假设一共有1万条消息，分散发送给10台机器，可能每台机器就是接收到1000条消息

每台机器上部署的RocketMQ进程一般称之为Broker，每个Broker都会收到不同的消息，然后就会把这批消息存储在自己本地的磁盘文件里

#### 高可用保障：万一Broker宕机了怎么办？
RocketMQ的解决思路是Broker主从架构以及多副本策略。

Master Broker收到消息之后会同步给Slave Broker，这样Slave Broker上就能有一模一样的一份副本数据！

这样同一条消息在RocketMQ整个集群里不就有两个副本了，一个在Master Broker里，一个在Slave Broker里！

这个时候如果任何一个Master Broker出现故障，还有一个Slave Broker上有一份数据副本，可以保证数据不丢失，还能继续对外提供服务，保证了MQ的可靠性和高可用性

#### 数据路由：怎么知道访问哪个Broker？

有一个NameServer的概念，他也是独立部署在几台机器上的 , 如果是集群, 注册的时候 会在每个NameServer 都注册


### 20 授人以渔：结合你对其他 MQ 的了解，思考 RocketMQ 的设计有何特点？

kafka集群架构分为leader和follower，每个topic下分为partition，partition分为primary和replication，partition主节点和副本节点存储在不同的节点上来保证集群的高可用，每个消息的partition可以分配到不同的节点来分布式存储，follower来同步leader节点的信息

kafka和Rocketmq都是分布式的消息系统，

在集群化部署方面，kafka通过zk进行节点协调，而rocketmq通过自身namesrv进行节点协调，所以在协调节点的设计上rocket显得更加轻。 

存储方面，在Kafka中，是1个topic（就是一个业务场景）有多个partition（对应3个物理文件目录），当需要存储数据的时候，会把topic中一个parition大文件分成多个小文件段。通过多个小文件段，就轻松实现定期清除或删除已经消费完文件。降低磁盘占用。

在rocketmq中，采用的是混合型的存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。

两者在对比上，Kafka采用的是独立型的存储结构，每个队列一个文件。RocketMQ采用混合型存储结构的缺点在于，会存在较多的随机读操作，因此读的效率偏低。同时消费消息需要依赖ConsumeQueue，构建该逻辑消费队列需要一定开销。 

主从备份方面：生产者向kafka写入消息时，一般会写入多个分区（partition），kafka提供冗余机制，即每个分区都有多个相同的备份(replica)，kafka把分区所有副本均匀分配到其他broker上，并从这些副本中挑选一个作为leader副本对外提供服务，其他副本称为follwer副本。当leader副本所在的broker有可能宕机，这时follower副本会竞争成为leader，继续提供服务。 

而当生产者者向rocketmq写入消息时，会将数据写入集群中的相关master broker上，而每个master broker都有1到多个slave broker, 这样在一定程度上保证master出现了不可恢复的故障时，不丢失数据。 同时如果master宕机了，消费者会自动重连到相应的salve上，不会出现消费停滞，那么同时在master和slave数据同步分为同步复制(有一定的效率损失)和异步复制(数据不一致) 

在生产和消费消息方面: 在kafka中，每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic，物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处，而这种能力实现的底层我想应该就是通过zk来完成的。 在rocketmq中，NameSrv提供

### 21 设计生产架构之前的功课：消息中间件路由中心的架构原理是什么？

四个核心的部分：
+ 第一块就是他的NameServer，这个东西很重要，他要负责去管理集群里所有Broker的信息，让使用MQ的系统可以通过他感知到集群里有哪些Broker。

+ 第二块就是Broker集群本身了，必须得在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本存储和高可用。
+ 第三块就是向MQ发送消息的那些系统了，这些系统一般称之为生产者，这里也有很多细节是值得深究的，因为这些生产者到底是如何
    
    从NameServer拉取路由信息的？如何选择Broker机器建立连接以及发送消息的？

+ 第四块就是从MQ获取消息的那些系统，这些系统一般称之为消费者。

#### 关于RocketMQ NameServer设计原理

NameServer 如果要高可用 就要多部署几台,做成集群

每个Broker启动都得向所有的NameServer进行注册

### 系统如何从NameServer获取Broker信息？

每个系统自己每隔一段时间，定时发送请求到NameServer去拉取最新的集群Broker信息  自己主动去NameServer拉取Broker信息 的。

#### 如果Broker挂了，NameServer是怎么感知到的？
这个问题，靠的是Broker跟NameServer之间的心跳机制，Broker会每隔30s给所有的NameServer发送心跳，告诉每个NameServer自己目前还活着。

#### Broker挂了，系统是怎么感知到的？
刚开始集群里有10个Broker，各个系统从NameServer那里得知，都以为有10个Broker。

结果此时突然挂了一个Broker，120s没发心跳给NameServer，NameServer是知道现在只有9个Broker了。

但是此时其他系统是不知道只有9个Broker的，还以为有10个Broker，此时可能某个系统就会发送消息到那个已经挂掉的Broker上去，此时是绝对不可能成功发送消息的

而且过一会儿，系统又会重新从NameServer拉取最新的路由信息了，此时就会知道有一个Broker已经宕机了。

### 22 授人以渔：要是没有这个路由中心，消息中间件可以正常运作么？
假设这个NameServer集群整体都故障了，失去了这个NameServer集群之后：
+ RocketMQ还能正常运行吗？
+ 生产者还能发送消息到Broker吗？
+ 消费者还能从Broker拉取消息吗？

首先启动注册中心nameserver，每个nameserver之间互不通信，

启动broker时，会把自己的信息注册到每一个nameserver，broker每30s发送心跳包给注册中心，注册中心更新broker的最后更新时间。

nameserver会定时10秒检测更新时间是否超过120s，超过则将这个broker路由原信息剔除。

生产者和消费者定时去获取broker的路由信息，根据轮询生产消息和消费消息的负载。

当注册中心挂了，本地还会有缓存信息能够继续通信。

### 23 设计生产架构之前的功课：Broker的主从架构原理是什么？

#### Master Broker是如何将消息同步给Slave Broker的？

为了保证MQ的数据不丢失而且具备一定的高可用性，所以一般都是得将Broker部署成Master-Slave模式的，也就是一个Master Broker对应一个Slave Broker

然后Master需要在接收到消息之后，将数据同步给Slave，这样一旦Master Broker挂了，还有Slave上有一份数据。

说明：
+ Slave Broker也会向所有的NameServer进行注册
+ Slave Broker也会向所有的NameServer每30s发送心跳

RocketMQ的Master-Slave模式采取的是Slave Broker不停的发送请求到Master Broker去拉取消息。

所以首先要明白这一点，就是RocketMQ自身的Master-Slave模式采取的是Pull模式拉取消息

#### RocketMQ 实现读写分离了吗？

有可能从Master Broker获取消息，也有可能从Slave Broker获取消息

作为消费者的系统在获取消息的时候会先发送请求到Master Broker上去，请求获取一批消息，此时Master Broker是会返回一批消息给消费者系统的

然后Master Broker在返回消息给消费者系统的时候，会根据当时Master Broker的负载情况和Slave Broker的同步情况，向消费者系统建议下一次拉取消息的时候是从Master Broker拉取还是从Slave Broker拉取。

要是这个时候Master Broker负载很重，本身要抗10万写并发了，你还要从他这里拉取消息，给他加重负担，那肯定是不合适的。

所以此时Master Broker就会建议你从Slave Broker去拉取消息。

或者举另外一个例子，本身这个时候Master Broker上都已经写入了100万条数据了，结果Slave Broke不知道啥原因，同步的特别慢，才同步了96万条数据，落后了整整4万条消息的同步，这个时候你作为消费者系统可能都获取到96万条数据了，那么下次还是只能从Master Broker去拉取消息。

因为Slave Broker同步太慢了，导致你没法从他那里获取更新的消息了。

所以这一切都会由Master Broker根据情况来决定

#### 如果Slave Broke挂掉了有什么影响？
有一点影响，但是影响不太大

因为消息写入全部是发送到Master Broker的，然后消息获取也可以走Master Broker，只不过有一些消息获取可能是从Slave Broker去走的。

所以如果Slave Broker挂了，那么此时无论消息写入还是消息拉取，还是可以继续从Master Broke去走，对整体运行不影响。

只不过少了Slave Broker，会导致所有读写压力都集中在Master Broker上。

#### 如果Master Broker挂掉了该怎么办？

在RocketMQ 4.5版本之前，都是用Slave Broker同步数据，尽量保证数据不丢失，但是一旦Master故障了，Slave是没法自动切换成Master的。

所以在这种情况下，如果Master Broker宕机了，这时就得手动做一些运维操作，把Slave Broker重新修改一些配置，重启机器给调整为Master Broker，这是有点麻烦的，而且会导致中间一段时间不可用

所以这种Master-Slave模式不是彻底的高可用模式，他没法实现自动把Slave切换为Master

#### 基于Dledger实现RocketMQ高可用自动切换
在RocketMQ 4.5之后，这种情况得到了改变，因为RocketMQ支持了一种新的机制，叫做Dledger

简单来说，把Dledger融入RocketMQ之后，就可以让一个Master Broker对应多个Slave Broker，也就是说一份数据可以有多份副本，比如一个Master Broker对应两个Slave Broker。

然后依然会在Master和Slave之间进行数据同步

此时一旦Master Broker宕机了，就可以在多个副本，也就是多个Slave中，通过Dledger技术和Raft协议算法进行leader选举，直接将一个Slave Broker选举为新的Master Broker，然后这个新的Master Broker就可以对外提供服务了。

整个过程也许只要10秒或者几十秒的时间就可以完成，这样的话，就可以实现Master Broker挂掉之后，自动从多个Slave Broker中选举出来一个新的Master Broker，继续对外服务，一切都是自动的。

### 24 授人以渔：Broker主从同步有没有数据不一致问题？

问题：
+ 假设如果没有RocketMQ 4.5新版本引入的Dledger技术，仅仅是靠之前的Master-Slave主从同步机制，那么在Master崩溃的时
候，可能会造成多长时间的系统不可用？这个时候如何能够尽快的恢复集群运行？依赖手工运维的话，如何能尽快的去完成这个运
维操作？
+ 在RocketMQ 4.5之后引入了Dledger技术可以做到自动选举新的Master，那么在Master崩溃一直到新的Master被选举出来的这
个过程中，你觉得对于使用MQ的系统而言，会处于一个什么样的状态呢？

+ 希望大家去研究一下Kafka和RabbitMQ的多副本和高可用机制，Kafka是如何在集群里维护多个副本的？出现故障的时候能否实
现自动切换？RabbitMQ是如何在集群里维护多个数据副本的？出现故障的时候能否实现自动切换？

+ 既然有主从同步机制，那么有没有主从数据不一致的问题？Slave永远落后Master一些数据，这就是主从不一致。那么这种不一致
有没有什么问题？有办法保证主从数据强制一致吗？这样做又会有什么缺点呢？

可以和kafka一样做一个设置 保证只有消息至少被所有follwer同步成功后才算消息写入成功，这样即使leader挂了，从新选举出的follwer也会拥有全部的消息，只不过消息写入吞吐量会下降，这是肯定的 

所有的消息中间件主从要做的强一致性这里都要牺牲吞吐量，必须等待同步写到从节点，写入不成功就返回异常，具体场景具体考虑吧 

kafka这里可以配置多种模式，还可以直接发送后不等待写入成功就返回，还有一个是等待leader写入成功在返回，

Broker 采用主从架构存在延迟， 必然存在主从同步数据不一致的问题。 

1. producer 生产消息， 存放到 主Broker， 从Broker主动定时拉取消息。
 
2. consumer 拉取消息， 向 主Broker 拉取消息， 主Broker会记录消费相关信息， 然后从Broker再向主Broker同步。
  
3. 保证数据一致， 可以在producer写入消息， consumer 拉取消息后提交commitLog时改成同步， 多台机器成功之后才彻底成功。

###  25 落地第一步：设计一套高可用的消息中间件生产部署架构

#### NameServer集群化部署，保证高可用性
NameServer的设计是采用的Peer-to-Peer的模式来做的，也就是可以集群化部署，但是里面任何一台机器都是独立运行的，跟其他的机器没有任何通信。

每台NameServer实际上都会有完整的集群路由信息，包括所有的Broker节点信息，我们的数据信息，等等。所以只要任何一台NameServer存活下来，就可以保证MQ系统正常运行，不会出现故障。

#### 基于Dledger的Broker主从架构部署

采用RocketMQ 4.5以前的那种普通的Master-Slave架构来部署，能在一定程度上保证数据不丢失，也能保证一定的可用性。

但是那种方式的缺陷是很明显的，最大的问题就是当Master Broker挂了之后，没办法让Slave Broker自动切换为新的MasterBroker，需要手工做一些运维操作，修改配置以及重启机器才行，这个非常麻烦。

所以选择基于Dledger的主备自动切换的功能来进行生产架构的部署。

而且Dledger技术是要求至少得是一个Master带两个Slave，这样有三个Broke组成一个Group，也就是作为一个分组来运行。一旦Master宕机，他就可以从剩余的两个Slave中选举出来一个新的Master对外提供服务。

ps:每个Broker（不论是Master和Slave）都会把自己注册到所有的NameServer上去。


#### Broker是如何跟NameServer进行通信的？
Broker会每隔30秒发送心跳到所有的NameServer上去，然后每个NameServer都会每隔10s检查一次有没有哪个Broker超过120s没发送心跳的，
如果有，就认为那个Broker已经宕机了，从路由信息里要摘除这个Broker。

在RocketMQ的实现中，采用的是TCP长连接进行通信。

也就是说，Broker会跟每个NameServer都建立一个TCP长连接，然后定时通过TCP长连接发送心跳请求过去

#### 使用MQ的系统都要多机器集群部署

很多的系统使用RocketMQ，有些系统是作为生产者往MQ发送消息，有些系统是作为消费者从MQ获取消息，当然还有的系统是既作为生产者，又作为消费者，所以我们要考虑这些系统的部署。

对于这些系统的部署本身不应该在MQ的考虑范围内，但是我们还是应该给出一个建议，就是无论作为生产者还是消费者的系统，都应该多机器集群化部署，保证他自己本身作为生产者或者消费者的高可用性。

#### MQ的核心数据模型：Topic到底是什么？
Topic其实就是一个数据集合的意思，不同类型的数据你得放不同的Topic里去。

要是你有一些商品数据要发送消息到MQ里，你就应该创建一个Topic叫做“topic_product_info”，代表里面都是商品数据，那些想

要从MQ里获取商品数据的系统就可以从“topic_product_info”里获取了。

所以简单来说，你的系统如果要往MQ里写入消息或者获取消息，首先得创建一些Topic，作为数据集合存放不同类型的消息，比如说订单Topic，商品Topic，等等。

#### Topic作为一个数据集合是怎么在Broker集群里存储的？

首先我们来想一下，比如我们有一个订单Topic，可能订单系统每天都会往里面投递几百万条数据，然后这些数据在MQ集群上还得保留好多天，那么最终可能会有几千万的数据量，这还只是一个Topic。

那么如果有很多的Topic，并且里面都有大量的数据，最终加起来的总和也许是一个惊人的数字，此时这么大量的数据本身是不太可能存放在一台机器上的。


分布式存储。

我们可以在创建Topic的时候指定让他里面的数据分散存储在多台Broker机器上，比如一个Topic里有1000万条数据，此时有2台Broker，那么就可以让每台Broker上都放500万条数据。

这样就可以把一个Topic代表的数据集合分布式存储在多台机器上了

另外很重要的一件事是，每个Broke在进行定时的心跳汇报给NameServer的时候，都会告诉NameServer自己当前的数据情况，

比如有哪些Topic的哪些数据在自己这里，这些信息都是属于路由信息的一部分。

#### 生产者系统是如何将消息发送给Broker的？

+ 在发送消息之前，得先有一个Topic，然后在发送消息的时候你得指定你要发送到哪个Topic里面去。

+ 接着既然你知道你要发送的Topic，那么就可以跟NameServer建立一个TCP长连接，然后定时从他那里拉取到最新的路由信息，包括:

    集群里有哪些Broker，集群里有哪些Topic，每个Topic都存储在哪些Broker上

+ 然后生产者系统自然就可以通过路由信息找到自己要投递消息的Topic分布在哪几台Broker上，此时可以根据负载均衡算法，从里面选择一台Broke机器出来，比如round robine轮询算法，或者是hash算法，都可以。

+ 总之，选择一台Broker之后，就可以跟那个Broker也建立一个TCP长连接，然后通过长连接向Broker发送消息即可.Broker收到消息之后就会存储在自己本地磁盘里去

这里唯一要注意的一点，就是生产者一定是投递消息到Master Broker的，然后Master Broker会同步数据给他的Slave Brokers，实现
一份数据多份副本，保证Master故障的时候数据不丢失，而且可以自动把Slave切换为Master提供服务。


#### 消费者是如何从Broker上拉取消息的？

消费者系统其实跟生产者系统原理是类似的，他们也会跟NameServer建立长连接，然后拉取路由信息，接着找到自己要获取消息的Topic在哪几台Broker上，就可以跟Broker建立长连接，从里面拉取消息了。

#### 整体架构：高可用、高并发、海量消息、可伸缩

整个这套生产架构是实现完全高可用的，因为NameServer随便一台机器挂了都不怕，他是集群化部署的，每台机器都有完整的路由信息；

Broker随便挂了一台机器也不怕，挂了Slave对集群没太大影响，挂了Master也会基于Dledger技术实现自动Slave切换为Master；

生产者系统和消费者系统随便挂了一台都不怕，因为他们都是集群化部署的，其他机器会接管工作。

而且这个架构可以抗下高并发，因为假设订单系统对订单Topic要发起每秒10万QPS的写入，那么只要订单Topic分散在比如5台Broker上，实际上每个Broker会承载2万QPS写入，也就是说高并发场景下的10万QPS可以分散到多台Broker上抗下来。

然后集群足以存储海量消息，因为所有数据都是分布式存储的，每个Topic的数据都是存储在多台Broker机器上的，用集群里多台Master Broker就足以存储海量的消息。

所以，用多个Master Broker部署的方式，加上Topic分散在多台Broker上的机制，可以抗下高并发访问以及海量消息的分布式存储。

然后每个Master Broker有两个Slave Broker结合Dledger技术可以实现故障时的自动Slave-Master切换，实现高可用性。

最后，这套架构还具备伸缩性，就是说如果要抗更高的并发，存储跟多的数据，完全可以在集群里加入更多的Broker机器，这样就可以线性扩展集群了。

### 27 部署一个小规模的 RocketMQ 集群，为压测做好准备

<a  href="/src/main/resources/note/中间件/rocket配置.md"> Rocket 配置  </a>

### 28 授人以渔：动手完成一个小规模的RocketMQ集群的部署进行练习


### 29 生产运维：如何对RocketMQ集群进行可视化的监控和管理？

#### RocketMQ的大优势：可视化的管理界面
整个RocketMQ集群的元数据都集中在了NameServer里，包括有多少Broker，有哪些Topic，有哪些Producer，有哪些Consumer，目前集群里有多少消息，等等。

是RocketMQ里既然有大量的信息可以让我们进行监控和查看，他自然会提供一些办法来让我们看到，这就是他最大的优势之一，一个可视化的管理界面。

我们可以随便找一台机器，用NameServer的三台机器中的任意一台机器就可以，在里面执行如下命令拉取RocketMQ运维工作台的源码：
>git clone https://github.com/apache/rocketmq-externals.git

然后进入rocketmq-console的目录：
>cd rocketmq-externals/rocketmq-console

执行以下命令对rocketmq-cosole进行打包，把他做成一个jar包：
>mvn package -DskipTests

然后进入target目录下，可以看到一个jar包，接着执行下面的命令启动工作台：
>java -jar rocketmq-console-ng-1.0.1.jar --server.port=8080 --rocketmq.config.namesrvAddr=127.0.0.1:9876

这里务必要在启动的时候设置好NameServer的地址，如果有多个地址可以用分号隔开，接着就会看到工作台启动了，然后就通过浏览器访问那台机器的8080端口就可以了，就可以看到精美的工作台界面。

#### 如何通过工作台进行集群监控

你可以看到各个Broker的分组，哪些是Master，哪些是Slave，他们各自的机器地址和端口号，还有版本号

包括最重要的，就是他们每台机器的生产消息TPS和消费消息TPS，还有消息总数。

这是非常重要的，通过这个TPS统计，就是每秒写入或者被消费的消息数量，就可以看出RocketMQ集群的TPS和并发访问量。

#### 机器本身的监控应该如何做？
有了这个东西，我们是可以在压测的时候看到整个RocketMQ的TPS了，也就是Transaction PerSecond，就是每秒事务的意思，在这里就是每秒消息数量的意思。

但是我们要同时看到集群每台机器的CPU、IO、磁盘、内存、JVM GC的负载和情况怎么办呢？

其实这些东西都有很好的监控系统可以去看了，比如说Zabbix、Open-Falcon等等，一般公司都会用这些东西来监控机器的性能和资源使用率。

### 30 授人以渔：你们公司的MQ集群是如何进行监控和管理的？

假设

核心链路用的是rabbitmq，使用的监控是其自带的可视化控制面板rabbitmq_management， 运维的同事平时主要看概览，包括集群内节点状态（观察集群内broker状态），以及相关messag rates，消费监听者的连接进程(消费系统当前是否正常)，以及消息队列中的消息处理情况（比如失败的，是否存在消息堆积等， 总得来说，他们主要需求是监控Rabbit内部状态、确认RabbitMQ可用并且能够响应、观察队列状态检测消费者异常、检测消息通信结构中不合需求的配置更改等 如果我来负责 我除了上述的一些指标外，我还会关心，当前各个队列的准备完成的数据有多少，没有被ack的有多少，另外我会用到admin 管理，方便管理帐户信息及权限管理（非常方便），管理vhost(虚拟主机起到消息的逻辑区分)等， 此外我还需要部署Nagios：监控系统或服务状态异常时发出邮件或短信报警第一时间通知我，在状态恢复后发出正常的邮件或短信通知


### 31 RocketMQ生产集群准备：进行OS内核参数和JVM参数的调整

#### 对RocketMQ集群进行OS内核参数的调整
1. vm.overcommit_memory
    “vm.overcommit_memory”这个参数有三个值可以选择，0、1、2。
    
    如果值是0的话，在你的中间件系统申请内存的时候，os内核会检查可用内存是否足够，如果足够的话就分配内存给你，如果感觉剩余
    内存不是太够了，干脆就拒绝你的申请，导致你申请内存失败，进而导致中间件系统异常出错。
    
    因此一般需要将这个参数的值调整为1，意思是把所有可用的物理内存都允许分配给你，只要有内存就给你来用，这样可以避免申请内
    存失败的问题。
    
    比如我们曾经线上环境部署的Redis就因为这个参数是0，导致在save数据快照到磁盘文件的时候，需要申请大内存的时候被拒绝了，进
    而导致了异常报错。
    
    可以用如下命令修改：
    >echo 'vm.overcommit_memory=1' >> /etc/sysctl.conf。

2. vm.max_map_count
    这个参数的值会影响中间件系统可以开启的线程的数量，同样也是非常重要的
    
    如果这个参数过小，有的时候可能会导致有些中间件无法开启足够的线程，进而导致报错，甚至中间件系统挂掉。
    
    他的默认值是65536，但是这个值有时候是不够的，比如我们大数据团队的生产环境部署的Kafka集群曾经有一次就报出过这个异常，说无法开启足够多的线程，直接导致Kafka宕机了。
    
    因此建议可以把这个参数调大10倍，比如655360这样的值，保证中间件可以开启足够多的线程。
    
    可以用如下命令修改：
    >echo 'vm.max_map_count=655360' >> /etc/sysctl.conf。

3. vm.swappiness

    这个参数是用来控制进程的swap行为的，这个简单来说就是os会把一部分磁盘空间作为swap区域，然后如果有的进程现在可能不是太
    活跃，就会被操作系统把进程调整为睡眠状态，把进程中的数据放入磁盘上的swap区域，然后让这个进程把原来占用的内存空间腾出
    来，交给其他活跃运行的进程来使用。
    
    如果这个参数的值设置为0，意思就是尽量别把任何一个进程放到磁盘swap区域去，尽量大家都用物理内存。
    
    如果这个参数的值是100，那么意思就是尽量把一些进程给放到磁盘swap区域去，内存腾出来给活跃的进程使用。
    
    默认这个参数的值是60，有点偏高了，可能会导致我们的中间件运行不活跃的时候被迫腾出内存空间然后放磁盘swap区域去。
    
    因此通常在生产环境建议把这个参数调整小一些，比如设置为10，尽量用物理内存，别放磁盘swap区域去。
    
    可以用如下命令修改：
    >echo 'vm.swappiness=10' >> /etc/sysctl.conf。

4. ulimit

    这个是用来控制linux上的最大文件链接数的，默认值可能是1024，一般肯定是不够的，因为你在大量频繁的读写磁盘文件的时候，或
    者是进行网络通信的时候，都会跟这个参数有关系
    
    对于一个中间件系统而言肯定是不能使用默认值的，如果你采用默认值，很可能在线上会出现如下错误：error: too many open files。
    
    因此通常建议用如下命令修改这个值：
    > echo 'ulimit -n 1000000' >> /etc/profile。


+ 中间件系统肯定要开启大量的线程（跟vm.max_map_count有关）
+ 而且要进行大量的网络通信和磁盘IO（跟ulimit有关）
+ 然后大量的使用内存（跟vm.swappiness和vm.overcommit_memory有关）

#### 对JVM参数进行调整

<a  href="/src/main/resources/note/中间件/borkerjvm配置.md"> 对JVM参数进行调整  </a>

#### 对RocketMQ核心参数进行调整
dledger的示例配置文件：rocketmq/distribution/target/apacherocketmq/conf/dledger

在这里主要是有一个较为核心的参数：sendMessageThreadPoolNums=16

这个参数的意思就是RocketMQ内部用来发送消息的线程池的线程数量，默认是16

其实这个参数可以根据你的机器的CPU核数进行适当增加，比如机器CPU是24核的，可以增加这个线程数量到24或者30，都是可以的。


### 32 授人以渔：你们公司的MQ集群是如何配置生产参数的？
> 无 

Kafka，因为Kafka设计之初的原则就是尽量少的依赖JVM，比如他的两个比较重要的特性：磁盘顺序写和零拷贝，并且大量使用OS的cache，所以，在给JVM分配内存的时候，只要分配足够运行的内存就行了，剩下的大部分内存都要留出来给OS cache，用来提高Kafka的写和读的并发。其他针对网络通信（线程数量貌似也没有那么大的需求）以及磁盘IO（ulimit）的配置大体相同，主要是尽量开大Linux的阈值，而不至于因为这些参数限制了Kafka的吞吐和运行效率。补充一个相关的知识，因为用的是磁盘顺序写，所以Kafka的机器只要HDD就可以了，不需要机械硬盘SSD。


1、os的内核参数，在分配内存分配策略为1，内核允许分配所有的物理内存 2、可以开启的最大线程数为默认3、进程的内存交换 设置为默认，服务器只部署了mq，默认的没什么问题； 4、文件的最大连接数默认值为1024 jvm参数：jvm配置的一团糟，注册中心与broker是部署在一起，分配的jvm参数都是2g，年轻代为1，因为为4核8g内存，还需要留内存给os； 中间件的配置参数：压测环境过程中，运维把sendMessaageThreadPooNums 设置为256；

###  33 对小规模RocketMQ集群进行压测，同时为生产集群进行规划

> 压测:在RocketMQ的TPS和机器的资源使用率和负载之间取得一个平衡。

#### 一次RocketMQ小规模集群的压测

 <a  href="/src/main/resources/note/中间件/rocket压测.md"> 一次RocketMQ小规模集群的压测  </a>
 
### 34 授人以渔：你们公司的MQ集群做过压测吗？生产集群是如何规划的？

+ 他们对MQ集群做过压测吗？
+ 使用什么样的机器配置做的压测？
+ 使用多大规模的集群做的压测？如何压测的？
+ 在压测的过程中发现单Broker的TPS最高有多少？
+ 在压测过程中，cpu负载、内存使用率、jvm gc频率、磁盘io负载、网卡流量负载，这些值都是如何变化的？
+ 在压测过后，是如何规划生产集群的？
+ 目前公司线上MQ集群的TPS多高？机器资源的负载情况如何？能否抗住？ 

### 35 阶段性复习：一张思维导图给你梳理消息中间件集群生产部署架构规划

<img src="https://s1.ax1x.com/2020/07/22/UbszE4.jpg" alt="UbszE4.jpg" border="0" />

### 36 阶段性复习：按照你们公司的真实负载，设计消息中间件集群生产架构
+ 你们系统有没有使用MQ技术的业务场景？
+ 你们公司是如何进行技术选型的？
+ 你能对RocketMQ、Kafka、RabbitMQ三种技术的架构原理都进行一个思考和横向对比吗？
+ 如果RocketMQ没有路由中心了能正常运转吗？
+ 主从同步是否有数据一致性问题？
+ 你们公司的MQ集群是采用什么样的部署架构？
+ 你有没有动手完成一个小规模RocketMQ集群的部署？
+ 你们公司都是如何对MQ集群进行可视化监控的？
+ 你们公司的MQ集群是如何调整生产参数的？公司的MQ集群做过压测吗？你们公司的MQ集群生产环境是如何部署的？

rocketmq，主要进行异步，削峰，分布式事务一致性。

kafka吞吐量高，但功能简单不适合我们电商业务，适合日志采集不需要业务操作。

rocketmq公司有对源码进行二次开发封装客户端，增加发送失败的系统自动补偿机制。

没有了路由中心能运行一段时间，新的broker，生产者，消费者不能注册，还有缓存信息。

主从同步双写不会出现数据丢失。

我们公司broker采用多主零从同步刷盘，nameserver集群部署，生产者，消费者集群部署


### 37 基于MQ实现订单系统的核心流程异步化改造，性能优化完成！

#### 改造订单系统

1. 下单核心流程环节太多，性能较差
2. 订单退款的流程可能面临退款失败的风险
3. 关闭过期订单的时候，存在扫描大量订单数据的问题
4. 跟第三方物流系统耦合在一起，性能存在抖动的风险
5. 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
6. 做秒杀活动时订单数据库压力过大

在用户支付完毕后，只要执行最核心的更新订单状态以及扣减库存就可以了，保证速度足够快。

然后诸如增加积分、发送优惠券、发送短信、通知发货的操作，都可以通过MQ实现异步化执行。

一旦你支付成功，实际上订单系统只需要更新订单状态（30ms）+扣减库存（80ms）+发送订单消息到
RocketMQ（10ms），一共120ms就可以了

#### 在订单系统中如何发送消息到RocketMQ？

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md" >官网的demo 看生产者</a>

#### 其他系统改造为从RocketMQ中获取订单消息

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md" >官网的demo 看消费者</a>

### 38 授人以渔：如果在你们系统的核心流程引入MQ，应该如何改造系统？
采用大量mq进行不同系统的交互，比如一个消息，多方都要订阅。自己做的一个项目，发货系统的改造，由第三方同步请求我们，把参数检验和更新数据库，通知其他部门发货信息全部解藕，提高了对外接口的响应，通过mq的重试消费加告警保证本地事务的执行成功。推送给第三方消息。

数据产品的系统，每次操作都需要记录审计信息。这类信息不是必须保证不丢失的，而且时效性要求没有那么高，所以，我们在切面拦截到，并把信息发送到MQ中，异步处理；还有一类是比较关键的数据（不允许丢，时效性要求不高），首先一次请求，可能会在多个分布式系统差生这类数据，而且我们需要在这些数据都保存到数据库后，再从数据库中另外一个表中查出来（前面生成的数据都有触发器）发送到搜索平台。上面这种就比较复杂，我们本来就使用了分布式事务，但是为了加快性能，把上传搜索平台的消息做成了异步，但是因为是一个分布式事务，可能会发生操作失败回滚，但是消息已经发出去了，没办法取消，从而产生脏数据的问题，针对上述问题，我们采用了分布式事务+RocketMQ的事务消息的机制解决的（实际上用的是阿里云的GTS+RocketMQ事务）

### 39 基于MQ实现订单系统的第三方系统异步对接改造，解耦架构完成！

上面那6个问题,如果用mq 事实上1,4 都解决了


<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md" >官网的demo </a>

#### 什么叫做同步发送消息到RocketMQ？
 > 普通的send 就是同步发送
 
#### 什么叫做异步发送消息到RocketMQ？
<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#2%E5%8F%91%E9%80%81%E5%BC%82%E6%AD%A5%E6%B6%88%E6%81%AF" >官网的demo  异步发送</a>

主要代码片段
````
         // 启动Producer实例
          producer.start();
          producer.setRetryTimesWhenSendAsyncFailed(0);
          int messageCount = 100;
                  // 根据消息数量实例化倒计时计算器
          	final CountDownLatch2 countDownLatch = new CountDownLatch2(messageCount);
              	for (int i = 0; i < messageCount; i++) {
                          final int index = i;
                      	// 创建消息，并指定Topic，Tag和消息体
                          Message msg = new Message("TopicTest",
                              "TagA",
                              "OrderID188",
                              "Hello world".getBytes(RemotingHelper.DEFAULT_CHARSET));
                          // SendCallback接收异步返回结果的回调
                          producer.send(msg, new SendCallback() {
                              @Override
                              public void onSuccess(SendResult sendResult) {
                                   // 成功
                              }
                              @Override
                              public void onException(Throwable e) {
                	             // 异常
                              }
                      	});
              	}
```` 
 #### 什么叫做发送单向消息到RocketMQ？
 这种方式主要用在不特别关心发送结果的场景，例如日志发送。
 
  <a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#3%E5%8D%95%E5%90%91%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF" >官网的demo  单向消息</a>
  
#### 消费模式

rocket 两种消费模式

+ Push消费模式


<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#2%E6%B6%88%E8%B4%B9%E8%80%85%E6%A0%B7%E4%BE%8B" >官网的demo  Push消费模式</a>

注册后,就是Broker会主动把消息发送给你的消费者，你的消费者是被动的接收Broker推送给过来的消息，然后进行处理
````
// 实例化消费者
  DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name");
  ````
 
 + 做Pull消费模式
 
 
<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#82-omspullconsumer " >官网的demo 做Pull消费模式</a>

Broker不会主动推送消息给Consumer，而是消费者主动发送请求到Broker去拉取消息过来。


### 40 授人以渔：如果你们系统要对接第三方系统，应该如何设计？

mq对比:

kafka发送的三种模式：1.发送并忘记(单向) 2.同步发送 3.异步发送+回调函数，和rocket mq一样。 

kafa消费采用pull模式。 

rabbit mq confirm发送方确认模式：普通Confirm模式、批量确认模式、异步监听发送方确认模式。 

rabbit mq消费支持pull和push两种模式

kafka 消费是pull模式 客户端主动去服务端拉取消息 这样好处是可以更灵活的控制消费速度，要是服务端主动推送的话可能每个消费者消费速率不一样导致消费者消费不过来，kafka发送也有同步发送 异步发送带callback回调接口的，也可以发送后不关注结果配合acks=0 这样发送速率最大 但也是消息最不可靠的一种方式 大数据日志采集很适合 追求吞吐量

集群总结

1，zookeeper集群： 角色：leader,follower,observer 解读：只能写leader，可以从leader，follower和observer中读取数据，但observer没有投票权 
 
2，kafka集群： 角色：leader,follower 解读：只能读写leader，不可以从follower中读取数据，依靠zookeeper选举

3，rocketmq集群： 角色：master,slave 解读：只能写master，可以从master和slave中读取数据
   
4，redis集群： 角色：master,slave 解读：只能写master，可以从master和slave中读取数据

### 41 基于MQ实现订单数据同步给大数据团队，应该如何设计？

#### 大数据团队的几百行大SQL是如何影响订单数据库的？

实际上要解决这个问题，就必须要避免大数据团队直接查询订单数据库

比如最简单的办法，就是将订单数据落地到大数据团队自己的一个MySQL数据库中，然后从自己的MySQL数据库里统计报表。

还有MySQL Binlog同步系统

这种系统会监听MySQL数据库的Binlog，所谓Binlog大致可以理解为MySQL的增删改操作日志。

然后MySQL Binlog同步系统会将监听到的MySQL Binlog（也就是增删改操作日志）发送给你的系统，让你来处理这些增删改操作日志。

实际上大数据团队并没有必要仅仅只通过MySQL来出数据报表，完全可以采用Hadoop、Spark、Flink等大数据技术来出数据报表。

<img src="https://s1.ax1x.com/2020/07/22/UbTSOJ.jpg" alt="UbTSOJ.jpg" border="0" />

方案1: 刚开始的方案设计是通过job的方式拉去业务方的数据库来插入数据或更新es的数据存在时效性问题和业务数据多了厚性能问题，

方案2: 后来改造为canal监控binlog日志再通过canal adpater适配器写数据到rocketmq，消费者在消费消息进行业务插入后插入到es中

### 42 授人以渔：对其他团队要获取你们核心数据的问题，应该如何解决？

方案3:  mysql 主从, 主给业务系统用, 从给其他系统用

### 43 秒杀系统的技术难点以及秒杀商品详情页系统的架构设计

下问题清单：
1. 下单核心流程环节太多，性能较差
1. 订单退款的流程可能面临退款失败的风险
1. 关闭过期订单的时候，存在扫描大量订单数据的问题
1. 跟第三方系统耦合在一起，性能存在抖动的风险
1. 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
1. 做秒杀活动时订单数据库压力过大

1,4,5 都解决了


#### 秒杀活动压力过大怎么办？难道是加机器吗？
+ 第一个问题，秒杀活动目前压力过大，应该如何解决？是不是简单的堆机器或者加机器就可以解决的？
    
    这个是没问题的，订单系统自己是可以通过部署更多的机器进行线性扩展的。
+ 第二个问题来了，那么数据库呢？是不是也要部署更多的服务器，进行分库分表，然后让更多的数据库服务器来抗超高的数据库高并发访问？
    
    这个思路是这样的，所谓分库分表，就是把目前的一台数据库服务器变成多台数据库服务器，然后把一张订单表变成多张订单表。
    
答案是不太靠谱的，除非是技术能力比较弱的公司，没有厉害的架构师去利用已有的技术合理设计优秀的架构，才会用这种堆机器的方
法简单的来抗下超高的并发。

因为如果用堆机器的方法来解决这个问题，必然存在一个问题，就是随着你的用户量越来越大，你的并发请求越来越多，会导致你要不
停的增加更多的机器

#### 不归订单管的部分：高并发的商品详情页请求
其实秒杀活动主要涉及到的并发压力就是两块，一个是高并发的读，一个是高并发的写。

在秒杀的时候,必然出现一种场景，就是首先大量用户会拿着APP不停的刷新一个秒杀商品的页面

本质上来说是从商品技术团队负责的商品详情页系统中加载出来的

在秒杀活动的时候，他面临的第一个问题就是，可能几十万人，甚至百万级的用户，会同一时间频繁的访问同一个秒杀商品的页面
比如“3折抢购原价6888的手机，限售100台”这样的活动，可能有几十万人在8:30之前会集中访问这个秒杀商品的活动页面，对商品
详情页系统造成过巨大的访问压力。

#### 商品团队的秒杀架构优化：页面数据静态化

实际上商品技术团队针对这个问题，采取的是页面数据静态化+多级缓存的方案。

以首先需要将这个秒杀活动的商品详情页里的数据做成静态化的，也就是说提前就从数据库里把这个页面需要的数据都提取出来组装
成一份静态数据放在别的地方，避免每次访问这个页面都要访问后端数据库。

#### 商品团队的秒杀架构优化：多级缓存

多级缓存的架构，我们会使用CDN + Nginx + Redis的多级缓存架构

可以将一些静态化好的数据放在就近的一个CDN上。

这个CDN缓存就是我们多级缓存架构里的第一级缓存。

那如果因为缓存过期之类的问题，CDN上没有用户要加载的商品详情页数据怎么办呢？

此时用户就会发送请求到我们公司的机房里的机器上去请求加载这个商品的数据了，这个时候我们需要在Nginx这样的服务器里做一级
缓存。

在Nginx中是可以基于Lua脚本实现本地缓存的，我们可以提前把秒杀商品详情页的数据放到Nginx中进行缓存，如果请求发送过来，
可以从Nginx中直接加载缓存数据，不需要把请求转发到我们商品系统上去

Nginx上的缓存数据过期之类的问题，导致没找到我们需要的数据?

此时就可以由Nginx中的Lua脚本发送请求到Redis集群中去加载我们提前放进去的秒杀商品数据

如果在Redis中还是没有找到呢？

那么就由Nginx中的Lua脚本直接把请求转发到商品详情页系统里去加载就可以了，此时就会直接从数据库中加载数据出来

### 44 授人以渔：你们有没有类似秒杀的业务场景？如果没有，自己想一个出来！

+ 秒杀的场景，应该采取差不多的方案，cdn+nginx缓存+redis缓存商详的静态页面化。后台可以通过redis扣减库存，mq更新数据操作。

+ 二级缓存架构， Redis缓存+Eache本地缓存，库存这些数据也是直接提前从数据预热到Redis缓存中，基于Redis做库存扣减，秒杀页面是动态，通过js使用ajax分模块加载不同的商品信息缓存，后台有商品基本数据介绍变更直接发送到MQ，然后秒杀服务监听MQ去缓存这些数据到Redis和本地缓存中。后期数量大了可以按照文章说的三级缓存架构+商品页面通过Nginx静态化生成

### 45 基于MQ实现秒杀订单系统的异步化架构以及精准扣减库存的技术方案

> 访问的问题 解决了,还有下单的问题

#### 用答题的方法避免作弊抢购以及延缓下单
考虑第一个问题，有没有可能会有人自己写一个抢购的脚本或者作弊软件，疯狂的发送请求去抢商品

答案是肯定的，肯定是有人会写作弊的脚本或者软件

所以一般来说，现在你要参与抢购，都会让你点击按钮之后先进行答题，就是说先弹出来一个框，让你回答一个问题，回答正确了你才
能发起抢购的请求

#### 为秒杀独立出来一套订单系统
用户的下单抢购的请求发送出去之后，会达到我们的后台系统，对于后台系统而言，我们需要思考一下，是否直接使用我们目前已
有的订单系统去抗所有的请求？

答案是否定的，这么做会有问题。

假设你有100万用户在这个时间段很活跃都会来购买商品，但是可能只有其中50万用户在参与秒杀活动，同一时间发送了大量的抢购请
求到后台系统，但是同时还有很多其他的用户这个时候并不在参与秒杀系统，他们在进行其他商品的常规性浏览和下单。

因此这个时候如果你让秒杀下单请求和普通下单请求都由一套订单系统来承载，那么可能会导致秒杀下单请求耗尽了订单系统的资源，
或者导致系统不稳定，然后导致其他普通下单请求也出现问题，没有办法完成的下单。

所以一般我们会对订单系统部署两个集群，一个集群是秒杀订单系统集群，一个集群是普通订单系统集群

#### 基于Redis实现下单时精准扣减库存

首先需要做的一个事情，就是扣减库存。

因为大家都知道，秒杀商品一般是有数量的限制的，比如几十万人可能就抢购1万个特价商品。

所以当大量的请求到达后台系统之后，首先第一步，就可以先去扣减库存。

扣减库存应该怎么来扣呢？如果还是直接由订单系统调用库存系统的接口，然后访问库存数据库去扣减，那么势必导致瞬时压力过大，可能让库存系统的压力很大。

因此在秒杀场景下，一般会采用另外一个思路。

通常在秒杀场景下，一般会将每个秒杀商品的库存提前写入Redis中，然后当请求到来之后，就直接对Redis中的库存进行扣减

Redis是可以轻松用单机抗每秒几万高并发的，因此这里就可以抗下高并发的库存扣减

#### 抢购完毕之后提前过滤无效请求
其实在Redis中的库存被扣减完之后，就说明后续其他的请求都没有必要发送到秒杀系统中了，因为商品已经被抢购完毕了

此时我们可以让Nginx在接收到后续请求的时候，直接就把后续请求过滤掉。

比如一旦商品抢购完毕，可以在ZooKeeper中写入一个秒杀完毕的标志位，然后ZK会反向通知Nginx中我们自己写的Lua脚本，通过

Lua脚本后续在请求过来的时候直接过滤掉，不要向后转发了。

#### 瞬时高并发下单请求进入RocketMQ进行削峰

哪怕是有1万件商品同时被1万人秒杀成功了，那么可能瞬间会有1万请求涌入正常的订单系统进行后续的处理，此时可能还是会有瞬间上万请求访问到订单数据库中创建订单。

所以这个时候，完全可以引入RocketMQ进行削峰处理

对于秒杀系统而言，如果判断发现通过Redis完成了库存扣减，此时库存还大于0，就说明秒杀成功了需要生成订单，此时就
直接发送一个消息到RocketMQ中即可。

然后让普通订单系统从RocketMQ中消费秒杀成功的消息进行常规性的流程处理即可，比如创建订单，等等。

这样的话，瞬间上万并发的压力会被RocketMQ轻松抗下来，然后普通的订单系统可以根据自己的工作负载慢慢的从RocketMQ中拉取

秒杀成功的消息，然后进行后续操作就可以了，不会对订单数据库造成过大的压力。

否则如果你让瞬间产生的一万或者几万的订单请求直接访问订单数据库，必然还是会让他压力过大，需要额外增加机器，那是没有必要
的。

因此在这里利用RocketMQ抗下每秒几万并发的下单请求，然后让订单系统以每秒几千的速率慢慢处理就可以了，也就是延迟个可能几
十秒，这些下单请求就会处理完毕。

#### 秒杀架构的核心要点
较重要的有以下几点：
1. 在前端/客户端设置秒杀答题，错开大量人下单的时间，阻止作弊器刷单
2. 独立出来一套秒杀系统，专门负责处理秒杀请求
3. 优先基于Redis进行高并发的库存扣减，一旦库存扣完则秒杀结束
4. 秒杀结束之后，Nginx层过滤掉无效的请求，大幅度削减转发到后端的流量
5. 瞬时生成的大量下单请求直接进入RocketMQ进行削峰，订单系统慢慢拉取消息完成下单操作

首先必须要避免直接基于数据库进行高并发的库存扣减

后续占据99%的请求，都可以直接在Nginx层面被拦截掉，不会转发到后台系统造成任何压力

用入RocketMQ中进行削峰，让RocketMQ轻松抗下高并发压力，让订单系统慢慢消费和处理下单操作

架构优化的核心就是独立出来一套系统专门处理，避免高并发请求落在MySQL上

因为MySQL天生不擅长抗高并发，我们需要通过Redis、Nginx、RocketMQ这些天生轻松可以单机抗几万甚至十万并发的系统来优化架构。


### 46 授人以渔：如果你们有类似秒杀的瞬时高并发场景，应该如何改造？
> mq 削峰, redis 先处理数据,异步同步到数据库

### 47 阶段性复习：一张思维导图给你梳理全面引入MQ的订单系统架构

<img src="https://s1.ax1x.com/2020/07/23/UbXRx0.jpg" alt="UbXRx0.jpg" border="0" />

### 48 阶段性复习：思考一下，如果你们系统全面接入MQ，架构该如何设计？

问题：

1. 你们的系统中是否存在核心链路环节过多导致性能较差的问题？如果有的话，是否可以引入MQ进行适当异步化提升链路性能？

1. 你们的系统是否存在核心链路耦合了第三方系统，进而导致链路性能不稳定的问题？如果有，是否可以引入MQ进行第三方系统的解耦，避免核心链路的性能受到影响？
 
1. 你们的系统是否存在有其他团队直接耦合访问你们数据库的情况，进而导致你们的数据库性能不稳定？如果有的话，是否可以引入MQ来推送你们的核心数据出去，跟其他团队进行解耦？
 
1. 你们的系统是否存在瞬时超高并发的场景？如果有的话，是否可以引入MQ来进行瞬时流量削峰，避免为了应对瞬时超高并发从而不停的增加机器？


### 49 精益求精：深入研究一下生产者到底如何发送消息的？
问题清单：
1. 下单核心流程环节太多，性能较差
1. 订单退款的流程可能面临退款失败的风险
1. 关闭过期订单的时候，存在扫描大量订单数据的问题
1. 跟第三方系统耦合在一起，性能存在抖动的风险
1. 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
1. 做秒杀活动时订单数据库压力过大

1,4,5,6 解决

#### 研究RocketMQ底层原理的顺序和思路

思路:
1. 对生产者往Broker集群发送消息的底层原理做一个研究
2. 看看Broker对于接收到的消息，到底是如何存储到磁盘上去的？
3. 基于DLedger技术部署的Broker高可用集群，到底如何进行数据同步的？
4. 消费者到底是基于什么策略选择Master或Slave拉取数据的？
5. 消费者是如何从Broker拉取消息回来，进行处理以及ACK的？如果消费者故障了会如何处理？

#### 创建Topic的时候为何要指定MessageQueue数量？
在创建Topic的时候需要指定一个很关键的参数，就是MessageQueue

就是你要指定你的这个Topic对应了多少个队列，也就是多少个MessageQueue。


#### Topic、MessageQueue以及Broker之间到底是什么关系？

MessageQueue就是RocketMQ中非常关键的一个数据分片机制，他通过MessageQueue将一个Topic的数据拆分为了很多个数据分片，然后在每个Broker机器上都存储一些MessageQueue。

通过这个方法，就可以实现Topic数据的分布式存储！

#### 生产者发送消息的时候写入哪个MessageQueue？

生产者会跟NameServer进行通信获取Topic的路由数据。

所以生产者从NameServer中就会知道，一个Topic有几个MessageQueue，哪些MessageQueue在哪台Broker机器上，哪些MesssageQueue在另外一台Broker机器上，这些都会知道

生产者会根据策略的把消息写入各个MessageQueue

可以让一个Topic中的数据分散在多个MessageQueue中，进而分散在多个Broker机器上. 这样就可以实现RocketMQ集群分布式存储海量的消息数据了！

#### 如果某个Broker出现故障该怎么办？
如果某个Broker临时出现故障了，比如Master Broker挂了，此时正在等待的其他Slave Broker自动热切换为Master Broker，那么这个时候对这一组Broker就没有Master Broker可以写入了

Producer中开启一个开关，就是sendLatencyFaultEnable

一旦打开了这个开关，那么他会有一个自动容错机制，比如如果某次访问一个Broker发现网络延迟有500ms，然后还无法访问，那么
就会自动回避访问这个Broker一段时间，比如接下来3000ms内，就不会访问这个Broker了。

这样的话，就可以避免一个Broker故障之后，短时间内生产者频繁的发送消息到这个故障的Broker上去，出现较多次数的异常。而是在
一个Broker故障之后，自动回避一段时间不要访问这个Broker，过段时间再去访问他。

### 50 授人以渔：Kafka、RabbitMQ有类似MessageQueue的数据分片机制吗

总结：kafka中分片是topic下分为多个partition，partition分为primary shard和副本等同于rocketmq master broker的messagequeue和slave，当发送消息时通过zk上的协调节点和zk存储topic消息对应partition来知道发送消息对应的partition在那个节点上

rabbitMQ没有，因为它仅仅是一个主从的系统，有两种模式，普通集群模式和镜像集群模式。在普通集群模式场景下，消息的元数据会存在于每个rabbitMQ的broker上，但是消息的本体只存在于一个broker上，这种情况有单点问题；而镜像集群模式是，每条消息在每个broker上都有，即每个broker有全量的消息，这种情况虽然没有单点问题，但是不能水平扩展。

但 RabbitMQ 官方有一个 sharding 的插件可以支持队列分片；https://github.com/rabbitmq/rabbitmq-sharding

### 51 精益求精：深入研究一下Broker是如何持久化存储消息的？

>Broker数据存储实际上才是一个MQ最核心的环节，他决定了生产者消息写入的吞吐量，决定了消息不能丢失，决定了消费者获取消息的吞吐量，这些都是由他决定的。

#### CommitLog消息顺序写入机制
+ 第一步，他会把这个消息直接写入磁盘上的一个日志文件，叫做CommitLog，直接顺序写入这个文件
    
    这个CommitLog是很多磁盘文件，每个文件限定最多1GB，Broker收到消息之后就直接追加写入这个文件的末尾，就跟上面的图里一
    样。如果一个CommitLog写满了1GB，就会创建一个新的CommitLog文件。
    
####  MessageQueue在数据存储中是体现在哪里呢？
+ 对Topic下的每个MessageQueue都会有一系列的ConsumeQueue文件。

    在Broker的磁盘上，会有下面这种格式的一系列文件：
    $HOME/store/consumequeue/{topic}/{queueId}/{fileName}
    
    {topic}指代的就是某个Topic，

    {queueId}指代的就是某个MessageQueue。
    
    然后对存储在这台Broker机器上的Topic下的一个MessageQueue，他有很多的ConsumeQueue文件，这个ConsumeQueue文件里
    存储的是一条消息对应在CommitLog文件中的offset偏移量。
    
当你的Broker收到一条消息先写入了CommitLog, 之后，其实他同时会将这条消息在CommitLog中的物理位置，也就是一个文
件偏移量，就是一个offset，写入到这条消息所属的MessageQueue对应的ConsumeQueue文件中去

实际上在ConsumeQueue中存储的每条数据不只是消息在CommitLog中的offset偏移量，还包含了消息的长度，以及tag
hashcode，一条数据是20个字节，每个ConsumeQueue文件保存30万条数据，大概每个文件是5.72MB。

所以实际上Topic的每个MessageQueue都对应了Broker机器上的多个ConsumeQueue文件，保存了这个MessageQueue的所有消息
在CommitLog文件中的物理位置，也就是offset偏移量。

#### 如何让消息写入CommitLog文件近乎内存写性能的？

> Broker是基于OS操作系统的PageCache和顺序写两个机制，来提升写入CommitLog文件的性能的。

首先Broker是以顺序的方式将消息写入CommitLog磁盘文件的，也就是每次写入就是在文件末尾追加一条数据就可以了，对文件进行
顺序写的性能要比对文件随机写的性能提升很多

另外，数据写入CommitLog文件的时候，其实不是直接写入底层的物理磁盘文件的，而是先进入OS的PageCache内存缓存中，然后后
续由OS的后台线程选一个时间，异步化的将OS PageCache内存缓冲中的数据刷入底层的磁盘文件。

采用磁盘文件顺序写+OS PageCache写入+OS异步刷盘的策略，基本上可以让消息写入CommitLog的性能
跟你直接写入内存里是差不多的，所以正是如此，才可以让Broker高吞吐的处理每秒大量的消息写入。

#### 同步刷盘与异步刷盘

上述的异步刷盘模式下，生产者把消息发送给Broker，Broker将消息写入OS PageCache中，就直接返回ACK给生产者了

有一定几率丢失消息

另外一种模式叫做同步刷盘，如果你使用同步刷盘模式的话，那么生产者发送一条消息出去，broker收到了消息，必须直接强制把这个
消息刷入底层的物理磁盘文件中，然后才会返回ack给producer，此时你才知道消息写入成功了。

除非是你的物理磁盘坏了导致数据丢失，否则正常来说数据就不会丢失了  ,但效率低

### 52 授人以渔：同步刷盘和异步刷盘分别适用于什么场景呢？

 1、rocket存储是存放到一个commitlog磁盘文件，所有的topic消息存储在一起，默认1g,
 
 为什么是1g，因为内存映射读取文件会传入偏移量，long类型再转化为int类型，32位所以最大为2g.
 
 在写入commitlog后，会有异步线程写入consumequeue文件，按topic和queue划分，存储了消息的在commitlog物理偏移量，消息大小等其他属性。
 
 commitlog采用顺序写和页缓存保证高效写入。
 
 页缓存每页8k可以设置多少页才刷盘一次。先讲消息写入缓存，再异步线程去将缓存刷入磁盘。
 
 同步刷盘数据不丢失(发送消息ack成功并且磁盘有数据)，异步刷盘吞吐量高，但broker宕机还没有刷入磁盘的内存数据会丢失。
 
 对于订单生成，金额相关可以同步，对于边缘业务可以异步。
 

### 53 精益求精：基于DLedger技术的Broker主从同步原理到底是什么？

> producer写入消息到broker之后，broker会将消息写入本地CommitLog磁盘文件里去，然后
  还有一些ConsumeQueue会存储Topic下各个MessageQueue的消息的物理位置。
  
 > 如果要让Broker实现高可用，那么必须有一个Broker组，里面有一个是Leader Broker可以写入数据，然后让
   Leader Broker接收到数据之后，直接把数据同步给其他的Follower Broker


#### 基于DLedger技术替换Broker的CommitLog

Broker上述高可用架构就是基于DLedger技术来实现的

DLedger技术实际上首先他自己就有一个CommitLog机制，你把数据交给他，他会写入CommitLog磁盘文件里去，这是他能干的第一件事情。

如果基于DLedger技术来实现Broker高可用架构，实际上就是用DLedger先替换掉原来Broker自己管理的CommitLog，由DLedger来管理CommitLog

<img src="https://s1.ax1x.com/2020/07/23/UOLa8A.jpg" alt="UOLa8A.jpg" border="0" />

#### DLedger是如何基于Raft协议选举Leader Broker的？

首先基于DLedger替换各个Broker上的CommitLog管理组件了，那么就是每个Broker上都有一个DLedger组件了

实际上DLedger是基于Raft协议来进行Leader Broker选举的

发起一轮一轮的投票，通过三台机器互相投票选出来一个人作为Leader

+ 三台Broker机器启动的时候，他们都会投票自己作为Leader，然后把这个投票发送给其他Broker。

    例子，Broker01是投票给自己的，Broker02是投票给自己的，Broker03是投票给自己的，他们都把自己的投票发送给了别
    人。
    
    此时在第一轮选举中，Broker01会收到别人的投票，他发现自己是投票给自己，但是Broker02投票给Broker02自己，Broker03投票给
    Broker03自己，似乎每个人都很自私，都在投票给自己，所以第一轮选举是失败的。
    
 + 接着每个人会进入一个随机时间的休眠，比如说Broker01休眠3秒，Broker02休眠5秒，Broker03休眠4秒。
    
    此时Broker01必然是先苏醒过来的，他苏醒过来之后，直接会继续尝试投票给自己，并且发送自己的选票给别人。
    
    接着Broker03休眠4秒后苏醒过来，他发现Broker01已经发送来了一个选票是投给Broker01自己的，此时他自己因为没投票，所以会
    尊重别人的选择，就直接把票投给Broker01了，同时把自己的投票发送给别人。  2也一样
    
    只要有（3台机器 / 2） + 1个人投票给某个人，就会选举他当Leader，这个（机器数量 / 2） + 1就是大多数的意思
    
依靠这个随机休眠的机制，基本上几轮投票过后，一般都是可以快速选举出来一个Leader。

### DLedger是如何基于Raft协议进行多副本同步的？

> 和zk 的,多数提交+2pc 差不多

数据同步会分为两个阶段，一个是uncommitted阶段，一个是commited阶段

+ 首先Leader Broker上的DLedger收到一条数据之后，会标记为uncommitted状态，然后他会通过自己的DLedgerServer组件把这个uncommitted数据发送给Follower Broker的DLedgerServer。

+ 接着Follower Broker的DLedgerServer收到uncommitted消息之后，必须返回一个ack给Leader Broker的DLedgerServer，然后如
果Leader Broker收到超过半数的Follower Broker返回ack之后，就会将消息标记为committed状态。

+ 然后Leader Broker上的DLedgerServer就会发送commited消息给Follower Broker机器的DLedgerServer，让他们也把消息标记为
comitted状态。

#### 如果Leader Broker崩溃了怎么办？

如果Leader Broker挂了，此时剩下的两个Follower Broker就会重新发起选举，他们会基于DLedger还是采用Raft协议的算法，去选举
出来一个新的Leader Broker继续对外提供服务，而且会对没有完成的数据同步进行一些恢复性的操作，保证数据不会丢失。

### 54 授人以渔：采用Raft协议进行主从数据同步，会影响TPS吗？

会降低tps，其实和刷盘一样道理，可以采用异步同步的方式提高tps，最终还是得看使用场景采取合适的方式

### 55 精益求精：深入研究一下消费者是如何获取消息处理以及进行ACK的？

#### 消费组到底是个什么概念？
消费者组的意思，就是让你给一组消费者起一个名字。比如我们有一个Topic叫“TopicOrderPaySuccess”，然后假设有库存系统、积分系统、营销系统、仓储系统他们都要去消费这个Topic中的数据。

正常情况下来说，这条消息进入Broker之后，库存系统和营销系统作为两个消费组，每个组都会拉取到这条消息。

也就是说这个订单支付成功的消息，库存系统会获取到一条，营销系统也会获取到一条，他们俩都会获取到这条消息

但一般情况下来说，库存系统的两台机器中只有一台机器会获取到这条消息，营销系统也是同理。

#### 集群模式消费 vs 广播模式消费
> 默认消费 ,就是集群模式,消费组内有一个消费者消费

为广播模式：
consumer.setMessageModel(MessageModel.BROADCASTING);

修改为广播模式，那么对于消费组获取到的一条消息，组内每台机器都可以获取到这条消息。但是相对而言广播模式其实用的很
少，常见基本上都是使用集群模式来进行消费的。


#### MessageQueue、CommitLog、ConsumeQueue之间的关系
Topic中的多个MessageQueue会分散在多个Broker上，在每个Broker机器上，一个MessageQueue就对应了一个ConsumeQueue，
当然在物理磁盘上其实是对应了多个ConsumeQueue文件的，但是我们大致也理解为一 一对应关系。

然后对于Topic的各个MessageQueue而言，就是通过各个ConsumeQueue文件来存储属于MessageQueue的消息在CommitLog文
件中的物理地址，就是一个offset偏移量

#### MessageQueue与消费者的关系

>  最好MessageQueue和消费者 数量对应

一个Topic的多个MessageQueue会均匀分摊给消费组内的多个机器去消费，这里的一个原则就是，一个
MessageQueue只能被一个消费机器去处理，但是一台消费者机器可以负责多个MessageQueue的消息处理

#### Push模式 vs Pull模式

一个消费组内的多台机器是分别负责一部分MessageQueue的消费的

两种消费模式了，一个是Push，一个是Pull。

实际上，这两个消费模式本质是一样的，都是消费者机器主动发送请求到Broker机器去拉取一批消息下来

Push模式的实现思路我这里简单说一下：当消费者发送请求到Broker去拉取消息的时候，如果有新的消息可以消费那么就会立马返回
一批消息到消费机器去处理，处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。

消息处理的时效性非常好，看起来就跟Broker一直不停的推送消息到消费机器一样。

Push模式下有一个请求挂起和长轮询的机制，也要给大家简单介绍一下。

当你的请求发送到Broker，结果他发现没有新的消息给你处理的时候，就会让请求线程挂起，默认是挂起15秒，然后这个期间他会有
后台线程每隔一会儿就去检查一下是否有的新的消息给你，另外如果在这个挂起过程中，如果有新的消息到达了会主动唤醒挂起的线
程，然后把消息返回给你。

#### Broker是如何将消息读取出来返回给消费机器的？

+ 假设一个消费者机器发送了拉取请求到Broker了，他说我这次要拉取MessageQueue0中的消息，然后我之前都没拉取过消息，所以就
从这个MessageQueue0中的第一条消息开始拉取好了。

    于是，Broker就会找到MessageQueue0对应的ConsumeQueue0，从里面找到第一条消息的offset

    接着Broker就需要根据ConsumeQueue0中找到的第一条消息的地址，去CommitLog中根据这个offset地址去读取出来这条消息的数
    据，然后把这条消息的数据返回给消费者机器
    
其实消费消息的时候，本质就是根据你要消费的MessageQueue以及开始消费的位置，去找到对应的ConsumeQueue读取里面对
应位置的消息在CommitLog中的物理offset偏移量，然后到CommitLog中根据offset读取消息数据，返回给消费者机器。

#### 消费者机器如何处理消息、进行ACK以及提交消费进度？

当我们处理完这批消息之后，消费者机器就会提交我们目前的一个消费进度到Broker上去，然后Broker就会存储我们的消费进度

那么他会记录下来一个ConsumeOffset的东西去标记我们的消费进度

那么下次这个消费组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始继续拉取，不用重头开始拉取了。

#### 如果消费组中出现机器宕机或者扩容加机器，会怎么处理？

会进入一个rabalance的环节，也就是说重新给各个消费机器分配他们要处理的MessageQueue。

大家举个例子，比如现在机器01负责MessageQueue0和Message1，机器02负责MessageQueue2和MessageQueue3，现在机器
02宕机了，那么机器01就会接管机器02之前负责的MessageQueue2和MessageQueue3。

ps: 扩容,重新分配可能出现重复消费


### 56 授人以渔：消费者到底什么时候可以认为是处理完消息了？

+ 消费组：
    消费topic的一个组群；不同消费组能同时拉取到消息；公用相同的consumequeue，但是储存的消费进度不一样；
    
    消费进度是以topic@消费组名为key，存储消费进度的；"topic@_group":{6:13,5:15,0:28,7:13,2:20,1:23,3:17,4:16} 由于线上都是集群部署消费者，有多个消费者实例（ip@instanceName），消费者启动的时候，就会重新消费的负载，在消费客服端的宕机与扩容都会重新分配新的messagequeue；
    
    包装一个messagequeue只能被一个consumer消费，一个consumer可以消费多个topic的messagqueue 集群模式：一个messagequeue只能一个consume instance消费,消费进度存在服务端；
    
    广播模式：一个messagequeue可以被所有的消费端消费，消费进度存储在客户端； 
    
    topic：主题，存储的时候对数据进行分片，多个messagequeue；
    
    多个topic的数据存储在commitlog磁盘；每个topic都对应一个consumequeue，consumequeue含多个文件，
    
    每个文件30万条数据记录，每条数据20字节，每个文件5.85m；前8字节存commitlog的物理偏移量，4位存长度，8位存tags和hashcode；
    
    consumeOffset存的是consumequeue的逻辑偏移量即下标

+ ack模式有两种  一种自动还有一种手动

    默认每次拉取32条消息，可以设置每次消费为1；

+ 一个消费组内多台机器分配messagequeue，有5种算法，保证一个message只能被一个consume消费； 

    5、主要从brokerpull取，push模式即consumer和broker建立长连接，有消息就一直拉取，处理完一批消息后就去处理下一批消息；
    
    没有消息时会挂起，有消息达到会后台线程唤醒； 从哪里开始获取消费进度还需要去翻源码；
     
     6、在消费进度获取到consumequeue的逻辑偏移量，找到queue对应的consumequeue的下标，获取commitlig的物理偏移量和消息长度，通过tags去过滤消息，通过mmap内存映射读取文件返回给消费端； 
     
     7、消息消费者通过回调，获取到消息处理完后通过ack机制告诉服务端消息是否消费成功，成功后提交消费进度，消费进度不是每消费一次就立马更新一次； 
     
     8、有后台线程去轮询消费端与服务端的负载关系与重新分配；
     

### 57 精益求精：消费者到底是根据什么策略从Master或Slave上拉取消息的？

> 消息消费，可以从Master Broker拉取，也可以从Slave Broker拉取，具体是要看机器负载来定。

刚开始消费者都是连接到Master Broker机器去拉取消息的，然后如果Master Broker机器觉得自己负载比较高，就会
告诉消费者机器，下次可以从Slave Broker机器去拉取。

#### CommitLog基于os cache提升写性能的回顾

这个ConsumeQueue文件的读取是如何进行性能优化的，其实他本质就是基于os cache来进行优化的

1. broker收到一条消息，会写入CommitLog文件，但是会先把CommitLog文件中的数据写入os cache(操作系统管理的缓存)

2. 然后os自己有后台线程，过一段时间后会异步把os cache缓存中的CommitLog文件的数据刷入磁盘中去

就是依靠这个写入CommitLog时先进入os cache缓存，而不是直接进入磁盘的机制，就可以实现broker写CommitLog文件的性能是
内存写级别的，这才能实现broker超高的消息接入吞吐量

#### 一个很关键的问题：ConsumeQueue文件也是基于os cache的

关键的问题，那就是ConsumeQueue会被大量的消费者发送的请求给高并发的读取，所以
ConsumeQueue文件的读操作是非常频繁的，而且同时会极大的影响到消费者进行消息拉取的性能和消费吞吐量。

所以实际上broker对ConsumeQueue文件同样也是基于os cache来进行优化的

也就是说，对于Broker机器的磁盘上的大量的ConsumeQueue文件，在写入的时候也都是优先进入os cache中的

而且os自己有一个优化机制，就是读取一个磁盘文件的时候，他会自动把磁盘文件的一些数据缓存到os cache中。

而且大家之前知道ConsumeQueue文件主要是存放消息的offset，所以每个文件很小，30万条消息的offset就只有5.72MB而已。所以

实际上ConsumeQueue文件们是不占用多少磁盘空间的，他们整体数据量很小，几乎可以完全被os缓存在内存cache里。

所以实际上在消费者机器拉取消息的时候，第一步大量的频繁读取ConsumeQueue文件，几乎可以说就是跟读内存里的数据的性能是
一样的，通过这个就可以保证数据消费的高性能以及高吞吐

#### 第二个关键问题：CommitLog是基于os cache+磁盘一起读取的

看第二个比较关键的问题，在进行消息拉取的时候，先读os cache里的少量ConsumeQueue的数据，这个性能是极高的，
然后第二步就是要根据你读取到的offset去CommitLog里读取消息的完整数据了。

答案是：两者都有

因为CommitLog是用来存放消息的完整数据的，所以内容量是很大的，毕竟他一个文件就要1GB，所以整体完全有可能多达几个TB。

明显是不可能的，因为os cache用的也是机器的内存，一般多也就几十个GB而已，何况Broker自身的JVM也要用一些内存，留个os
cache的内存只是一部分罢了，比如10GB~20GB的内存，所以os cache对于CommitLog而言，是无法把他全部数据都放在里面给你
读取的！

也就是说，os cache对于CommitLog而言，主要是提升文件写入性能，当你不停的写入的时候，很多最新写入的数据都会先停留在os
cache里，比如这可能有10GB~20GB的数据。

之后os会自动把cache里的比较旧的一些数据刷入磁盘里，腾出来空间给更新写入的数据放在os cache里，所以大部分数据可能多达几
个TB都是在磁盘上的

最终结论来了，当你拉取消息的时候，可以轻松从os cache里读取少量的ConsumeQueue文件里的offset，这个性能是极高的，
但是当你去CommitLog文件里读取完整消息数据的时候，会有两种可能。

+ 第一种可能，如果你读取的是那种刚刚写入CommitLog的数据，那么大概率他们还停留在os cache中，此时你可以顺利的直接从os
cache里读取CommitLog中的数据，这个就是内存读取，性能是很高的。

+ 第二种可能，你也许读取的是比较早之前写入CommitLog的数据，那些数据早就被刷入磁盘了，已经不在os cache里了，那么此时你
就只能从磁盘上的文件里读取了，这个性能是比较差一些的。

#### 什么时候会从os cache读？什么时候会从磁盘读？

这个问题很简单了，如果你的消费者机器一直快速的在拉取和消费处理，紧紧的跟上了生产者写入broker的消息速率，那么你每次
拉取几乎都是在拉取最近人家刚写入CommitLog的数据，那几乎都在os cache里。

但是如果broker的负载很高，导致你拉取消息的速度很慢，或者是你自己的消费者机器拉取到一批消息之后处理的时候性能很低，处理
的速度很慢，这都会导致你跟不上生产者写入的速率。

#### Master Broker什么时候会让你从Slave Broker拉取数据？

问题的解答，本质是对比你当前没有拉取消息的数量和大小，以及最多可以存放在os cache内存里的消息的大小，

如果你没拉取的消息超过了最大能使用的内存的量，那么说明你后续会频繁从磁盘加载数据，此时就让你从slave broker去加载数据了！

比如: os cache里面是5万-8万的数据, 你的偏移量是在2万, 那你还会把 2万-5万的数据 读到cache  ,

### 58 授人以渔：消费者是跟所有Broker建立连接，还是跟部分Broker建立连接？


（1）消费者机器到底是跟少数几台Broker建立连接，还是跟所有Broker都建立连接？这是不少朋友之前在评论区提出的问题，但是我
想这里大家肯定都有自己的答案了。

（2）RocketMQ是支持主从架构下的读写分离的，而且什么时候找Slave Broker读取大家也都了解的很清楚了，那么大家思考一下，
Kafka、RabbitMQ他们支持主从架构下的读写分离吗？支持Slave Broker的读取吗？为什么呢？

（3）如果支持读写分离的话，有没有一种可能，就是出现主从数据不一致的问题？比如有的数据刚刚到Master Broker和部分Slave
Broker，但是你刚好是从那个没有写入数据的Slave Broker去读取了？

（4）消费吞吐量似乎是跟你的处理速度有很大关系，如果你消费到一批数据，处理太慢了，会导致你严重跟不上数据写入的速度，这
会导致你后续几乎每次拉取数据都会从磁盘上读取，而不是os cache里读取，所以你觉得你在拉取到一批消息处理的时候，应该有哪些
要点需要注意的？


1. 消费者机器跟少数的broker建立长连接。
    会跟nameserver这台机器建立连接，然后去找对应的监听消费者的机器去连接，不是所有的机器都会连接的

2. 当主节点内存不够，积压比较多会从从节点拉取。

3. 主节点挂掉  肯定会存在主从数据不一致，延迟的情况。所以为了考虑消息的一致性，主从同步双写。
4. 消费消息时提高响应速度。


### 59 探秘黑科技：RocketMQ 是如何基于Netty扩展出高性能网络通信架构的？

#### Reactor主线程与长短连接

+ Reactor模式: (netty )
    
    两个线程池, 主线程池和工作线程池,
    
+ 长连接、短连接
    
    长连接:你建立一个连接 -> 发送请求 -> 接收响应 -> 发送请求 -> 接收响应 -> 发送请求 -> 接收响应
     
     大家会发现，当你建立好一个长连接之后，可以不停的发送请求和接收响应，连接不会断开，等你不需要的时候再断开就行了
                  
    短连接 :   建立连接 -> 发送请求 -> 接收响应 -> 断开连接，下一次你要发送请求的时候，这个过程得重新来一遍

+ 基于Reactor线程池监听连接中的请求
    
    >通过 SocketChannel 打开一个长链接
    
    这个线程池里默认是3个线程！
    
    Reactor主线程建立好的每个连接SocketChannel，都会交给这个Reactor线程池里的其中一个线程去监听请求
    
+ 基于Worker线程池完成一系列准备工作
    
    Worker线程池，他默认有8个线程，此时Reactor线程收到的这个请求会交给Worker线程池中的一个线程进行处理
+ 基于业务线程池完成请求的处理 
    
    Worker线程完成了一系列的预处理之后，比如SSL加密验证、编码解码、连接空闲检查、网络连接管理，等等，接着就需 要对这个请求进行正式的业务处理了
    
    你接收到了消息，肯定是要写入CommitLog文件的，后续还有一些ConsumeQueue之类的事情需要处理，类似这种操作，就是业务处理逻辑
    
+ 为什么这套网络通信框架会是高性能以及高并发的？

    分配一个Reactor主线程出来，就是专门负责跟各种Producer、Consumer之类的建立长连接。
    
    一旦连接建立好之后，大量的长连接均匀的分配给Reactor线程池里的多个线程。
    
    每个Reactor线程负责监听一部分连接的请求，这个也是一个优化点，通过多线程并发的监听不同连接的请求，可以有效的提升大量并
    发请求过来时候的处理能力，可以提升网络框架的并发能力。
    
    接着后续对大量并发过来的请求都是基于Worker线程池进行预处理的，当Worker线程池预处理多个请求的时候，Reactor线程还是可
    以有条不紊的继续监听和接收大量连接的请求是否到达。
    
    而且最终的读写磁盘文件之类的操作都是交给业务线程池来处理的，当他并发执行多个请求的磁盘读写操作的时候，不影响其他线程池
    同时接收请求、预处理请求，没任何的影响。
    
    所以最终的效果就是：
    + Reactor主线程在端口上监听Producer建立连接的请求，建立长连接
    + Reactor线程池并发的监听多个连接的请求是否到达
    + Worker请求并发的对多个请求进行预处理
    + 业务线程池并发的对多个请求进行磁盘读写业务操作 
    

###  60 授人以渔：BIO、NIO、AIO以及Netty之间的关系是什么？

bio 同步阻塞模型，一个线程负责建立负责业务操作，

nio同步非阻塞，监听线程链接请求后注册到selector选择器上 然后通过轮训各个客户端的请求 如果有读写请求则交给另外的线程去处理 ，

netty只不过是对nio进行了封装 加了线程池还有一些操作

### 61 探秘黑科技：基于mmap内存映射实现磁盘文件的高性能读写

#### mmap：Broker读写磁盘文件的核心技术
Broker中大量的使用mmap技术去实现CommitLog这种大磁盘文件的高性能读写优化的。

通过之前的学习，我们知道了一点，就是Broker对磁盘文件的写入主要是借助直接写入os cache来实现性能优化的，因为直接写入os
cache，相当于就是写入内存一样的性能，后续等os内核中的线程异步把cache中的数据刷入磁盘文件即可

#### 传统文件IO操作的多次数据拷贝问题

读写数据,会分别发生两次数据拷贝,一共四次

#### RocketMQ是如何基于mmap技术+page cache技术优化的？
JDK NIO包下的MappedByteBuffer.map()函数干的事情，底层就是基于mmap技术实现的。 他的的本质就是操作对外内存


因为刚开始你建立映射的时候，并没有任何的数据拷贝操作，其实磁盘文件还是停留在那里，只不过他把物理上的磁盘文件的一些地址
和用户进程私有空间的一些虚拟内存地址进行了一个映射

另外这里给大家说明白的一点是，这个mmap技术在进行文件映射的时候，一般有大小限制，在1.5GB~2GB之间

所以RocketMQ才让CommitLog单个文件在1GB，ConsumeQueue文件在5.72MB，不会太大。

这样限制了RocketMQ底层文件的大小，就可以在进行文件读写的时候，很方便的进行内存映射了
     
#### 基于mmap技术+pagecache技术实现高性能的文件读写
接下来就可以对这个已经映射到内存里的磁盘文件进行读写操作了，比如要写入消息到CommitLog文件，你先把一个CommitLog文件
通过MappedByteBuffer的map()函数映射其地址到你的虚拟内存地址。

接着就可以对这个MappedByteBuffer执行写入操作了，写入的时候他会直接进入PageCache中，然后过一段时间之后，由os的线程
异步刷入磁盘中 

他就是从PageCache里拷贝到磁盘文件里而已！这个就是你使用mmap技术之后，相比于传统磁盘IO的一个性能优化。

而且PageCache技术在加载数据的时候，还会将你加载的数据块的临近的其他数据块也一起加载到PageCache里去。

在你读取数据的时候，他会判断PageCache有没有,有的话,其实也仅仅发生了一次拷贝，而不是两次拷贝，所以这个性能相较于传统IO来说，肯定又是提高了。

#### 预映射机制 + 文件预热机制
（1）内存预映射机制：Broker会针对磁盘上的各种CommitLog、ConsumeQueue文件预先分配好MappedFile，也就是提前对一些
可能接下来要读写的磁盘文件，提前使用MappedByteBuffer执行map()函数完成映射，这样后续读写文件的时候，就可以直接执行
了。

（2）文件预热：在提前对一些文件完成映射之后，因为映射不会直接将数据加载到内存里来，那么后续在读取尤其是CommitLog、
ConsumeQueue的时候，其实有可能会频繁的从磁盘里加载数据到内存中去。

### 62 授人以渔：思考一个小问题，Java工程师真的只会Java就可以了吗？
> ,,,,  wo

### 63 抛砖引玉：通过本专栏的大白话讲解之后，再去深入阅读一些书籍和源码

### 64 授人以渔：一个学习方法的探讨，如何深入研究一个技术？
> 学习路径:1 官网，百度入门案例，环境部署与搭建，简单的使用层面。 2.项目实战，在哪些业务场景可以使用。3.阅读源码，多读几遍，每次读一遍的感悟不一样，毕竟不同时间自己的技术积累和心得都有提高


### 65 阶段性复习：一张思维导图带你梳理 RocketMQ 的底层实现原理
<img src="https://s1.ax1x.com/2020/07/23/UXVXDO.jpg" alt="UXVXDO.jpg" border="0" />

### 66 阶段性复习：在深度了解RocketMQ底层原理的基础之上，多一些主动思考

（1）Kafka、RabbitMQ他们有类似的数据分片机制吗？他们是如何把一个逻辑上的数据集合概念（比如一个Topic）给在物理上拆分
为多个数据分片的？然后拆分后的多个数据分片又是如何在物理的多台机器上分布式存储的？


（2）为什么一定要让MQ实现数据分片的机制？如果不实现数据分片机制，让你来设计MQ中一个数据集合的分布式存储，你觉得好设
计吗？

（3）同步刷盘和异步刷盘两种策略，分别适用于什么不同的场景呢？

（4）异步刷盘可以提供超高的写入吞吐量，但是有丢失数据的风险，这个适用于什么业务场景？在你所知道的业务场景，或者工作接
触过的业务场景中，有哪些场景需要超高的写入吞吐量，但是可以适度接受数据丢失？

（5）同步刷盘会大幅度降低写入吞吐量，但是可以让你的数据不丢失，你接触哪些场景，是严格要求数据务必不能丢失任何一条，但
是吞吐量并没有那么高的呢？

（6）Kafka、RabbitMQ他们的broker收到消息之后是如何写入磁盘的？采用的是同步刷盘还是异步刷盘的策略？为什么？

（7）每次写入都必须有超过半数的Follower Broker都写入消息才可以算做一次写入成功，那么大家思考一个问题，这样做是不是会对
Leader Broker的写入性能产生影响？是不是会降低TPS？是不是必须要在所有的场景都这么做？为什么呢？

（8）一般我们获取到一批消息之后，什么时候才可以认为是处理完这批消息了？是刚拿到这批消息就算处理完吗？还是说要对这批消
息执行完一大堆的数据库之类的操作，才算是处理完了？

（9）如果获取到了一批消息，还没处理完呢，结果机器就宕机了，此时会怎么样？这些消息会丢失，再也无法处理了吗？如果获取到
了一批消息，已经处理完了，还买来得及提交消费进度，此时机器宕机了，会怎么样呢？

（10）消费者机器到底是跟少数几台Broker建立连接，还是跟所有Broker都建立连接？这是不少朋友之前在评论区提出的问题，但是
我想这里大家肯定都有自己的答案了。

（11）RocketMQ是支持主从架构下的读写分离的，而且什么时候找Slave Broker读取大家也都了解的很清楚了，那么大家思考一下，
Kafka、RabbitMQ他们支持主从架构下的读写分离吗？支持Slave Broker的读取吗？为什么呢？

（12）如果支持读写分离的话，有没有一种可能，就是出现主从数据不一致的问题？比如有的数据刚刚到Master Broker和部分Slave
Broker，但是你刚好是从那个没有写入数据的Slave Broker去读取了？

（13）消费吞吐量似乎是跟你的处理速度有很大关系，如果你消费到一批数据，处理太慢了，会导致你严重跟不上数据写入的速度，这
会导致你后续几乎每次拉取数据都会从磁盘上读取，而不是os cache里读取，所以你觉得你在拉取到一批消息处理的时候，应该有哪些
要点需要注意的？

1.kafka也有类似的数据分片机制，通过逻辑数据集合，分为多个分区，每个分区指定一个consumequeue。多个数据分片，btokername相同的数据分片储存在对应的机器上。

2.如果不设计数据分片，消费消息并发比较低。

3.同步刷盘适合对数据不丢失要求比较高，订单金额相关，对于日志一些其他数据可以用异步刷盘。

8.需要返回ack.

9.重试消费

### 67 生产案例：从 RocketMQ 全链路分析一下为什么用户支付后没收到红包？
    
#### 客服反馈的一个奇怪问题：支付之后没有收到红包
> 消息丢失

#### 消息丢失的 三种情况

+ 订单系统推送消息到MQ的过程
    
     如网络状况
     
+ 消息到达MQ了，MQ自己会导致消息丢失
    
    如消息在 PageCache ,还没写到磁盘,服务器出了问题
    
    写到磁盘,磁盘坏了
    
+  红包系统拿到了消息，丢失
    
    拿到了消息，但是他把消息搞丢了，结果红包还没来得及发
    
### 68 发送消息零丢失方案：RocketMQ事务消息的实现流程分析

#### 解决消息丢失的第一个问题：订单系统推送消息丢失
在RocketMQ中，有一个非常强悍有力的功能，就是事务消息的功能，凭借这个事务级的消息机制，就可以让我们确保订单系统推送给
出去的消息一定会成功写入MQ里，绝对不会半路就搞丢了。

+ 发送half消息到MQ去，试探一下MQ是否正常
    
    消息的状态是half状态，这个时候红包系统是看不见这个half消息的
    
+  万一要是half消息写入失败了呢？
    
    可能你发现报错了，可能MQ就挂了，或者这个时候网络就是故障了，所以导致你的half消息都没发送成功，总之你现在肯定没法跟
    MQ通信了。
    
    这个时候你的订单系统就应该执行一系列的回滚操作，比如对订单状态做一个更新，让状态变成“关闭交易”，同时通知支付系统自动
    进行退款，这才是正确的做法。
 
 + half消息成功之后，订单系统完成自己的任务
 
    一旦half消息写成功了，就说明MQ肯定已经收到这条消息了，MQ还活着，而且目前你是可以跟MQ正常沟通的。
    
+ 如果订单系统的本地事务执行失败了怎么办？
    
    订单系统的数据库当时也有网络异常，或者数据库挂了，总而言之，就是你想把订单更新为“已完成”这个状态，是干不成了。
    
    这个时候其实也很简单，直接就是让订单系统发送一个rollback请求给MQ就可以了。这个意思就是说，你可以把之前我发给你的half
    消息给删除掉了，因为我自己这里都出问题了，已经无力跟你继续后续的流程了
    
+ 如果订单系统完成了本地事务之后，接着干什么？
    
    如果订单系统成功完成了本地的事务操作，比如把订单状态都更新为“已完成”了，此时你就可以发送一个commit请求给MQ，要求
    让MQ对之前的half消息进行commit操作，让红包系统可以看见这个订单支付成功消息 
 
 + 让流程严谨一些：如果发送half消息成功了，但是没收到响应呢？
    
    如果我们把half消息发送给MQ了，MQ给保存下来了，但是MQ返回给我们的响应我们没收到呢？
    
    其实RocketMQ这里有一个补偿流程，他会去扫描自己处于half状态的消息，如果我们一直没有对这个消息执行commit/rollback操
    作，超过了一定的时间，他就会回调你的订单系统的一个接口
    
    他会问问你：这个消息到底怎么回事？你到底是打算commit这个消息还是要rollback这个消息？
    
    去查一下数据库，看看这个订单当前的状态，如果发现订单状态是“已关闭”，此时就知道，你必然得发送rollback请求给MQ去删除之前那个half消息了！
    
+ 如果rollback或者commit发送失败了呢？
    
    如果订单系统是收到了half消息写入成功的响应了，同时尝试对自己的数据库更新了，然后根据失败或者成功去执行了rollback或者commit请求，发送给MQ了，结果因为网络故障，导致rollback或者commit请求发送失败了呢？
    
    本质这个MQ的回调就是一个补偿机制，如果你的half消息响应没收到，或者rollback、commit请求没发送成功，他都会来找你问问对
    half消息后续如何处理
    
    再假设一种场景，如果订单系统收到了half消息写入成功的响应了，同时尝试对自己的数据库更新了，然后根据失败或者成功去执行了
    rollback或者commit请求，发送给MQ了。很不巧，mq在这个时候挂掉了，导致rollback或者commit请求发送失败，怎么办？
    
    如果是这种情况的话，那就等mq自己重启了，重启之后他会扫描half消息，然后还是通过上面说到的补偿机制，去回调你的接口

### 69 RocketMQ黑科技解密：事务消息机制的底层实现原理

#### half 消息是如何对消费者不可见的？

之前的底层原理剖析的环节都知道，其实你写入一个Topic，最终是定位到这个Topic的某个MessageQueue，然后定位到
一台Broker机器上去，然后写入的是Broker上的CommitLog文件，同时将消费索引写入MessageQueue对应的ConsumeQueue文件

同时消息的offset会写入MessageQueue对应的ConsumeQueue，这个ConsumeQueue是属于OrderPaySuccuessTopic的，然后红
包系统按理说会从这个ConsumeQueue里获取到你写入的这个half消息。

但是实际上红包系统却没法看到这条消息，其本质原因就是RocketMQ一旦发现你发送的是一个half消息，他不会把这个half消息的
offset写入OrderPaySuccessTopic的ConsumeQueue里去。

他会把这条half消息写入到自己内部的“RMQ_SYS_TRANS_HALF_TOPIC”这个Topic对应的一个ConsumeQueue里去

RocketMQ是写入内部Topic的ConsumeQueue的，不是写入你指定的OrderPaySuccessTopic的ConsumeQueue的

所以你的红包系统自然无法从OrderPaySuccessTopic的ConsumeQueue中看到这条half消息了

#### 在什么情况下订单系统会收到half消息成功的响应？
必须要half消息进入到RocketMQ内部的RMQ_SYS_TRANS_HALF_TOPIC的ConsumeQueue文件了，此时就会认为half消息写入成功了，然后就会返回响应给订单系统。

所以这个时候，一旦你的订单系统收到这个half消息写入成功的响应，必然就知道这个half消息已经在RocketMQ内部了。

#### 假如因为各种问题，没有执行rollback或者commit会怎么样？
假如因为网络故障，订单系统没收到half消息的响应，或者说自己发送的rollback/commit请求失败了，那么RocketMQ
会干什么？

其实这个时候他会在后台有定时任务，定时任务会去扫描RMQ_SYS_TRANS_HALF_TOPIC中的half消息，如果你超过一定时间还是
half消息，他会回调订单系统的接口，让你判断这个half消息是要rollback还是commit

#### 如果执行rollback操作的话，如何标记消息回滚？

假设我们的订单系统执行了rollback请求，那么此时就需要对消息进行回滚。

因为RocketMQ都是顺序把消息写入磁盘文件的，所以在这里如果你执行rollback，他的本质就是用一个OP操作来标记half消息的状态

RocketMQ内部有一个OP_TOPIC，此时可以写一条rollback OP记录到这个Topic里，标记某个half消息是rollback了

假设你一直没有执行commit/rollback，RocketMQ会回调订单系统的接口去判断half消息的状态，但是他最多就
是回调15次，如果15次之后你都没法告知他half消息的状态，就自动把消息标记为rollback。

#### 如果执行commit操作，如何让消息对红包系统可见？

你执行commit操作之后，RocketMQ就会在OP_TOPIC里写入一条记录，标记half消息已经是commit状态了。

接着需要把放在RMQ_SYS_TRANS_HALF_TOPIC中的half消息给写入到OrderPaySuccessTopic的ConsumeQueue里去，然后我们的
红包系统可以就可以看到这条消息进行消费了

### 70 为什么解决发送消息零丢失方案，一定要使用事务消息方案？
<img src="https://s1.ax1x.com/2020/07/23/UXK2vR.jpg" alt="UXK2vR.jpg" border="0" />

我们真的有必要使用这么复杂的机制去确保消息到达MQ，而且绝对不会丢吗？

#### 一个小思考：能不能基于重试机制来确保消息到达MQ？

只要我们在代码中发送消息到MQ之后，同步等待MQ返回响应给我们，一直等待，如果半路中有网络异常或者MQ内部异常，我
们肯定会收到一个异常，比如网络错误，或者请求超时之类的。

如果我们在收到异常之后，就认为消息到MQ发送失败了，然后再次重试尝试发送消息到MQ，接着再次同步等待MQ返回响应给我
们，这样反复重试，是否可以确保消息一定会到达MQ？

确实如此，而且在基于Kafka作为消息中间件的消息零丢失方案中，对于发送消息这块，因为Kafka本身不具备RocketMQ这种事务消
息的高级功能，所以一般我们都是对Kafka会采用同步发消息 + 反复重试多次的方案，去保证消息成功投递到Kafka的。

但是如果是在类似我们目前这个较为复杂的订单业务场景中，仅仅采用同步发消息 + 反复重试多次的方案去确保消息绝对投递到MQ
中，似乎还是不够的

#### 先执行订单本地事务，还是先发消息到MQ？

如果我们先执行订单本地事务，接着再发送消息到MQ的话

有一个问题，假设你刚执行完成了订单本地事务了，结果还没等到你发送消息到MQ，结果你的订单系统突然崩溃了！

这就导致你的订单状态可能已经修改为了“已完成”，但是消息却没发送到MQ去！ 这就是这个方案最大的隐患

如果出现这种场景，那你的多次重试发送MQ之类的代码根本没机会执行！而且订单本地事务还已经执行成功了，你的消息没发送出
去，红包系统没机会派发红包，必然导致用户支付成功了，结果看不到自己的红包！

#### 把订单本地事务和重试发送MQ消息放到一个事务代码中

如果把订单本地事务代码和发送MQ消息的代码放到一个事务代码中呢？

````

    @transactional
    punlic  void  pay(){
        try{
            updateOrder()    ;
            mq.send(message);
        }catch(Excepton e){
            for(){
                //  重试
            }
            
            throw new XXXEXception();
        }
    }
````

方法上加入事务，在这个事务方法中，我们哪怕执行了orderService.finishOrderPay()，但是其实也仅仅执行了一些增删改SQL语句，还没提交订单本地事务。

如果发送MQ消息失败了，而且多次重试还不奏效，则我们抛出异常会自动回滚订单本地事务；

如果你刚执行了orderService.finishOrderPay()，结果订单系统直接崩溃了，此时订单本地事务会回滚，因为根本没提交过。

但是对于这个方案，还是非常的不理想，原因就出在那个MQ多次重试的地方

假设用户支付成功了，然后支付系统回调通知你的订单系统说，有一笔订单已经支付成功了，这个时候你的订单系统卡在多次重试MQ
的代码那里，可能耗时了好几秒种，此时回调通知你的系统早就等不及可能都超时异常了。

而且你把重试MQ的代码放在这个逻辑里，可能会导致订单系统的这个接口性能很差


#### 你就一定可以依靠本地事务回滚吗？
方法上加了事务注解，但是代码里还有更新Redis缓存和Elasticsearch数据的代码逻辑，如果你要是已经完成了订单数据库更新、Redis缓存更新、ES数据更新了，结果没法送MQ呢订单系统崩溃了。

虽然订单数据库的操作会回滚，但是Redis、Elasticsearch中的数据更新会自动回滚吗？

不会的，因为他们根本没法自动回滚，此时数据还是会不一致的。所以说，完全寄希望于本地事务自动回滚是不现实的。

#### 保证业务系统一致性的最佳方案：基于RocketMQ的事务消息机制

所以真正要保证消息一定投递到MQ，同时保证业务系统之间的数据完全一致，业内最佳的方案还是用基于RocketMQ的事务消息机制。


### 71 用支付后发红包的案例场景，分析RocketMQ事物消息的代码实现细节


#### 发送half事务消息出去
<a  href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#6-%E6%B6%88%E6%81%AF%E4%BA%8B%E5%8A%A1%E6%A0%B7%E4%BE%8B"> Rocket 事物消息  官方demo  </a>


#### 假如half消息发送失败，或者没收到half消息响应怎么办？
在执行“producer.sendMessageInTransaction(msg, null)”的时候，收到一个异常，发现消息发送失败了。

我们可以把发送出去的half消息放在内存里，或者写入本地磁盘文件，后台开启一个线程去检查，如果一个half消息超
过比如10分钟都没有收到响应，那就自动触发回滚逻辑。

#### 如果half消息成功了，如何执行订单本地事务？
<img src="https://s1.ax1x.com/2020/07/24/UX8oIP.jpg" alt="UX8oIP.jpg" border="0" />

````
public class TransactionListenerImpl implements TransactionListener {
  private AtomicInteger transactionIndex = new AtomicInteger(0);
  private ConcurrentHashMap<String, Integer> localTrans = new ConcurrentHashMap<>();
  
  
  @Override
  public LocalTransactionState executeLocalTransaction(Message msg, Object arg) {
      int value = transactionIndex.getAndIncrement();
      int status = value % 3;
      localTrans.put(msg.getTransactionId(), status);
      return LocalTransactionState.UNKNOW;
  }
  @Override
  public LocalTransactionState checkLocalTransaction(MessageExt msg) {
      Integer status = localTrans.get(msg.getTransactionId());
      if (null != status) {
          switch (status) {
              case 0:
                  return LocalTransactionState.UNKNOW;
              case 1:
                  return LocalTransactionState.COMMIT_MESSAGE;
              case 2:
                  return LocalTransactionState.ROLLBACK_MESSAGE;
          }
      }
      return LocalTransactionState.COMMIT_MESSAGE;
  }
}
````

### 72 Broker消息零丢失方案：同步刷盘 + Raft协议主从同步

#### 用了事务消息机制，消息就一定不会丢了吗？

你的这条消息在commit之后，会从half topic里进入OrderPaySuccessTopic中，但是此时仅仅是消息进入了
这个你预定的Topic而已，仅仅是可以被红包系统看到而已，此时可能你的红包系统还没来得及去获取这条消息。

然后恰巧在此时，你的这条消息又仅仅停留在os cache中，还没进入到ConsumeQueue磁盘文件里去，然后此时这台机器突然宕机
了，os cache中的数据全部丢失，此时必然会导致你的消息丢失，红包系统再没机会读到这条消息了。

#### 就算你走运，消息进了磁盘就不会丢了吗？

即使消息已经进入磁盘文件了，但是这个时候红包系统还没来得及消费这条消息，然后此时这台机器的磁盘突然就坏了，就会一样导致
消息丢失，而且可能消息再也找不回来了，同样会丢失数据。

#### 明确一个前提：保证消息写入MQ不代表不丢失
我们无论是通过比较简单的同步发送消息 + 反复多次重试的方案，还是事务消息的方案，哪
怕我们确保消息已经写入MQ成功了，此时也未必消息就不会丢失了。

因为即使你写入MQ成功了，这条消息也大概率是仅仅停留在MQ机器的os cache中，一旦机器宕机内存里的数据都会丢失，或者哪怕
消息已经进入了MQ机器的磁盘文件里，但是磁盘一旦坏了，消息也会丢失。


#### 异步刷盘 vs 同步刷盘
将异步刷盘调整为同步刷盘。

异步刷盘的模式下，我们的写入消息的吞吐量肯定是极高的，毕竟消息只要进入os cache这个内存就可以了，写消息的性能就是
写内存的性能，那每秒钟可以写入的消息数量肯定更多了，但是这个情况下，可能就会导致数据的丢失。

所以如果一定要确保数据零丢失的话，可以调整MQ的刷盘策略，我们需要调整broker的配置文件，将其中的flushDiskType配置设置
为：SYNC_FLUSH，默认他的值是ASYNC_FLUSH，即默认是异步刷盘的。

#### 如何通过主从架构模式避免磁盘故障导致的数据丢失？

对Broker使用主从架构的模式

必须让一个Master Broker有一个Slave Broker去同步他的数据，而且你一条消息写入成功，必须是让Slave Broker也写入
成功，保证数据有多个副本的冗余

这样一来，你一条消息但凡写入成功了，此时主从两个Broker上都有这条数据了，此时如果你的Master Broker的磁盘坏了，但是
Slave Broker上至少还是有数据的，数据是不会因为磁盘故障而丢失的。

对于主从同步的架构，我们本来就是讲解了基于DLedger技术和Raft协议的主从同步架构，你如果采用了这套架构，对于你所有的消息
写入，只要他写入成功，那就一定会通过Raft协议同步给其他的Broker机器，这里的原理我们之前都已经讲解过了，大家有遗忘的回去
看看即可。

#### MQ确保数据零丢失的方案总结

这样一来，你一条消息但凡写入成功了，此时主从两个Broker上都有这条数据了，此时如果你的Master Broker的磁盘坏了，但是
Slave Broker上至少还是有数据的，数据是不会因为磁盘故障而丢失的。

对于主从同步的架构，我们本来就是讲解了基于DLedger技术和Raft协议的主从同步架构，你如果采用了这套架构，对于你所有的消息
写入，只要他写入成功，那就一定会通过Raft协议同步给其他的Broker机器，这里的原理我们之前都已经讲解过了，大家有遗忘的回去
看看即可。


### 73 Consumer消息零丢失方案：手动提交offset + 自动故障转移

#### 红包系统拿到了消息就一定会派发红包吗？

未必

我们之前也给大家分析过这个问题，如果红包系统已经拿到了这条消息，但是消息目前还在他的内存里，还没执行派发红包的逻辑，此
时他就直接提交了这条消息的offset到broker去说自己已经处理过了

接着红包系统在上图这个状态的时候就直接崩溃了，内存里的消息就没了，红包也没派发出去，结果Broker已经收到他提交的消息
offset了，还以为他已经处理完这条消息了。

等红包系统重启的时候，就不会再次消费这条消息了

#### Kafka消费者的数据丢失问题

说Kafka消费者可能会出现上面说的，拿到一批消息，还没来得及处理呢，结果就提交offset到broker去了，完了消费者系统就
挂掉了，这批消息就再也没机会处理了，因为他重启之后已经不会再次获取提交过offset的消息了。

#### RocketMQ消费者的与众不同的地方

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#13-%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF"> 官方 消费者demo</a>

RocketMQ的消费者中会注册一个监听器，就是上面小块代码中的MessageListenerConcurrently这个东西，当你的消费
者获取到一批消息之后，就会回调你的这个监听器函数，让你来处理这一批消息。

然后当你处理完毕之后，你才会返ConsumeConcurrentlyStatus.CONSUME_SUCCESS作为消费成功的示意，告诉RocketMQ，这批
消息我已经处理完毕了。

所以对于RocketMQ而言，其实只要你的红包系统是在这个监听器的函数中先处理一批消息，基于这批消息都派发完了红包，然后返回
了那个消费成功的状态，接着才会去提交这批消息的offset到broker去。

所以在这个情况下，如果你对一批消息都处理完毕了，然后再提交消息的offset给broker，接着红包系统崩溃了，此时是不会丢失消息
的

#### 需要警惕的地方：不能异步消费消息
````
    consumer.registerMessageListener(new MessageListenerConcurrently() {
                @Override
                public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext context) {
                   
                   new Thread({
                        public void run(){
                            // TODO
                        }
                   }).start();
                    // 标记该消息已经被成功消费
                    return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
                }
            });

````

如果要是用这种方式来处理消息的话，那可能就会出现你开启的子线程还没处理完消息呢，你已经返回
ConsumeConcurrentlyStatus.CONSUME_SUCCESS状态了，就可能提交这批消息的offset给broker了，认为已经处理结束了。

然后此时你红包系统突然宕机，必然会导致你的消息丢失了！

因此在RocketMQ的场景下，我们如果要保证消费数据的时候别丢消息，你就老老实实的在回调函数里处理消息，处理完了你再返回
ConsumeConcurrentlyStatus.CONSUME_SUCCESS状态表明你处理完毕了！


### 74 基于 RocketMQ 设计的全链路消息零丢失方案总结

#### 对全链路消息零丢失方案进行总结

1. 发送消息到MQ的零丢失：
   
    + 方案一（同步发送消息 + 反复多次重试）
    
    + 方案二（事务消息机制），两者都有保证消息发送零丢失的效果，但是经过分析，事务消息方案整体会更好一些
2. MQ收到消息之后的零丢失：开启同步刷盘策略 + 主从架构同步机制，只要让一个Broker收到消息之后同步写入磁盘，同时同步复制
    
    给其他Broker，然后再返回响应给生产者说写入成功，此时就可以保证MQ自己不会弄丢消息
3. 消费消息的零丢失：采用RocketMQ的消费者天然就可以保证你处理完消息之后，才会提交消息的offset到broker去，只要记住别采用
多线程异步处理消息的方式即可


#### 消息零丢失方案的优势与劣势

如果在系统中落地一套消息零丢失方案，不管是哪个系统，不管是哪个场景，都可以确保消息流转的过程中不会丢失，看起来似乎很有
吸引力，这也是消息零丢失方案的优势所在，可以让系统的数据都是正确的，不会有丢失的。

但整个从头到尾的消息流转链路的性能大幅度下降，让你的MQ的吞吐量大幅度的下降


#### 为什么消息零丢失方案会导致吞吐量大幅度下降？

1. 生产者

    涉及到half消息、commit or rollback、写入内部topic、回调机制，等诸多复杂的环节
    
    不说别的，光是你成功发送一条消息，都至少要half + commit两次请求。
    
    所以当你一旦上了如此复杂的方案之后，势必会导致你的发送消息的性能大幅度下降，同时发送消息到MQ的吞吐量大幅度下降。

2. Borker 储存 

    MQ的一台broker机器收到了消息之后，必然直接把消息刷入磁盘里，这个性能就远远低于你写入os cache了，完全不是一个数量
    级的，比如你写入os cache相当于是内存，可能仅仅需要0.1ms，但是你写入磁盘文件可能就需要10ms！
    
    
    接着你的这台broker机器还必须直接把消息复制给其他的broker，完成多副本的冗余，这个过程涉及到两台broker机器之间的网络通
    信，另外一台broker机器写数据到自己本地磁盘去，同样会比较慢

3. 消费者
    看你的消费者，当你的消费者拿到消息之后，比如他直接开启一个子线程去处理这批消息，然后他就直接返回
    CONSUME_SUCCESS状态了，接着他就可以去处理下一批消息了！如果这样的话，你消费消息的速度会很快，吞吐量会很高！
    
    但是如果为了保证数据不丢失，你必须是处理完一批消息再返回CONSUME_SUCCESS状态，那么此时你消费者处理消息的速度会降
    低，吞吐量 自然也会下降了！


#### 消息零丢失方案到底适合什么场景？

一般我们建议，对于跟金钱、交易以及核心数据相关的系统和核心链路，可以上这套消息零丢失方案。

比如支付系统，他是绝对不能丢失任何一条消息的，你的性能可以低一些，但是不能有任何一笔支付记录丢失。

比如订单系统，公司一般是不能轻易丢失一个订单的，毕竟一个订单就对应一笔交易，如果订单丢失，用户还支付成功了，你轻则要给

用户赔付损失，重则弄不好要经受官司，特别是一些B2B领域的电商，一笔线上交易可能多大几万几十万。

所以对这种非常非常核心的场景和少数几条核心链路，才会建议大家上这套复杂的消息0丢失方案。


#### 简化

比如你可以把事务消息方案退化成“同步发送消息 + 反复重试几次”的方案，如果发送消息失败，就重试几次，但是大部分时候可能
不需要重试，那么也不会轻易的丢失消息的！最多在这个方案里，可能会出现一些数据不一致的问题。

或者你把broker的刷盘策略改为异步刷盘，但是上一套主从架构，即使一台机器挂了，os cache里的数据丢失了，但是其他机器上还有
数据。但是大部分时候broker不会随便宕机，那么异步刷盘策略下性能还是很高的。

所以说，对于非核心的链路，非金钱交易的链路，大家可以适当简化这套方案，用一些方法避免数据轻易丢失，但是同时性能整体很
高，即使有极个别的数据丢失，对非核心的场景，也不会有太大的影响。

### 75 生产案例：从 RocketMQ 底层原理分析为什么会重复发优惠券？

#### 客服反馈的奇怪问题：有用户重复收到了多个优惠券

#### 问题的定位：优惠券系统重复消费了一条消息

#### 订单系统发送消息到MQ的时候会重复吗？
假设用户在支付成功之后，我们的订单系统收到了一个支付成功的通知，接着他就向MQ发送了一条订单支付成功的消息，这个
大家都知道没有什么问题。

但是偏偏可能因为不知道什么原因，你的订单系统处理的速度有点慢

然后可能就因为你的订单系统处理的速度有点慢了，这就导致支付系统跟你订单系统之间的请求出现了超时，此时有可能支付系统再次
重试调用了你订单系统的接口去通知你，这个订单支付成功了，然后你的订单系统这个时候可能又一次推送了一条消息到MQ里去，相
当于是一个订单支付成功的消息，你重复推送了两次到MQ！

#### 重试是一把双刃剑：订单系统自己重复发送消息

这种重试的方式，其实是一把双刃剑，因为正是这个重试就可能导致消息重复发送

此时MQ里可能已经有你发送过去的消息了，只不过他返回给你的响应没能给到你而已！

#### 优惠券系统重复消费一条消息

假设你的优惠券系统拿到了一条订单成功支付的消息，然后都已经进行处理了，也就是说都已经对这个订单给你发了一张优惠券了，本
来我们之前讲过，这个时候他应该返回一个CONSUME_SUCCESS的状态，然后提交消费进度offset到broker的。

但是不巧的是，你刚刚发完优惠券，还没来得及提交消息offset到broker呢！优惠券系统就进行了一次重启！比如可能优惠券系统的代
码更新了，需要重启进行重新部署。

因为你没提交这条消息的offset给broker，broker并不知道你已经处理完了这条消息，然后优惠券系统重启之后，broker就会再次
把这条消息交给你，让你再一次进行处理，然后你会再一次发送一张优惠券，导致重复发送了两次优惠券！

#### 消息重复问题应该是一种家常便饭

对类似优惠券系统这样的业务系统，我们肯定是会频繁的更新代码的，可能每隔几天就需要重启一次系统进行代码
的更新

所以其实你重启优惠券系统的时候，可能有一批消息刚处理完，还没来得及提交offset给broker呢，然后你重启之后就会再一次重复处
理这批消息，这种情况可能是家常便饭！

另外就是对于系统之间的调用，有的时候出现超时和重试的情况也是很常见的，所以你负责发消息到MQ的系统，很可能时不时的出现
一次超时，然后被别人重试调用你的接口，你可能会重复发送一条消息到MQ里去，这可能也是家常便饭！


### 76 对订单系统核心流程引入 幂等性机制，保证数据不会重复

#### 到底什么是幂等性机制？
幂等性机制，其实就是用来避免对同一个请求或者同一条消息进行重复处理的机制，所谓的幂等，他的意思就是，比如你有一个接
口，然后如果别人对一次请求重试了多次，来调用你的接口，你必须保证自己系统的数据是正常的，不能多出来一些重复的数据，这就
是幂等性的意思。

#### 发送消息到MQ的时候如何保证幂等性？

常见的方案有两种。

+ 第一个方案就是业务判断法，也就是说你的订单系统必须要知道自己到底是否发送过消息到MQ去，消息到底是否已经在MQ里了。

    业务判断法的核心就在于，你的消息肯定是存在于MQ里的，到底发没发送过，只有MQ知道。如果没发送过这个消息，MQ里肯
    定没有这个消息，如果发送过这个消息，MQ里肯定给有这个消息。
    
    所以当你的订单系统的接口被重试调用的时候，你这个接口上来就应该发送请求到MQ里去查询一下，比如对订单id=1100这个订单的
    支付成功消息，在你MQ那里有没有？如果有的话，我就不再重复发送消息了！
#### 基于Redis缓存的幂等性机制
讲第二种方法，就是状态判断法
    
这个方法的核心在于，你需要引入一个Redis缓存来存储你是否发送过消息的状态，如果你成功发送了一个消息到MQ里去，你得在
Redis缓存里写一条数据，标记这个消息已经发送过

其实两种幂等性机制都是很常用的，但是大家这里一定要知道一个事情，那就是对于基于Redis的状态判断法，有可能没办法完全做到
幂等性

举个例子，你的支付系统发送请求给订单系统，然后已经发送消息到MQ去了，但是此时订单系统突然崩溃了，没来得及把消息发送的
状态写入Redis

这个时候如果你的订单系统在其他机器上部署了，或者他重启了，那么这个时候订单系统被重试调用的时候，他去找Redis查询消息发
送状态，会以为消息没发送过，然后会再次发送重复消息到MQ去

所以这种方案一般情况下是可以做到幂等性的，但是如果有时候你刚发送了消息到MQ，还没来得及写Redis，系统就挂了，之后你的
接口被重试调用的时候，你查Redis还以为消息没发过，就会发送重复的消息到MQ去。


#### 有没有必要在订单系统环节保证消息不重复发送？
如果在订单系统环节要保证消息不重复发送，要不然就是直接通过查询MQ来判断消息是否发过，要不然就是
通过引入Redis来保存消息发送状态。其实这两种方案都不是太好。

从MQ查询消息是没这个必要的，他的性能也不是太好，会影响你的接口的性能。

另外基于Redis的消息发送状态的方案，在极端情况下还是没法100%保证幂等性，所以也不是特别好的一个方案。


#### 优惠券系统如何保证消息处理的幂等性？

这里就比较简单了，直接基于业务判断法就可以了，因为优惠券系统每次拿到一条消息后给用户发一张优惠券，实际上核心就是在
数据库里给用户插入一条优惠券记录

#### MQ消息幂等性的方案总结
 一般来说，对于MQ的重复消息问题而言，我们往MQ里重复发送一样的消息其实是还可以接收的，因为MQ里有多条重复消息，他不
 会对系统的核心数据直接造成影响，但是我们关键要保证的，是你从MQ里获取消息进行处理的时候，必须要保证消息不能重复处理。
 
 这里的话，要保证消息的幂等性，我们优先推荐的其实还是业务判断法，直接根据你的数据存储中的记录来判断这个消息是否处理过，
 如果处理过了，那就别再次处理了。因为我们要知道，基于Redis的消息发送状态的方案，在一些极端情况下还是没法完全保证幂等性
 的
 
### 77 如果优惠券系统的数据库宕机，如何用死信队列解决这种异常场景？

#### 如果优惠券系统的数据库宕机，会怎么样？
优惠券系统的数据库宕机了，就必然会导致我们从MQ里获取到消息之后是没办法进行处理的

#### 数据库宕机的时候，你还可以返回CONSUME_SUCCESS吗？
如果拿到一批消息,宕机了,如果你返回的话，下一次就会处理下一批消息，但是这批消息其实没处理成功，此时必然导致这批消息就丢失了。
 肯定会导致有一批用户没法收到优惠券的！
 
#### 如果对消息的处理有异常，可以返回RECONSUME_LATER状态

以实际上如果我们因为数据库宕机等问题，对这批消息的处理是异常的，此时没法处理这批消息，我们就应该返回一个
RECONSUME_LATER状态

他的意思是，我现在没法完成这批消息的处理，麻烦你稍后过段时间再次给我这批消息让我重新试一下！

如果消息处理失败了，就在 catch 里面返回RECONSUME_LATER状态，让RocketMQ
稍后再重新把这批消息给我，让我重试对这批消息进行处理！


#### RocketMQ是如何让你进行消费重试的？

RocketMQ会有一个针对你这个ConsumerGroup的重试队列。如果遗忘了ConsumerGroup消费组概念的朋友可以再回过
头去复习一下。

如果你返回了RECONSUME_LATER状态，他会把你这批消息放到你这个消费组的重试队列中去

比如你的消费组的名称是“VoucherConsumerGroup”，意思是优惠券系统的消费组，那么他会有一个
“%RETRY%VoucherConsumerGroup”这个名字的重试队列

然后过一段时间之后，重试队列中的消息会再次给我们，让我们进行处理。如果再次失败，又返回了RECONSUME_LATER，那么会再
过一段时间让我们来进行处理，默认最多是重试16次！每次重试之间的间隔时间是不一样的，这个间隔时间可以如下进行配置：
>messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h

上面这段配置的意思是，第一次重试是1秒后，第二次重试是5秒后，第三次重试是10秒后，第四次重试是30秒后，第五次重试是1分钟
后，以此类推，最多重试16次！

#### 如果连续重试16次还是无法处理消息，然后怎么办？

如果在16次重试范围内消息处理成功了，自然就没问题了，但是如果你对一批消息重试了16次还是无法成功处理呢？

这个时候就需要另外一个队列了，叫做死信队列，所谓的死信队列，顾名思义，就是死掉的消息就放这个队列里。

死信队列的名字是“%DLQ%VoucherConsumerGroup”，我们其实在RocketMQ的管理后台上都是可以看到的

其实这个就看你的使用场景了，比如我们可以专门开一个后台线程，就是订阅“%DLQ%VoucherConsumerGroup”这个死信队列，
对死信队列中的消息，还是一直不停的重试。

#### 消息处理失败场景下的方案总结

另外一个生产环境下的问题，就是消费者底层的一些依赖可能有故障了，比如数据库宕机，缓存宕机之类的，此
时你就没办法完成消息的处理了，那么可以通过一些返回状态去让消息进入RocketMQ自带的重试队列，同时如果反复重试还是不行，
可以让消息进入RocketMQ自带的死信队列，后续针对死信队列中的消息进行单独的处理就可以了。

### 78 生产案例：为什么基于 RocketMQ 进行订单库数据同步时会消息乱序？

#### 订单数据库的binlog消息乱序

简单来说，比如订单系统在更新订单数据库的时候，有两条SQL语句：
````
insert into order values(xx, 0)
update order set xxvalue=100 where id=xxx
````
就是先插入了一条订单数据，刚开始他一个字段的值是0，接着更新他的一个字段的值是100。

然后这两条SQL语句是对应着两个binlog的，也就是两个更新日志，一个binlog是insert语句的，一个binlog是update语句的，这个
binlog会进入到MQ中去。

然后大数据系统从MQ获取出来binlog的时候，居然是先获取出来了update语句的binlog，然后再获取了insert语句的binlog

也就是说，这个时候会先执行更新操作，但是此时数据根本不存在，没法进行更新，接着执行插入操作，也就是插入一条字段值为0的
订单数据进去，最后大数据存储中的订单记录的字段值就是0。

#### 为什么基于MQ来传输数据会出现消息乱序？

非常简单，我们之前都学习过，可以给每个Topic指定多个MessageQueue，然后你写入消息的时候，其实是会把消息均匀分发给
不同的MessageQueue的。

比如我们这里在写入binlog到MQ的时候，可能会把insert binlog写入到一个MessageQueue里去，update binlog写入到另外一个
MessageQueue里去

接着大数据系统在获取binlog的时候，可能会部署多台机器组成一个Consumer Group，对于Consumer Group中的每台机器都会负
责消费一部分MessageQueue的消息，所以可能一台机器从上图的ConsumeQueue01中获取到了insert binlog，一台机器从上图的
ConsumeQueue02中获取到了update binlog


#### 消息乱序：必须要正视的一个问题
在使用MQ的时候出现消息乱序是非常正常的一个问题，因为我们原本有顺序的消
息，完全可能会分发到不同的MessageQueue中去，然后不同机器上部署的Consumer可能会用混乱的顺序从不同的MessageQueue
里获取消息然后处理。

### 79 在RocketMQ中，如何解决订单数据库同步的消息乱序问题？

#### 让属于同一个订单的binlog进入一个MessageQueue

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#21-%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF%E7%94%9F%E4%BA%A7"> 官方 顺序消息demo</a>

要解决这个消息的乱序问题，最根本的方法其实非常简单，就是得想办法让一个订单的binlog进入到一个MessageQueue里去。

我们这里可以采用取模的方法，比如有一个订单id是1100，那么他可能有2个binlog，对这两个binlog，我们必须要用订单id=1100对
MessageQueue的数量进行取模，比如MessageQueue一共有15个，那么此时订单id=1100对15取模，就是5
也就是说，凡是订单id=1100的binlog，都应该进入位置为5的MessageQueue中去！

#### 真的这么简单吗？获取binlog的时候也得有序！

+ 首先，我们的MySQL数据库的binlog一定都是有顺序的。

+ 接着我们将binlog发送给MQ的时候，必须将一个订单的binlog都发送到一个MessageQueue里去，而且发送过去的时候，也必须是
  严格按照顺序来发送的
  
只有这样，最终才能让一个订单的binlog进入同一个MessageQueue，而且还是有序的

#### Consumer有序处理一个订单的binlog

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#22-%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9%E6%B6%88%E6%81%AF" >官方消费 顺序消息 demo </a>

一个Consumer可以处理多个MessageQueue的消息，但是一个MessageQueue只能交给一个Consumer来进
行处理，所以一个订单的binlog只会有序的交给一个Consumer来进行处理！

#### 这就完了吗？没有，万一消息处理失败了可以走重试队列吗？

对于有序消息的方案中，如果你遇到消息处理失败的场景，就必须返回SUSPEND_CURRENT_QUEUE_A_MOMENT这个状态，意
思是先等一会儿，一会儿再继续处理这批消息，而不能把这批消息放入重试队列去，然后直接处理下一批消息。

重试队列还是会乱序

#### 有序消息方案与其他消息方案的结合

如果你一定要求消息是有序的，那么必须得用上述的有序消息方案，同时对这个方案，如果你要确保消息不丢失，那么可以和消息零丢
失方案结合起来，如果你要避免消息重复处理，还需要在消费者那里处理消息的时候，去看一下，消息如果已经存在就不能重复插入，
等等。

同时还需要设计自己的消息处理失败的方案，也就是不能进入重试队列，而是暂停等待一会儿，继续处理这批消息。

### 80 基于订单数据库同步场景，来分析RocketMQ的顺序消息机制的代码实现

#### 如何让一个订单的binlog进入一个MessageQueue？

````
    SendResult sendResult = producer.send(msg, new MessageQueueSelector() {
                   @Override
                   public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
                       Long id = (Long) arg;  //根据订单id选择发送queue
                       long index = id % mqs.size();
                       return mqs.get((int) index);
                   }
               }, orderList.get(i).getOrderId());//订单id
````
关键因素就是两个，一个是发送消息的时候传入一个MessageQueueSelector，在里面你要根据
订单id和MessageQueue数量去选择这个订单id的数据进入哪个MessageQueue。

同时在发送消息的时候除了带上消息自己以外，还要带上订单id，然后MessageQueueSelector就会根据订单id去选择一个
MessageQueue发送过去，这样的话，就可以保证一个订单的多个binlog都会进入一个MessageQueue中去。

#### 消费者如何保证按照顺序来获取一个MessageQueue中的消息？

````
     consumer.registerMessageListener(new MessageListenerOrderly() {
     
                Random random = new Random();
                @Override
                public ConsumeOrderlyStatus consumeMessage(List<MessageExt> msgs, ConsumeOrderlyContext context) {
                    context.setAutoCommit(true);
                    for (MessageExt msg : msgs) {
                        // 可以看到每个queue有唯一的consume线程来消费, 订单对每个queue(分区)有序
                        System.out.println("consumeThread=" + Thread.currentThread().getName() + "queueId=" + msg.getQueueId() + ", content:" + new String(msg.getBody()));
                    }
     
                    try {
                        //模拟业务逻辑处理中...
                        TimeUnit.SECONDS.sleep(random.nextInt(10));
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                    return ConsumeOrderlyStatus.SUCCESS;
                }
            });
````
使用的是MessageListenerOrderly这个东西，他里面有Orderly这个名称

也就是说，Consumer会对每一个ConsumeQueue，都仅仅用一个线程来处理其中的消息。

比如对ConsumeQueue01中的订单id=1100的多个binlog，会交给一个线程来按照binlog顺序来依次处理。否则如果
ConsumeQueue01中的订单id=1100的多个binlog交给Consumer中的多个线程来处理的话，那还是会有消息乱序的问题。

###  81 如何基于RocketMQ的数据过滤机制，提升订单数据库同步的处理效率

#### 混杂在一起的订单数据库的binlog

一个数据库中可能会包含很多表的数据，比如订单数据库，他里面除了订单信息表以外，可能还包含很多其他的表。

所以我们在进行数据库binlog同步的时候，很可能是把一个数据库里所有表的binlog都推送到MQ里去的！

所以在MQ的某个Topic中，可能是混杂了订单数据库里几个甚至十几个表的binlog数据的，不一定仅仅包含我们想要的表的binlog数
据！

#### 处理不关注的表的binlog
如果不是表A的binlog，那么就直接丢弃不要处理；如果是表A的binlog，才会去进行处理！

但是这样的话，必然会导致大数据系统处理很多不关注的表的binlog，也会很浪费时间，降低消息的效率

#### 在发送消息的时候，给消息设置tag和属性

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#1%E7%94%9F%E4%BA%A7%E8%80%85%E6%A0%B7%E4%BE%8B"> 官方 发消息demo tag</a>

我们发送消息的时候，其实是可以给消息设置tag、属性等多个附加的信息的。

````
Message msg = new Message("TopicTest",
   tag,
   ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET)
);
// 设置一些属性
msg.putUserProperty("a", String.valueOf(i));
SendResult sendResult = producer.send(msg);

````

#### 在消费数据的时候根据tag和属性进行过滤

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#2%E6%B6%88%E8%B4%B9%E8%80%85%E6%A0%B7%E4%BE%8B"> 官方 消费者,过滤demo</a>

````
     consumer.subscribe("TopicTest", "TagA || TagC || TagD");
````
过滤 tag

````
     // 只有订阅的消息有这个属性a, a >=0 and a <= 3
     consumer.subscribe("TopicTest", MessageSelector.bySql("a between 0 and 3");
````
过滤 赋予的值
````
RocketMQ还是支持比较丰富的数据过滤语法的，如下所示：
（1）数值比较，比如：>，>=，<，<=，BETWEEN，=；
（2）字符比较，比如：=，<>，IN；
（3）IS NULL 或者 IS NOT NULL；
（4）逻辑符号 AND，OR，NOT；
（5）数值，比如：123，3.1415；
（6）字符，比如：'abc'，必须用单引号包裹起来；
（7）NULL，特殊的常量
（8）布尔值，TRUE 或 FALSE
````

#### 基于数据过滤减轻Consumer负担
如果MQ里混杂了大量的数据，可能Consumer仅仅对其中一部分数据感
兴趣，此时可以在Consumer端使用tag等数据过滤语法，过滤出自己感兴趣的数据来消费。

### 82 生产案例：基于延迟消息机制优化大量订单的定时退款扫描问题！
订单系统的后台线程必须要不停的扫描各种未支付的订单，这种实现方式实际上并不是很好。

一个原因是未支付状态的订单可能是比较多的，然后你需要不停的扫描他们，可能每个未支付状态的订单要被扫描N多遍，才会发现他
已经超过30分钟没支付了。

另外一个是很难去分布式并行扫描你的订单。因为假设你的订单数据量特别的多，然后你要是打算用多台机器部署订单扫描服务，但是
每台机器扫描哪些订单？怎么扫描？什么时候扫描？这都是一系列的麻烦问题。

因此针对类似这种场景，MQ里的延迟消息往往就会出场了，他是特别适合在这种场景里使用的，而且在实际项目中，MQ的延迟消息
使用的往往是很多的。

所谓延迟消息，意思就是说，我们订单系统在创建了一个订单之后，可以发送一条消息到MQ里去，我们指定这条消息是延迟消息，比
如要等待30分钟之后，才能被订单扫描服务给消费到

这种方式就比你用后台线程扫描订单的方式要好的多了，一个是对每个订单你只会在他创建30分钟后查询他一次而已，不会反复扫描订
单多次。

另外就是如果你的订单数量很多，你完全可以让订单扫描服务多部署几台机器，然后对于MQ中的Topic可以多指定一个
MessageQueue，这样每个订单扫描服务的机器作为一个Consumer都会处理一部分订单的查询任务。

### 83 基于订单定时退款场景，来分析RocketMQ的延迟消息的代码实现

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#3-%E5%BB%B6%E6%97%B6%E6%B6%88%E6%81%AF%E6%A0%B7%E4%BE%8B">官方延时消息 demo</a>

````
     // 实例化一个生产者来产生延时消息
          DefaultMQProducer producer = new DefaultMQProducer("ExampleProducerGroup");
          // 启动生产者
          producer.start();
          int totalMessagesToSend = 100;
          for (int i = 0; i < totalMessagesToSend; i++) {
              Message message = new Message("TestTopic", ("Hello scheduled message " + i).getBytes());
              // 设置延时等级3,这个消息将在10s之后发送(现在只支持固定的几个时间,详看delayTimeLevel)
              message.setDelayTimeLevel(3);
              // 发送消息
              producer.send(message);
          }
````
现在RocketMq并不支持任意时间的延时，需要设置几个固定的延时等级，从1s到2h分别对应着等级1到18 消息消费失败会进入延时消息队列，消息发送时间与设置的延时等级和重试次数有关，详见代码SendMessageProcessor.java

### 84 在RocketMQ的生产实践中积累的各种一手经验总结

#### 灵活的运用 tags来过滤数据
基于tags来过滤数据的功能，其实在真正的生产项目中，建议大家合理的规划Topic和里面的tags，一个Topic代表了一
类业务消息数据，然后对于这类业务消息数据，如果你希望继续划分一些类别的话，可以在发送消息的时候设置tags。

#### 基于消息key来定位消息是否丢失
怎么从MQ里查消息是否丢失呢？

可以基于消息key来实现，比如通过下面的方式设置一个消息的key为订单id：message.setKeys(orderId)，这样这个消息就具备一个
key了。

接着这个消息到broker上，会基于key构建hash索引，这个hash索引就存放在IndexFile索引文件里。

然后后续我们可以通过MQ提供的命令去根据key查询这个消息，类似下面这样：
>mqadmin queryMsgByKey -n 127.0.0.1:9876 -t SCANRECORD -k orderId

#### 消息零丢失方案的补充
一般假设MQ集群彻底崩溃了，你生产者就应该把消息写入到本地磁盘文件里去进行持久化，或者是写入数据库里去暂存起来，等待
MQ恢复之后，然后再把持久化的消息继续投递到MQ里去。

#### 提高消费者的吞吐量

如果消费的时候发现消费的比较慢，那么可以提高消费者的并行度，常见的就是部署更多的consumer机器

但是这里要注意，你的Topic的MessageQueue得是有对应的增加，因为如果你的consumer机器有5台，然后MessageQueue只有4
个，那么意味着有一个consumer机器是获取不到消息的。

然后就是可以增加consumer的线程数量，可以设置consumer端的参数：consumeThreadMin、consumeThreadMax，这样一台
consumer机器上的消费线程越多，消费的速度就越快。

此外，还可以开启消费者的批量消费功能，就是设置consumeMessageBatchMaxSize参数，他默认是1，但是你可以设置的多一些，
那么一次就会交给你的回调函数一批消息给你来处理了，此时你可以通过SQL语句一次性批量处理一些数据，比如：update xxx set
xxx where id in (xx,xx,xx)。

通过批量处理消息的方式，也可以大幅度提升消息消费的速度。

#### 要不要消费历史消息
consumer是支持设置从哪里开始消费消息的，常见的有两种：一个是从Topic的第一条数据开始消费，一个是从最后一次消费过
的消息之后开始消费。对应的是：CONSUME_FROM_LAST_OFFSET，CONSUME_FROM_FIRST_OFFSET

一般来说，我们都会选择CONSUME_FROM_FIRST_OFFSET，这样你刚开始就从Topic的第一条消息开始消费，但是以后每次重启，
你都是从上一次消费到的位置继续往后进行消费的。

### 85 企业级的RocketMQ集群如何进行权限机制的控制？

<a  href="/src/main/resources/note/中间件/rocket权限控制.md"> rocket权限控制  </a>


### 86 如何对线上生产环境的RocketMQ集群进行消息轨迹的追踪？
对于一条消息的丢失，可能就想要了解到这样的一个消息轨迹，协助我们去进行线上问题的排查，所以此时就可以使用
RocketMQ支持的消息轨迹功能，我们看下面的配置过程。

首先需要在broker的配置文件里开启traceTopicEnable=true这个选项，此时就会开启消息轨迹追踪的功能。

接着当我们开启了上述的选项之后，我们启动这个Broker的时候会自动创建出来一个内部的Topic，就是RMQ_SYS_TRACE_TOPIC，
这个Topic就是用来存储所有的消息轨迹追踪的数据的。

接着做好上述这一切事情之后，我们需要在发送消息的时候开启消息轨迹，此时创建Producer的时候要用如下的方式，下面构造函数
中的第二个参数，就是enableMsgTrace参数，他设置为true，就是说可以对消息开启轨迹追踪。

在订阅消息的时候，对于Consumer也是同理的，在构造函数的第二个参数设置为true，就是开启了消费时候的轨迹追踪。
其实大家可以思考一下，一旦当我们在Broker、Producer、Consumer都配置好了轨迹追踪之后，其实Producer在发送消息的时候，

就会上报这个消息的一些数据到内置的RMQ_SYS_TRACE_TOPIC里去

此时会上报如下的一些数据：Producer的信息、发送消息的时间、消息是否发送成功、发送消息的耗时。
接着消息到Broker端之后，Broker端也会记录消息的轨迹数据，包括如下：消息存储的Topic、消息存储的位置、消息的key、消息的
tags。

然后消息被消费到Consumer端之后，他也会上报一些轨迹数据到内置的RMQ_SYS_TRACE_TOPIC里去，包括如下一些东西：

Consumer的信息、投递消息的时间、这是第几轮投递消息、消息消费是否成功、消费这条消息的耗时。

接着如果我们想要查询消息轨迹，也很简单，在RocketMQ控制台里，在导航栏里就有一个消息轨迹，在里面可以创建查询任务，你可
以根据messageId、message key或者Topic来查询，查询任务执行完毕之后，就可以看到消息轨迹的界面了。

在消息轨迹的界面里就会展示出来刚才上面说的Producer、Broker、Consumer上报的一些轨迹数据了。

### 087 由于消费系统故障导致的RocketMQ百万消息积压问题，应该如何处理？
+ 1
    一般来说有几种方案可以快速搞定他，如果这些消息你是允许丢失的，那么此时你就可以紧急修改消费者系
    统的代码，在代码里对所有的消息都获取到就直接丢弃，不做任何的处理，这样可以迅速的让积压在MQ里的百万消息被处理掉，只不
    过处理方式就是全部丢弃而已。
    
    但是往往对很多系统而言，不能简单粗暴的丢弃这些消息，所以最常见的办法，还是先等待消费者系统底层依赖的NoSQL数据库先恢
    复了，恢复之后，就可以根据你的线上Topic的MessageQueue的数量来看看如何后续处理。

+ 2
    假如你的Topic有20个MessageQueue，然后你只有4个消费者系统在消费，那么每个消费者系统会从5个MessageQueue里获取消
    息，所以此时如果你仅仅依靠4个消费者系统是肯定不够的，毕竟MQ里积压了百万消息了。
    
    所以此时你可以临时申请16台机器多部署16个消费者系统的实例，然后20个消费者系统同时消费，每个人消费一个MessageQueue的
    消息，此时你会发现你消费的速度提高了5倍，很快积压的百万消息都会被处理完毕。
    
    但是这里你同时要考虑到你的消费者系统底层依赖的NoSQL数据库必须要能抗住临时增加了5倍的读写压力，因为原来就4个消费者系
    统在读写NoSQL，现在临时变成了20个消费者系统了。
    
    当你处理完百万积压的消息之后，就可以下线多余的16台机器了。
    
    这是一个最最常见的处理百万消息积压的办法

+ 3 那么如果你的Topic总共就只有4个MessageQueue，然后你就只有4个消费者系统呢？

    这个时候就没办法扩容消费者系统了，因为你加再多的消费者系统，还是只有4个MessageQueue，没法并行消费。
    
    所以此时往往是临时修改那4个消费者系统的代码，让他们获取到消息然后不写入NoSQL，而是直接把消息写入一个新的Topic，这个
    速度是很快的，因为仅仅是读写MQ而已。
    
    然后新的Topic有20个MessageQueue，然后再部署20台临时增加的消费者系统，去消费新的Topic后写入数据到NoSQL里去，这样
    子也可以迅速的增加消费者系统的并行处理能力，使用一个新的Topic来允许更多的消费者系统并行处理。

### 088 金融级的系统如何针对RocketMQ集群崩溃设计高可用方案？

跟金钱相关的一些系统，他可能需要依赖MQ去传递消息，如果你MQ突然崩溃了，可能导致很多跟钱相关的东西就会出问题。

针对这种场景，我们通常都会在你发送消息到MQ的那个系统中设计高可用的降级方案，这个降级方案通常的思路是，你需要在你发送
消息到MQ代码里去try catch捕获异常，如果你发现发送消息到MQ有异常，此时你需要进行重试。

如果你发现连续重试了比如超过3次还是失败，说明此时可能就是你的MQ集群彻底崩溃了，此时你必须把这条重要的消息写入到本地
存储中去，可以是写入数据库里，也可以是写入到机器的本地磁盘文件里去，或者是NoSQL存储中去

之后你要不停的尝试发送消息到MQ去，一旦发现MQ集群恢复了，你必须有一个后台线程可以把之前持久化存储的消息都查询出来，

然后依次按照顺序发送到MQ集群里去，这样才能保证你的消息不会因为MQ彻底崩溃会丢失。

这里要有一个很关键的注意点，就是你把消息写入存储中暂存时，一定要保证他的顺序，比如按照顺序一条一条的写入本地磁盘文件去
暂存消息。

而且一旦MQ集群故障了，你后续的所有写消息的代码必须严格的按照顺序把消息写入到本地磁盘文件里去暂存，这个顺序性是要严格
保证的。

### 90 设计一套Kafka到RocketMQ的双写+双读技术方案，实现无缝迁移！

假设你们公司本来线上的MQ用的主要是Kafka，现在要从Kafka迁移到RocketMQ去，那么这个迁移的过程应
该怎么做呢？应该采用什么样的技术方案来做迁移呢？

MQ集群迁移过程中的双写+双读技术方案。

一般来说，首先你要做到双写，也就是说，在你所有的Producer系统中，要引入一个双写的代码，让他同时往Kafka和RocketMQ中去
写入消息，然后多写几天，起码双写要持续个1周左右，因为MQ一般都是实时数据，里面数据也就最多保留一周。

当你的双写持续一周过后，你会发现你的Kafka和RocketMQ里的数据看起来是几乎一模一样了，因为MQ反正也就保留最近几天的数
据，当你双写持续超过一周过后，你会发现Kafka和RocketMQ里的数据几乎一模一样了。

但是光是双写还是不够的，还需要同时进行双读，也就是说在你双写的同时，你所有的Consumer系统都需要同时从Kafka和
RocketMQ里获取消息，分别都用一模一样的逻辑处理一遍。

只不过从Kafka里获取到的消息还是走核心逻辑去处理，然后可以落入数据库或者是别的存储什么的，但是对于RocketMQ里获取到的
消息，你可以用一样的逻辑处理，但是不能把处理结果具体的落入数据库之类的地方。

你的Consumer系统在同时从Kafka和RocketMQ进行消息读取的时候，你需要统计每个MQ当日读取和处理的消息的数量，这点非常
的重要，同时对于RocketMQ读取到的消息处理之后的结果，可以写入一个临时的存储中。

同时你要观察一段时间，当你发现持续双写和双读一段时间之后，如果所有的Consumer系统通过对比发现，从Kafka和RocketMQ读
取和处理的消息数量一致，同时处理之后得到的结果也都是一致的，此时就可以判断说当前Kafka和RocketMQ里的消息是一致的，而
且计算出来的结果也都是一致的。

这个时候就可以实施正式的切换了

### 91 如何从Github拉取RocketMQ源码以及导入Intellij IDEA中？

https://github.com/apache/rocketmq

### 92 如何在Intellij IDEA中启动NameServer以及本地调试源码？

+ ROCKETMQ_HOME的环境变量，他的值你的rocketmq运行目录

+ RocketMQ源码目录中的distrbution目录下的broker.conf、logback_namesvr.xml两个配置文件拷贝到刚才新建的conf目录中去，接着就需要修改这两个配置文件。

+ 修改logback_namesvr.xml这个文件，修改里面的日志的目录，修改为你的rocketmq运行目录中的logs目录。里面有很多的${user.home}，你直接把这些${user.home}全部替换为你的rocketmq运行目录就可以了。

### 93 如何在Intellij IDEA中启动Broker以及本地调试源码？

+ Broker启动的时候指定一个配置文件存放地址：-c 你的rocketmq运行目录/conf/broker.conf

+  ROCKETMQ_HOME的环境变量，你的rocketmq运行目录

+ broker配置文件的 内容
    ````
    brokerClusterName = DefaultCluster
    
    brokerName = broker-a
    
    brokerId = 0
    
    # 这是nameserver的地址
    
    namesrvAddr=127.0.0.1:9876
    
    deleteWhen = 04
    
    fileReservedTime = 48
    
    brokerRole = ASYNC_MASTER
    
    flushDiskType = ASYNC_FLUSH
    
    # 这是存储路径，你设置为你的rocketmq运行目录的store子目录
    
    storePathRootDir=你的rocketmq运行目录/store
    
    # 这是commitLog的存储路径
    
    storePathCommitLog=你的rocketmq运行目录/store/commitlog
    
    # consume queue文件的存储路径
    
    storePathConsumeQueue=你的rocketmq运行目录/store/consumequeue
    
    # 消息索引文件的存储路径
    
    storePathIndex=你的rocketmq运行目录/store/index
    
    # checkpoint文件的存储路径
    
    storeCheckpoint=你的rocketmq运行目录/store/checkpoint
    
    # abort文件的存储路径
    
    abortFile=你的rocketmq运行目录/abort
    
    # 设置topic会自动创建
    
    autoCreateTopicEnable=true
    ````

### 94 如何基于本地运行的RocketMQ进行消息的生产与消费？

就在example模块下面就有，我们可以在quickstart包下找到Producer和Consumer两个例子类

修改NameServices ,启动demo

### 95 源码分析的起点：从NameServer的启动脚本开始讲起

#### 从NameServer的启动开始说起

是基于rocketmq-master源码中的distribution/bin目录中的mqnamesrv这个脚本来启动的

是基于rocketmq-master源码中的distribution/bin目录中的mqnamesrv这个脚本来启动
````
   
set "JAVA_OPT=%JAVA_OPT% -server -Xms2g -Xmx2g -Xmn1g -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m"
set "JAVA_OPT=%JAVA_OPT% -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:SurvivorRatio=8 -XX:-UseParNewGC"
set "JAVA_OPT=%JAVA_OPT% -verbose:gc -Xloggc:"%USERPROFILE%\rmq_srv_gc.log" -XX:+PrintGCDetails"
set "JAVA_OPT=%JAVA_OPT% -XX:-OmitStackTraceInFastThrow"
set "JAVA_OPT=%JAVA_OPT% -XX:-UseLargePages"
set "JAVA_OPT=%JAVA_OPT% -Djava.ext.dirs=%BASE_DIR%lib"
set "JAVA_OPT=%JAVA_OPT% -cp "%CLASSPATH%""

"%JAVA%" %JAVA_OPT% %*
````

设置了jvm 参数,具体看jvm调优

你使用mqnamesrv脚本启动NameServer的时候，本质就是基于java命令启动了一个JVM进程，执行
NamesrvStartup类中的main()方法，完成NameServer启动的全部流程和逻辑，同时启动NameServer这个JVM进程的时
候，有一大堆的默认JVM参数，你当然可以在这里修改这些JVM参数，甚至进行优化。

#### 初步看一眼NamesrvStartup的main()方法
````
      public static void main(String[] args) {
          main0(args);
      }
      // 真正的mian
    public static NamesrvController main0(String[] args) {
        try {
            NamesrvController controller = createNamesrvController(args); // 在里面初始化了namesrv和netty,就是设置了它们的参数
            start(controller); //主要是启动了一个netty服务用来监听
            String tip = "The Name Server boot success. serializeType=" + RemotingCommand.getSerializeTypeConfigInThisServer();
            log.info(tip);
            System.out.printf("%s%n", tip);
            return controller;
        } catch (Throwable e) {
            e.printStackTrace();
            System.exit(-1);
        }
    
        return null;
    }
````

### 96 NameServer在启动的时候都会解析哪些配置信息？

#### NamesrvController到底是个什么东西？

> NamesrvController controller = createNamesrvController(args); 

NamesrvController这个组件，就是NameServer中专门用来接受Broker和客户端的网络请求的一个组件！因为平时我们写Java Web系统的时候，大家都喜欢用Spring MVC框架，在Spring MVC
框架中，用于接受HTTP请求的，就是Controlller组件！

<img src="https://s1.ax1x.com/2020/07/28/aExpyd.jpg" alt="aExpyd.jpg" border="0" />

#### NamesrvController是如何被创建出来的？
源码
https://gitee.com/mirrors/rocketmq/blob/master/namesrv/src/main/java/org/apache/rocketmq/namesrv/NamesrvStartup.java

截取部分代码
  ````
      //用来解析命令行中的参数args，保留在commandLIne中，new Option("h","help",false,"print help");
          Options options = ServerUtil.buildCommandlineOptions(new Options());
          commandLine = ServerUtil.parseCmdLine("mqnamesrv", args, buildCommandlineOptions(options), new PosixParser());
          if (null == commandLine) {
              System.exit(-1);
              return null;
          }
          // 这两个配置类 是重点
          final NamesrvConfig namesrvConfig = new NamesrvConfig(); // namesrv配置文件存放在该类
          final NettyServerConfig nettyServerConfig = new NettyServerConfig(); //netty的配置信息存放在该类
          nettyServerConfig.setListenPort(9876);
      
          //如果在启动namesrv的时候，使用了命令-c 指定了配置文件，那么会将配置文件变量与namesrvConfig/nettyServerConfig中的变量进行设定。具体拿Method中以set开头，setXXX方法中的XXX进行对应，配置文件到对象的生成。
          if (commandLine.hasOption('c')) {
              String file = commandLine.getOptionValue('c');
              if (file != null) {
                  InputStream in = new BufferedInputStream(new FileInputStream(file));
                  properties = new Properties();
                  properties.load(in);
                  MixAll.properties2Object(properties, namesrvConfig);
                  MixAll.properties2Object(properties, nettyServerConfig);
      
                  namesrvConfig.setConfigStorePath(file);
      
                  System.out.printf("load config properties file OK, %s%n", file);
                  in.close();
              }
          }
          
          //-p 打印配置信息 ， 如果命令带有-p参数,则打印出NamesrvConfig、NettyServerConfig的属性 
          if (commandLine.hasOption('p')) {
              InternalLogger console = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_CONSOLE_NAME);
              MixAll.printObjectProperties(console, namesrvConfig);
              MixAll.printObjectProperties(console, nettyServerConfig);
              System.exit(0);
          }
      
          MixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig);
      
          // 判断ROCKETMQ_HOME不能为空 
          if (null == namesrvConfig.getRocketmqHome()) {
              System.out.printf("Please set the %s variable in your environment to match the location of the RocketMQ installation%n", MixAll.ROCKETMQ_HOME_ENV);
              System.exit(-2);
          }
  ````

#### 看看NameServer的两个核心配置类里都包含了什么？

````
     public class NamesrvConfig {
         /**
          * RocketMQ安装目录
          * 如果没有指定的话，默认值为系统环境变量ROCKETMQ_HOME
          * 通过System.getenv获取，可以在~/.profile中export
          * 或者可以在配置文件中指定rocketmqHome=***
          */
         private String rocketmqHome = System.getProperty(MixAll.ROCKETMQ_HOME_PROPERTY, 
                 System.getenv(MixAll.ROCKETMQ_HOME_ENV));
         /**
          * KV配置持久化地址
          * 默认为System.getProperty("user.home")/namesrv/kvConfig.json文件
          */
         private String kvConfigPath = System.getProperty("user.home") + 
                 File.separator + "namesrv" + 
                 File.separator + "kvConfig.json";
         /**
          * 持久化配置路径
          * 默认为System.getProperty("user.home")/namesrv/namesrv.properties文件
          */
         private String configStorePath = System.getProperty("user.home") + 
                 File.separator + "namesrv" + 
                 File.separator + "namesrv.properties";
         // 下面三个属性暂不清楚是干嘛的
         private String productEnvName = "center";
         private boolean clusterTest = false;
         private boolean orderMessageEnable = false;
         ...
     }
````
NettyServerConfig类
````

     public class NettyServerConfig implements Cloneable {
         // 默认监听端口 代码设为9876
         private int listenPort = 8888;
         // Netty服务工作线程数量
         private int serverWorkerThreads = 8;
         // Netty服务异步回调线程池线程数量
         private int serverCallbackExecutorThreads = 0;
         // Netty Selector线程数量
         private int serverSelectorThreads = 3;
         // 控制单向的信号量
         private int serverOnewaySemaphoreValue = 256;
         // 控制异步信号量
         private int serverAsyncSemaphoreValue = 64;
         // 服务空闲心跳检测时间间隔 单位秒
         private int serverChannelMaxIdleTimeSeconds = 120;
         // Netty发送缓冲区
         private int serverSocketSndBufSize = NettySystemConfig.socketSndbufSize;
         // Netty接受缓冲区
         private int serverSocketRcvBufSize = NettySystemConfig.socketRcvbufSize;
         // 是否使用Netty内存池
         private boolean serverPooledByteBufAllocatorEnable = true;
         ...
     }
````
#### NameServer的核心配置到底是如何进行解析的？
````
       /**
             * 如果命令带有-c参数，则读取文件内容，转换成全局Properties
             * 通过反射，将Properties中的值赋值给NamesrvConfig、NettyServerConfig
             */
            if (commandLine.hasOption('c')) {
                String file = commandLine.getOptionValue('c');
                if (file != null) {
                    InputStream in = new BufferedInputStream(new FileInputStream(file));
                    properties = new Properties();
                    properties.load(in);
                    MixAll.properties2Object(properties, namesrvConfig);
                    MixAll.properties2Object(properties, nettyServerConfig);
                    namesrvConfig.setConfigStorePath(file);
                    System.out.printf("load config properties file OK, " + file + "%n");
                    in.close();
                }
            }

````
 用-c选项带上了一个配置文件的
 地址，然后此时他启动的时候，运行到上面的代码，就会把你配置文件里的配置，放入两个核心配置类里去。
 
 比如你有一个配置文件是：nameserver.properties，里面有一个配置是serverWorkerThreads=16，那么上面的代码
 就会读取出来这个配置，然后覆盖到NettyServerConfig里去！

其他
````
     // 如果命令带有-p参数，则打印出NamesrvConfig、NettyServerConfig的属性
            if (commandLine.hasOption('p')) {
                MixAll.printObjectProperties(null, namesrvConfig);
                MixAll.printObjectProperties(null, nettyServerConfig);
                System.exit(0);
            }
            /**
             * 解析命令行参数，并加载到namesrvConfig配置中
             * 通过debug发现，这一行在这里没用
             * commandLine2Properties()方法中将参数全名和属性值转换成Properties
             * 目前支持的参数的全名为configFile、help、namesrvAddr、printConfigItem
             * 但是NamesrvConfig类中没有与之对应的set方法，所以不知道意义何在
             */
            MixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig);
            // 判断ROCKETMQ_HOME不能为空
            if (null == namesrvConfig.getRocketmqHome()) {
                System.out.printf("Please set the " + MixAll.ROCKETMQ_HOME_ENV
                    + " variable in your environment to match the location of the RocketMQ installation%n");
                System.exit(-2);
            }
            /**
             * 初始化Logback日志工厂
             * RocketMQ默认使用Logback作为日志输出
             */
            LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory();
            JoranConfigurator configurator = new JoranConfigurator();
            configurator.setContext(lc);
            lc.reset();
            configurator.doConfigure(namesrvConfig.getRocketmqHome() + "/conf/logback_namesrv.xml");
            final Logger log = LoggerFactory.getLogger(LoggerName.NAMESRV_LOGGER_NAME);
            MixAll.printObjectProperties(log, namesrvConfig);
            MixAll.printObjectProperties(log, nettyServerConfig);
````

#### 完成NamesrvController组件的创建
````
      /**
      * 初始化NamesrvController
      * 该类是Name Server的主要控制类
      */
     final NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig);
     /**
      * remember all configs to prevent discard
      * 将全局Properties的内容复制到NamesrvController.Configuration.allConfigs中
      */
     controller.getConfiguration().registerConfig(properties);
````

### 97 NameServer是如何初始化基于Netty的网络通信架构的？

<img src="https://s1.ax1x.com/2020/07/28/aVPc1f.jpg" alt="aVPc1f.jpg" border="0" />

#### NamesrvController是如何被启动的？
上面只是把各种配置,放到类里面了,并没有启动

start(controller)这个代码，他就是启动了NamesrvController这个核心的组件！

org.apache.rocketmq.namesrv.NamesrvStartup#start：
````
     // 步骤一
     NamesrvController controller = createNamesrvController(args);
     // 步骤二
     start(controller);
     
     
     public static NamesrvController start(final NamesrvController controller) throws Exception {
     
             boolean initResult = controller.initialize();
             if (!initResult) {
                 System.exit(-3);
             }
         }
````
最为关键的一行代码就是boolean initResult = controller.initialize()这个地方，他其实就是对
NamesrvController执行了initialize初始化的操作。

#### org.apache.rocketmq.namesrv.NamesrvController#initialize：
````
    public boolean initialize() {
        // 加载KV配置
        this.kvConfigManager.load();
        // 创建Netty网络服务对象
        this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService);
    
        this.remotingExecutor =
            Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl("RemotingExecutorThread_"));
        this.registerProcessor();
    
        // 创建定时任务--每个10s扫描一次Broker，并定时剔除不活跃的Broker
        this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                NamesrvController.this.routeInfoManager.scanNotActiveBroker();
            }
        }, 5, 10, TimeUnit.SECONDS);
    
        // 创建定时任务--每个10分钟打印一遍KV配置
        this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                NamesrvController.this.kvConfigManager.printAllPeriodically();
            }
        }, 1, 10, TimeUnit.MINUTES);
    
        // ...
    
        return true;
    }
````

该方法主要是对核心控制器进行启动前的一些初始化操作，包括根据NamesrvConfig的kvConfigPath存储KV配置属性的路径加载KV配置，创建定时任务：每个10s扫描一次Broker，并定时剔除不活跃的Broker；每个10分钟打印一遍KV配置。

这里的每个10s扫描一次Broker，并定时剔除不活跃的Broker，这里是路由删除的一些逻辑

#### NettyRemotingServer是如何初始化的？
org.apache.rocketmq.remoting.netty.NettyRemotingServer#NettyRemotingServer

> this.serverBootstrap = new ServerBootstrap();

主要 这个ServerBootstrap，就是Netty里的一个核心的类，他就是代表了一个Netty网络服务器，通过这个东西，最终可以让Netty监听一个端口号上的网络请求。


### 98 NameServer最终是如何启动Netty网络通信服务器的？

<img src="https://s1.ax1x.com/2020/07/28/aVk074.jpg" alt="aVk074.jpg" border="0" />

#### 回到start(controller)方法里看看
  org.apache.rocketmq.namesrv.NamesrvStartup#start：
````
public static NamesrvController start(final NamesrvController controller) throws Exception {

    if (null == controller) {
        throw new IllegalArgumentException("NamesrvController is null");
    }

    // 对核心控制器进行初始化操作
    boolean initResult = controller.initialize();
    if (!initResult) {
        controller.shutdown();
        System.exit(-3);
    }

    // 注册一个钩子函数，用于JVM进程关闭时，优雅地释放netty服务、线程池等资源
    Runtime.getRuntime().addShutdownHook(new ShutdownHookThread(log, new Callable<Void>() {
        @Override
        public Void call() throws Exception {
            controller.shutdown();
            return null;
        }
    }));

    // 核心控制器启动操作
    controller.start();

    return controller;
}
````
controller.initialize()初始化这块代码我们实际上已经看完了，知道他已经初始化了Netty服务器出来，然后接
着我们往下看他通过Runtime类注册了一个JVM关闭时候的shutdown钩子，就是JVM关闭的时候会执行上述注册的回调函数。

那个回调函数里执行了NamesrvController.shutdown()方法，其实我们都不用看里面的代码，都会知道，这里无非都
是一些关闭Netty服务器的释放网络资源和线程资源的一些代码

org.apache.rocketmq.namesrv.NamesrvController#shutdown
````
   public void shutdown() {
         this.remotingServer.shutdown();
         this.remotingExecutor.shutdown();
         this.scheduledExecutorService.shutdown();
 
         if (this.fileWatchService != null) {
             this.fileWatchService.shutdown();
         }
     }
````
他就是在关闭NettyRemotingServer释放网络资源，然后关闭RemotingExecutor就是释放
Netty服务器的工作线程池的资源，还有关闭ScheduledExecutorService就是释放执行定时任务的后台线程资源。

其实这里最关键的一行代码是：controller.start()。说白了，他已经初始化了Netty服务器了，但是现在还没启动，没
启动的话，Netty服务器就不会监听9876这个默认的端口号，那么NameServer就什么也干不了。

#### org.apache.rocketmq.namesrv.NamesrvController#start：
````
     public void start() throws Exception {
         this.remotingServer.start();
     
         if (this.fileWatchService != null) {
             this.fileWatchService.start();
         }
     }
````

这里最终调用的 org.apache.rocketmq.remoting.netty.NettyRemotingServer#start

部分代码
````

    ....
    
     ServerBootstrap childHandler =
        this.serverBootstrap.group(this.eventLoopGroupBoss, this.eventLoopGroupSelector)
            .channel(useEpoll() ? EpollServerSocketChannel.class : NioServerSocketChannel.class)
            .option(ChannelOption.SO_BACKLOG, 1024)
            .option(ChannelOption.SO_REUSEADDR, true)
            .option(ChannelOption.SO_KEEPALIVE, false)
            .childOption(ChannelOption.TCP_NODELAY, true)
            .childOption(ChannelOption.SO_SNDBUF, nettyServerConfig.getServerSocketSndBufSize())
            .childOption(ChannelOption.SO_RCVBUF, nettyServerConfig.getServerSocketRcvBufSize())
            .localAddress(new InetSocketAddress(this.nettyServerConfig.getListenPort()))
            .childHandler(new ChannelInitializer<SocketChannel>() {
                @Override
                public void initChannel(SocketChannel ch) throws Exception {
                    ch.pipeline()
                        .addLast(defaultEventExecutorGroup, HANDSHAKE_HANDLER_NAME, handshakeHandler)
                        .addLast(defaultEventExecutorGroup,
                            encoder,
                            new NettyDecoder(),
                            new IdleStateHandler(0, 0, nettyServerConfig.getServerChannelMaxIdleTimeSeconds()),
                            connectionManageHandler,
                            serverHandler
                        );
                }
            });

    if (nettyServerConfig.isServerPooledByteBufAllocatorEnable()) {
        childHandler.childOption(ChannelOption.ALLOCATOR, PooledByteBufAllocator.DEFAULT);
    }

    try {
        // 启动 ,绑定监听端口
        ChannelFuture sync = this.serverBootstrap.bind().sync();
        InetSocketAddress addr = (InetSocketAddress) sync.channel().localAddress();
        this.port = addr.getPort();
    } catch (InterruptedException e1) {
        throw new RuntimeException("this.serverBootstrap.bind().sync() InterruptedException", e1);
    }
    
    ...
````
<img src="https://s1.ax1x.com/2020/07/28/aVVOu6.png" alt="aVVOu6.png" border="0" />

### 99 Broker启动的时候是如何初始化自己的核心配置的？

他的脚本和nameser 一样 ,直接进代码

org.apache.rocketmq.broker.BrokerStartup#main
````
    public static void main(String[] args) {
      start(createBrokerController(args));
    }
````
创建配置类
初始化配置主要任务是根据 properties 文件以及命令行参数值，创建了以下配置类：
   
 +  nettyServerConfig：封装了作为消息队列服务器的配置信息
 +  nettyClientConfig：封装了作为NameServer客户端配置信息
 +  brokerConfig：封装了 Broker 配置信息
  + messageStoreConfig：封装了 RocketMQ 存储系统的配置信息
  
org.apache.rocketmq.broker.BrokerStartup#createBrokerController

````
    public static BrokerController createBrokerController(String[] args) {
        //设置rocketMQ版本信息
        System.setProperty(RemotingCommand.REMOTING_VERSION_KEY, Integer.toString(MQVersion.CURRENT_VERSION));
    
        //验远程通信的发送缓存和接收缓存是否为空, 如果为空则设置默认值大小为131072
        if (null == System.getProperty(NettySystemConfig.COM_ROCKETMQ_REMOTING_SOCKET_SNDBUF_SIZE)) {
            NettySystemConfig.socketSndbufSize = 131072;
        }
    
        if (null == System.getProperty(NettySystemConfig.COM_ROCKETMQ_REMOTING_SOCKET_RCVBUF_SIZE)) {
            NettySystemConfig.socketRcvbufSize = 131072;
        }
    
        try {
            //解析命令行参数
              //PackageConflictDetect.detectFastjson();
              Options options = ServerUtil.buildCommandlineOptions(new Options());
              commandLine = ServerUtil.parseCmdLine("mqbroker", args, buildCommandlineOptions(options),
                  new PosixParser());
              if (null == commandLine) {
                  System.exit(-1);
              }
    
              final BrokerConfig brokerConfig = new BrokerConfig();
              //netty服务器配置，与生产者通信
              final NettyServerConfig nettyServerConfig = new NettyServerConfig();
              //netty客户端配置，与NameSever通信
              final NettyClientConfig nettyClientConfig = new NettyClientConfig();
    
              nettyClientConfig.setUseTLS(Boolean.parseBoolean(System.getProperty(TLS_ENABLE,
                  String.valueOf(TlsSystemConfig.tlsMode == TlsMode.ENFORCING))));
              nettyServerConfig.setListenPort(10911);
              
              //这是个储存信息的配置信息
              
              final MessageStoreConfig messageStoreConfig = new MessageStoreConfig();
    
              //如果是从节点
              if (BrokerRole.SLAVE == messageStoreConfig.getBrokerRole()) {
                  int ratio = messageStoreConfig.getAccessMessageInMemoryMaxRatio() - 10;
                  messageStoreConfig.setAccessMessageInMemoryMaxRatio(ratio);
              }
          
            /**
             * 如果启动命令行参数包含 -c 参数，会读取配置到Propertis中, 让后通过MixAll.properties2Object(),
             * 将读取的配置文件信息存入brokerConfig, nettyServerConfig, nettyClientConfig, messageStoreConfig对应的实体类中
             */
            if (commandLine.hasOption('c')) {
                String file = commandLine.getOptionValue('c');
                if (file != null) {
                    configFile = file;
                    InputStream in = new BufferedInputStream(new FileInputStream(file));
                    properties = new Properties();
                    properties.load(in);
    
                    properties2SystemEnv(properties);
                    MixAll.properties2Object(properties, brokerConfig);
                    MixAll.properties2Object(properties, nettyServerConfig);
                    MixAll.properties2Object(properties, nettyClientConfig);
                    MixAll.properties2Object(properties, messageStoreConfig);
    
                    BrokerPathConfigHelper.setBrokerConfigPath(file);
                    in.close();
                }
            }
             
             // 解析命令参数
             
            MixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), brokerConfig);
             
             // 没有RocketmqHome 会直接退出
            if (null == brokerConfig.getRocketmqHome()) {
                System.out.printf("Please set the %s variable in your environment to match the location of the RocketMQ installation", MixAll.ROCKETMQ_HOME_ENV);
                System.exit(-2);
            }
    
            //从brokerConfig获取namesrvAddr信息，并将地址信息转为SocketAddress对象
            String namesrvAddr = brokerConfig.getNamesrvAddr();
            if (null != namesrvAddr) {
                try {
                    String[] addrArray = namesrvAddr.split(";");
                    for (String addr : addrArray) {
                        RemotingUtil.string2SocketAddress(addr);
                    }
                } catch (Exception e) {
                    System.out.printf(
                        "The Name Server Address[%s] illegal, please set it as follows, \"127.0.0.1:9876;192.168.0.1:9876\"%n",
                        namesrvAddr);
                    System.exit(-3);
                }
            }
    
            //设置当前broker的角色(master,slave), 如果是同步/异步MASTER信息，brokerId=0 ;如果是SLAVE信息，brokerId > 0 ; 如果brokerId < 0 , 会抛出异常
            switch (messageStoreConfig.getBrokerRole()) {
                case ASYNC_MASTER:
                case SYNC_MASTER:
                    brokerConfig.setBrokerId(MixAll.MASTER_ID);
                    break;
                case SLAVE:
                    if (brokerConfig.getBrokerId() <= 0) {
                        System.out.printf("Slave's brokerId must be > 0");
                        System.exit(-3);
                    }
    
                    break;
                default:
                    break;
            }
              //是否选择 dleger技术
             if (messageStoreConfig.isEnableDLegerCommitLog()) {
                 brokerConfig.setBrokerId(-1);
             }
             
            messageStoreConfig.setHaListenPort(nettyServerConfig.getListenPort() + 1);
            LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory();
            JoranConfigurator configurator = new JoranConfigurator();
            configurator.setContext(lc);
            lc.reset();
            configurator.doConfigure(brokerConfig.getRocketmqHome() + "/conf/logback_broker.xml");
    
            // ... 日志配置  ,略
    
            final BrokerController controller = new BrokerController(
                brokerConfig,
                nettyServerConfig,
                nettyClientConfig,
                messageStoreConfig);
            // remember all configs to prevent discard
            controller.getConfiguration().registerConfig(properties);
    
            boolean initResult = controller.initialize();
            if (!initResult) {
                controller.shutdown();
                System.exit(-3);
            }
    
            //通过Runtime.getRuntime().addShutdownHook()设置，在jvm关闭之前需要处理的一些事情，系统会处理内存清理、对象销毁等一系列操作
            Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {
                private volatile boolean hasShutdown = false;
                private AtomicInteger shutdownTimes = new AtomicInteger(0);
    
                @Override
                public void run() {
                    synchronized (this) {
                        log.info("Shutdown hook was invoked, {}", this.shutdownTimes.incrementAndGet());
                        if (!this.hasShutdown) {
                            this.hasShutdown = true;
                            long beginTime = System.currentTimeMillis();
                            controller.shutdown();
                            long consumingTimeTotal = System.currentTimeMillis() - beginTime;
                            log.info("Shutdown hook over, consuming total time(ms): {}", consumingTimeTotal);
                        }
                    }
                }
            }, "ShutdownHook"));
    
            return controller;
        } catch (Throwable e) {
            e.printStackTrace();
            System.exit(-1);
        }
    
        return null;
    }
````

### 100 BrokerController是如何构建出来的，以及他包含了哪些组件？
### 101 在初始化BrokerController的时候，都干了哪些事情？
### 102 BrokerContorller在启动的时候，都干了哪些事儿？

当配置信息设置完毕后, broker会将这些信息传入brokerController控制器当中，这个控制器会初始化加载很多的管理器：
+ rocketmq配置文件信息可以参考如下链接地址
+ rocketMQ配置文件信息
+ topicManager : 用于管理broker中存储的所有topic的配置
+ consumerOffsetManager: 管理Consumer的消费进度
+ subscriptionGroupManager: 用来管理订阅组，包括订阅权限等。

当管理器全部加载完毕后，控制器将开始进入下一步初始化，初始化多个线程池，包括sendMessageExecutor、pullMessageExecutor、adminBrokerExecutor、clientManagerExecutor, 分别用于发送消息执行器、拉取消息执行器、broker管理器执行器、客户端管理执行器， 这些执行器回放如线程池中处理, 来做并发执行

org.apache.rocketmq.broker.BrokerController#initialize

````
    public boolean  initialize() throws CloneNotSupportedException {
        //用于管理broker中存储的所有topic的配置
        boolean result = this.topicConfigManager.load();
        //管理Consumer的消费进度
        result = result && this.consumerOffsetManager.load();
        //用来管理订阅组，包括订阅权限等
        result = result && this.subscriptionGroupManager.load();
        // 用于消费者过滤配置
        result = result && this.consumerFilterManager.load();
    
        if (result) {//加载topicConfigManager、consumerOffsetManager、subscriptionGroupManager、consumerFilterManager ，将加载结果成功与否存储在result中
    
            try {
                //用于broker层的消息落地存储
                this.messageStore =
                    new DefaultMessageStore(this.messageStoreConfig, this.brokerStatsManager, this.messageArrivingListener,
                        this.brokerConfig);
                  // 如果有DLeger  需要去初始化
                        
                 if (messageStoreConfig.isEnableDLegerCommitLog()) {
                        DLedgerRoleChangeHandler roleChangeHandler = new DLedgerRoleChangeHandler(this, (DefaultMessageStore) messageStore);
                        ((DLedgerCommitLog)((DefaultMessageStore) messageStore).getCommitLog()).getdLedgerServer().getdLedgerLeaderElector().addRoleChangeHandler(roleChangeHandler);
                    }
                     
                 // 统计的组件
                this.brokerStats = new BrokerStats((DefaultMessageStore) this.messageStore);
                //load plugin
                MessageStorePluginContext context = new MessageStorePluginContext(messageStoreConfig, brokerStatsManager, messageArrivingListener, brokerConfig);
                this.messageStore = MessageStoreFactory.build(context, this.messageStore);
                this.messageStore.getDispatcherList().addFirst(new CommitLogDispatcherCalcBitMap(this.brokerConfig, this.consumerFilterManager));
            } catch (IOException e) {
                result = false;
                log.error("Failed to initialize", e);
            }
        }
    
        result = result && this.messageStore.load();
    
        if (result) {
            this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.clientHousekeepingService);
            NettyServerConfig fastConfig = (NettyServerConfig) this.nettyServerConfig.clone();
            fastConfig.setListenPort(nettyServerConfig.getListenPort() - 2);
            this.fastRemotingServer = new NettyRemotingServer(fastConfig, this.clientHousekeepingService);
            //发送消息的线程组
            this.sendMessageExecutor = new BrokerFixedThreadPoolExecutor(
                this.brokerConfig.getSendMessageThreadPoolNums(),
                this.brokerConfig.getSendMessageThreadPoolNums(),
                1000 * 60,
                TimeUnit.MILLISECONDS,
                this.sendThreadPoolQueue,
                new ThreadFactoryImpl("SendMessageThread_"));
    
            //设置拉取消息的线程组
            this.pullMessageExecutor = new BrokerFixedThreadPoolExecutor(
                this.brokerConfig.getPullMessageThreadPoolNums(),
                this.brokerConfig.getPullMessageThreadPoolNums(),
                1000 * 60,
                TimeUnit.MILLISECONDS,
                this.pullThreadPoolQueue,
                new ThreadFactoryImpl("PullMessageThread_"));
    
            //设置查询消息的线程组
            this.queryMessageExecutor = new BrokerFixedThreadPoolExecutor(
                this.brokerConfig.getQueryMessageThreadPoolNums(),
                this.brokerConfig.getQueryMessageThreadPoolNums(),
                1000 * 60,
                TimeUnit.MILLISECONDS,
                this.queryThreadPoolQueue,
                new ThreadFactoryImpl("QueryMessageThread_"));
    
            //broker管理器执行器
            this.adminBrokerExecutor =
                Executors.newFixedThreadPool(this.brokerConfig.getAdminBrokerThreadPoolNums(), new ThreadFactoryImpl(
                    "AdminBrokerThread_"));
    
            //客户端管理执行器
            this.clientManageExecutor = new ThreadPoolExecutor(
                this.brokerConfig.getClientManageThreadPoolNums(),
                this.brokerConfig.getClientManageThreadPoolNums(),
                1000 * 60,
                TimeUnit.MILLISECONDS,
                this.clientManagerThreadPoolQueue,
                new ThreadFactoryImpl("ClientManageThread_"));
    
            //心跳
            this.heartbeatExecutor = new BrokerFixedThreadPoolExecutor(
                this.brokerConfig.getHeartbeatThreadPoolNums(),
                this.brokerConfig.getHeartbeatThreadPoolNums(),
                1000 * 60,
                TimeUnit.MILLISECONDS,
                this.heartbeatThreadPoolQueue,
                new ThreadFactoryImpl("HeartbeatThread_", true));
    
            this.endTransactionExecutor = new BrokerFixedThreadPoolExecutor(
                this.brokerConfig.getEndTransactionThreadPoolNums(),
                this.brokerConfig.getEndTransactionThreadPoolNums(),
                1000 * 60,
                TimeUnit.MILLISECONDS,
                this.endTransactionThreadPoolQueue,
                new ThreadFactoryImpl("EndTransactionThread_"));
    
            //消费者管理线程组
            this.consumerManageExecutor =
                Executors.newFixedThreadPool(this.brokerConfig.getConsumerManageThreadPoolNums(), new ThreadFactoryImpl(
                    "ConsumerManageThread_"));
    
            //把刚刚创建的执行器注册到 remotingServer, fastRemotingServer对象中
            this.registerProcessor();
    
            final long initialDelay = UtilAll.computNextMorningTimeMillis() - System.currentTimeMillis();
            final long period = 1000 * 60 * 60 * 24;
            //定期查询如下信息
    
            //记录broker的状态
            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
                @Override
                public void run() {
                    try {
                        BrokerController.this.getBrokerStats().record();
                    } catch (Throwable e) {
                        log.error("schedule record error.", e);
                    }
                }
            }, initialDelay, period, TimeUnit.MILLISECONDS);
    
            //消费者当前信息的offset位置
            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
                @Override
                public void run() {
                    try {
                        BrokerController.this.consumerOffsetManager.persist();
                    } catch (Throwable e) {
                        log.error("schedule persist consumerOffset error.", e);
                    }
                }
            }, 1000 * 10, this.brokerConfig.getFlushConsumerOffsetInterval(), TimeUnit.MILLISECONDS);
    
            //消费者filterManaer信息
            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
                @Override
                public void run() {
                    try {
                        BrokerController.this.consumerFilterManager.persist();
                    } catch (Throwable e) {
                        log.error("schedule persist consumer filter error.", e);
                    }
                }
            }, 1000 * 10, 1000 * 10, TimeUnit.MILLISECONDS);
    
            //保护Broker，当Broker响应慢的时候
            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
                @Override
                public void run() {
                    try {
                        BrokerController.this.protectBroker();
                    } catch (Throwable e) {
                        log.error("protectBroker error.", e);
                    }
                }
            }, 3, 3, TimeUnit.MINUTES);
    
    
            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
                @Override
                public void run() {
                    try {
                        BrokerController.this.printWaterMark();
                    } catch (Throwable e) {
                        log.error("printWaterMark error.", e);
                    }
                }
            }, 10, 1, TimeUnit.SECONDS);
    
            //打印日志信息
            this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
    
                @Override
                public void run() {
                    try {
                        log.info("dispatch behind commit log {} bytes", BrokerController.this.getMessageStore().dispatchBehindBytes());
                    } catch (Throwable e) {
                        log.error("schedule dispatchBehindBytes error.", e);
                    }
                }
            }, 1000 * 10, 1000 * 60, TimeUnit.MILLISECONDS);
    
            //获取namesrv地址信息
            if (this.brokerConfig.getNamesrvAddr() != null) {
                this.brokerOuterAPI.updateNameServerAddressList(this.brokerConfig.getNamesrvAddr());
                log.info("Set user specified name server address: {}", this.brokerConfig.getNamesrvAddr());
            } else if (this.brokerConfig.isFetchNamesrvAddrByAddressServer()) {
                this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
    
                    @Override
                    public void run() {
                        try {
                            BrokerController.this.brokerOuterAPI.fetchNameServerAddr();
                        } catch (Throwable e) {
                            log.error("ScheduledTask fetchNameServerAddr exception", e);
                        }
                    }
                }, 1000 * 10, 1000 * 60 * 2, TimeUnit.MILLISECONDS);
            }
    
            if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) {
                if (this.messageStoreConfig.getHaMasterAddress() != null && this.messageStoreConfig.getHaMasterAddress().length() >= 6) {
                    this.messageStore.updateHaMasterAddress(this.messageStoreConfig.getHaMasterAddress());
                    this.updateMasterHAServerAddrPeriodically = false;
                } else {
                    this.updateMasterHAServerAddrPeriodically = true;
                }
                //同步从broker信息
                this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
    
                    @Override
                    public void run() {
                        try {
                            BrokerController.this.slaveSynchronize.syncAll();
                        } catch (Throwable e) {
                            log.error("ScheduledTask syncAll slave exception", e);
                        }
                    }
                }, 1000 * 10, 1000 * 60, TimeUnit.MILLISECONDS);
            } else {
                this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
    
                    @Override
                    public void run() {
                        try {
                            BrokerController.this.printMasterAndSlaveDiff();
                        } catch (Throwable e) {
                            log.error("schedule printMasterAndSlaveDiff error.", e);
                        }
                    }
                }, 1000 * 10, 1000 * 60, TimeUnit.MILLISECONDS);
            }
        }
        //省略部分代码
    }
````



### 103 第三个场景驱动：Broker是如何把自己注册到NameServer去的？

关键的一点，就是他执行了将自己注册到NameServer的一个过程，我们看一下这个注册自己到NameServer的源码入口，下面这行代码就是在BrokerController.start()方法中
>BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister());

 在BrokerStartup类的main方法运行的时候，创建了BrokerController，然后调用了BrokerController的start方法
````
// 启动的时候向每个NameServer发起注册
this.registerBrokerAll(true, false, true);
 
this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                try {
                    // Broker会每隔30s向NameSrv注册并更新自身topic信息,完成心跳功能
                    BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister());
                } catch (Throwable e) {
                    log.error("registerBrokerAll Exception", e);
                }
            }
        }, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS);
````

org.apache.rocketmq.broker.BrokerController#registerBrokerAll
````
     // true false true
     public synchronized void registerBrokerAll(final boolean checkOrderConfig, boolean oneway, boolean forceRegister) {
         // ConcurrentMap<String, TopicConfig> topicConfigTable
         // 将topicConfigTable封装到TopicConfigSerializeWrapper中
         TopicConfigSerializeWrapper topicConfigWrapper = this.getTopicConfigManager().buildTopicConfigSerializeWrapper();
  
         // 判断权限如果为不可读或不可写那么久在拼装一下topicConfigTable到TopicConfigSerializeWrapper中
         if (!PermName.isWriteable(this.getBrokerConfig().getBrokerPermission())
             || !PermName.isReadable(this.getBrokerConfig().getBrokerPermission())) {
             ConcurrentHashMap<String, TopicConfig> topicConfigTable = new ConcurrentHashMap<String, TopicConfig>();
             for (TopicConfig topicConfig : topicConfigWrapper.getTopicConfigTable().values()) {
                 TopicConfig tmp =
                     new TopicConfig(topicConfig.getTopicName(), topicConfig.getReadQueueNums(), topicConfig.getWriteQueueNums(),
                         this.brokerConfig.getBrokerPermission());
                 topicConfigTable.put(topicConfig.getTopicName(), tmp);
             }
             topicConfigWrapper.setTopicConfigTable(topicConfigTable);
         }
  
         // forceRegister是否强制注册
         // needRegister方法是与配置的每个NameServer进行通信，判断topicConfigTable是否改变了，只要其中一个改变了那么就需要发起注册
         // QUERY_DATA_VERSION = 322;
         if (forceRegister || needRegister(this.brokerConfig.getBrokerClusterName(),
                 this.getBrokerAddr(),
                 this.brokerConfig.getBrokerName(),
                 this.brokerConfig.getBrokerId(),
                 this.brokerConfig.getRegisterBrokerTimeoutMills())) {
             // REGISTER_BROKER = 103;
             doRegisterBrokerAll(checkOrderConfig, oneway, topicConfigWrapper);
         }
     }
  
 ```` 
 ####  继续探索真正的进行Broker注册的方法
 org.apache.rocketmq.broker.BrokerController#doRegisterBrokerAll
 ```` 
 private void doRegisterBrokerAll(boolean checkOrderConfig, boolean oneway,
         TopicConfigSerializeWrapper topicConfigWrapper) {
         List<RegisterBrokerResult> registerBrokerResultList = this.brokerOuterAPI.registerBrokerAll(
             this.brokerConfig.getBrokerClusterName(),
             this.getBrokerAddr(),
             this.brokerConfig.getBrokerName(),
             this.brokerConfig.getBrokerId(),
             this.getHAServerAddr(),
             topicConfigWrapper,
             this.filterServerManager.buildNewFilterServerList(),
             oneway,
             this.brokerConfig.getRegisterBrokerTimeoutMills(),
             this.brokerConfig.isCompressedRegister());
  
         if (registerBrokerResultList.size() > 0) {
             RegisterBrokerResult registerBrokerResult = registerBrokerResultList.get(0);
             if (registerBrokerResult != null) {
             
                 // 根据updateMasterHAServerAddrPeriodically标注位（在初始化时若Broker的配置文件中没有haMasterAddress参数配置，则标记为true，表示注册之后需要更新主用Broker地址）
                 // 以及NameServer返回的HaServerAddr地址是否为空，若标记位是true且返回的HaServerAddr不为空，则用HaServerAddr地址更新HAService.HAClient.masterAddress的值；
                 // 该HAClient.masterAddress值用于主备Broker之间的commitlog数据同步之用
                 if (this.updateMasterHAServerAddrPeriodically && registerBrokerResult.getHaServerAddr() != null) {
                     this.messageStore.updateHaMasterAddress(registerBrokerResult.getHaServerAddr());
                 }
  
                 // 用NameServer返回的MasterAddr值更新SlaveSynchronize.masterAddr值，用于主备Broker同步Config文件使用；
                 this.slaveSynchronize.setMasterAddr(registerBrokerResult.getMasterAddr());
  
                 if (checkOrderConfig) {
                     this.getTopicConfigManager().updateOrderTopicConfig(registerBrokerResult.getKvTable());
                 }
             }
         }
     }
````
doRegisterBrokerAll 调用了 org.apache.rocketmq.broker.out.BrokerOuterAPI#registerBrokerAll 返回了List<RegisterBrokerResult>
,这里的list 是因为注册所有NameSer ,实际注册是org.apache.rocketmq.broker.out.BrokerOuterAPI#registerBroker


### 106 Broker是如何发送定时心跳的，以及如何进行故障感知？
Broker中的发送注册请求给NameServer的一个源码入口，其实就是在BrokerController.start()方法中，在BrokerController启动的时候，他其实并不是仅仅发送一次注册请求，而是启动了一个定时任务，会每隔一段时间就发送一次注册请求。

````
     this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                try {
                    BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister());
                } catch (Throwable e) {
                    log.error("registerBrokerAll Exception", e);
                }
            }
        }, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS);
````

其实是启动了一个定时调度的任务，他默认是每隔30s就会执行一次Broker注册的过程，上面的registerNameServerPeriod是一个配置，他默认的值就是30s一次。

默认情况下，第一次发送注册请求就是在进行注册，他会把Broker路由数据放入到NameServer的RouteInfoManager的路由数据表里去。

但是后续每隔30s他都会发送一次注册请求，这些后续定时发送的注册请求，其实本质上就是Broker发送心跳给NameServer了

那么后续每隔30s，Broker就发送一次注册请求，作为心跳来发送给NameServer的时候，NameServer对后续重复发送过来的注册请求（也就是心跳）

nameserver 的 org.apache.rocketmq.namesrv.routeinfo.BrokerLiveInfo#registerBroker


````
    // RouteInfoManager#registerBroker
    public RegisterBrokerResult registerBroker(
        final String clusterName,
        final String brokerAddr,
        final String brokerName,
        final long brokerId,
        final String haServerAddr,
        final TopicConfigSerializeWrapper topicConfigWrapper,
        final List<String> filterServerList,
        final Channel channel) {
        RegisterBrokerResult result = new RegisterBrokerResult();
        try {
            try {
                // 加锁，防止并发修改RouteInfoManager中的路由表
                this.lock.writeLock().lockInterruptibly();
    
                // 判断broker所属集群是否存在，如果不存在则创建，然后将改broker加入到集群中去
                Set<String> brokerNames = this.clusterAddrTable.get(clusterName);
                if (null == brokerNames) {
                    brokerNames = new HashSet<String>();
                    this.clusterAddrTable.put(clusterName, brokerNames);
                }
                brokerNames.add(brokerName);
    
                // 是否首次注册标记
                boolean registerFirst = false;
    
                // 维护brokerData信息；如果不存在则创建并放入到brokerAddrTable集合中
                // HashMap<String/* brokerName */, BrokerData> brokerAddrTable
                BrokerData brokerData = this.brokerAddrTable.get(brokerName);
                if (null == brokerData) {
                    registerFirst = true;
                    brokerData = new BrokerData(clusterName, brokerName, new HashMap<Long, String>());
                    this.brokerAddrTable.put(brokerName, brokerData);
                }
                Map<Long, String> brokerAddrsMap = brokerData.getBrokerAddrs();
                //Switch slave to master: first remove <1, IP:PORT> in namesrv, then add <0, IP:PORT>
                //The same IP:PORT must only have one record in brokerAddrTable
                Iterator<Entry<Long, String>> it = brokerAddrsMap.entrySet().iterator();
                while (it.hasNext()) {
                    Entry<Long, String> item = it.next();
                    if (null != brokerAddr && brokerAddr.equals(item.getValue()) && brokerId != item.getKey()) {
                        it.remove();
                    }
                }
    
                String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr);
                registerFirst = registerFirst || (null == oldAddr);
    
                // 如果broker为master，并且broker topic配置信息发生变化或者初次注册则需要创建或者更新Topic路由元数据，填充topicQueueTable
                // 就是为默认主题自动注册路由信息，其中包含MixAll.DEFAULT_TOPIC的路由信息。当消息生产者发送主题时，
                // 如果该主题未创建并且BrokerConfig的autoCreateTopicEnable为true时，将返回MixALL.DEFAULT_TOPIC的路由信息
                if (null != topicConfigWrapper
                    && MixAll.MASTER_ID == brokerId) {
                    if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion())
                        || registerFirst) {
                        ConcurrentMap<String, TopicConfig> tcTable =
                            topicConfigWrapper.getTopicConfigTable();
                        if (tcTable != null) {
                            for (Map.Entry<String, TopicConfig> entry : tcTable.entrySet()) {
                                this.createAndUpdateQueueData(brokerName, entry.getValue());
                            }
                        }
                    }
                }
    
                // 更新prevBrokerLiveInfo
                BrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr,
                    new BrokerLiveInfo(
                        System.currentTimeMillis(),
                        topicConfigWrapper.getDataVersion(),
                        channel,
                        haServerAddr));
                if (null == prevBrokerLiveInfo) {
                    log.info("new broker registered, {} HAServer: {}", brokerAddr, haServerAddr);
                }
    
                if (filterServerList != null) {
                    if (filterServerList.isEmpty()) {
                        this.filterServerTable.remove(brokerAddr);
                    } else {
                        this.filterServerTable.put(brokerAddr, filterServerList);
                    }
                }
    
                if (MixAll.MASTER_ID != brokerId) {
                    String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID);
                    if (masterAddr != null) {
                        BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr);
                        if (brokerLiveInfo != null) {
                            result.setHaServerAddr(brokerLiveInfo.getHaServerAddr());
                            result.setMasterAddr(masterAddr);
                        }
                    }
                }
            } finally {
                this.lock.writeLock().unlock();
            }
        } catch (Exception e) {
            log.error("registerBroker Exception", e);
        }
    
        return result;
    }
````    

每隔30s你发送注册请求作为心跳的时候，RouteInfoManager里会进行心跳时间刷新的处理。

那么假设Broker已经挂了，或者故障了，隔了很久都没有发送那个每隔30s一次的注册请求作为心跳，那么此时NameServer是如何感知到这个Broker已经挂掉的呢

我们重新回到NamesrvController的initialize()方法里去，里面有一个代码是启动了RouteInfoManager中的一个定时扫描不活跃Broker的线程。

org.apache.rocketmq.namesrv.NamesrvController#initialize
````
 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {

            @Override
            public void run() {
                NamesrvController.this.routeInfoManager.scanNotActiveBroker();
            }
        }, 5, 10, TimeUnit.SECONDS);
````

org.apache.rocketmq.namesrv.routeinfo.BrokerLiveInfo#scanNotActiveBroker

````
     public void scanNotActiveBroker() {
           Iterator<Entry<String, BrokerLiveInfo>> it = this.brokerLiveTable.entrySet().iterator();
           while (it.hasNext()) {
               Entry<String, BrokerLiveInfo> next = it.next();
               long last = next.getValue().getLastUpdateTimestamp();
               
               // 判断超过 默认120 没有心跳,就剔除
               if ((last + BROKER_CHANNEL_EXPIRED_TIME) < System.currentTimeMillis()) {
                   RemotingUtil.closeChannel(next.getValue().getChannel());
                   it.remove();
                   log.warn("The broker channel expired, {} {}ms", next.getKey(), BROKER_CHANNEL_EXPIRED_TIME);
                   this.onChannelDestroy(next.getKey(), next.getValue().getChannel());
               }
           }
       }

````

### 107 我们系统中使用的Producer是如何创建出来的？


##### 回顾
NameServer启动之后，就会有一个核心的NamesrvController组件，他就是用于控制NameServer的所有
行为的，包括内部启动一个Netty服务器去监听一个9876端口号，然后接收处理Broker和客户端发送过来的请求。

Broker启动之后的核心架构

Broker启动之后，最核心的就是有一个BrokerController组件管控Broker的整体行为，包括初始化自己的
Netty服务器用于接收客户端的网络请求，包括启动处理请求的线程池、执行定时任务的线程池，初始化核心功能组
件，同时还会启动之后发送注册请求到NameServer去注册自己。

同时我们之前还讲解完了Broker启动之后进行注册以及定时发送注册请求作为心跳的机制，以及NameServer有一个
后台进程定时检查每个Broker的最近一次心跳时间，如果长时间没心跳就认为Broker已经故障

你可以认为在我们的RocketMQ集群里，已经启动好了NameServer，
而且还启动了一批Broker，同时Broker都已经把自己注册到NameServer里去了，NameServer也会去检查这批
Broker是否存活。

其实此时我们不需要去关注NameServer和Broker干了别的什么事情，这个时候我们只要知道已经有了一个可用的
RocketMQ集群就可以了

#### Producer
````
DefaultMQProducer producer = new DefaultMQProducer("order_producer_group");
producer.setNamesrvAddr("localhost:9876");
producer.start();
````
大家可以看到，其实构造Producer很简单，就是创建一个DefaultMQProducer对象实例，在其中传入你所属的Producer分组，然后设
置一下NameServer的地址，最后调用他的start()方法，启动这个Producer就可以了。

其实创建DefaultMQProducer对象实例是一个非常简单的过程，无非就是创建这么一个对象出来，然后保存一下他的Producer分组。
设置NameServer地址也是一个很简单的过程，无非就是保存一下NameServer地址罢了。

其实最核心的还是调用了这个DefaultMQProducer的start()方法去启动了这个消息生产组件

### 108 构建好的Producer是如何启动准备好相关资源的？

首先我们都知道一件事儿，假设我们后续要通过Producer发送消息，必然会指定我们要往哪个Topic里发送消息。所以我们也知道，Producer必然是知道Topic的一些路由数据的，比如Topic有哪些MessageQueue，每个MessageQueue在哪些Broker上

那么现在问题来了，到底是Producer刚启动初始化的时候，就会去拉取每个Topic的路由数据呢？还是等你第一次往一个Topic发送消息的时候再拉取路由数据呢？

其实答案是显而易见的，肯定不可能是刚初始化启动的时候就拉取Topic的路由数据，因为你刚开始启动的时候，不知道要发送消息到哪个Topic去啊！

所以这个问题，一定是在你第一次发送消息到Topic的时候，才会去拉取一个Topic的路由数据，包括这个Topic有几个MessageQueue，每个MessageQueue在哪个Broker上，然后从中选择一个MessageQueue，跟那台Broker建立网络连接，发送消息过去。

所以此时我们说第二个问题，Producer发送消息必然要跟Broker建立网络，这个是在Producer刚启动的时候就立马跟所有的Broker建立网络连接吗？

那必然也不是的，因为此时你也不知道你要跟哪个Broker进行通信。

所以其实很多核心的逻辑，包括Topic路有数据拉取，MessageQueue选择，以及跟Broker建立网络连接，通过网络连接发送消息到Broker去，这些逻辑都是在Producer发送消息的时候才会有。

### 109 当我们发送消息的时候，是如何从NameServer拉取Topic元数据的？

Producer初始化的过程

其实初始化的过程极为的复杂，但是我们却真的不用过于的深究，因为其实比如拉取Topic的路由数据，选择MessageQueue，跟Broker构建长连接，发送消息过去，这些核心的逻辑，都是封装在发送消息的方法中的。

因此我们今天就从发送消息的方法开始讲起，实际上当你调用Producer的send()方法发送消息的时候，这个方法调用会一直到比较底层的逻辑里去，最终会调用到DefaultMQProducerImpl类的sendDefaultImpl()方法里去，在这个方法里，上来什么都没干，直接就有一行非常关键的代码，如下。

org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendDefaultImpl
> TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic());

其实看到这行代码，大家就什么都明白了，每次你发送消息的时候，他都会先去检查一下，这个你要发送消息的那个Topic的路由数据是否在你客户端本地

如果不在的话，必然会发送请求到NameServer那里去拉取一下的，然后缓存在客户端本地。

其实当你进入了this.tryToFindTopicPublishInfo(msg.getTopic())这个方法逻辑之后，会发现他的逻辑非常的简单

其实简单来说，他就是先检查了一下自己本地是否有这个Topic的路由数据的缓存，如果没有的话就发送网络请求到NameServer去拉取，如果有的话，就直接返回本地Topic路由数据缓存了


其实看源码，一个是看源码的技巧，一个就是从源码里提取核心业务逻辑和流程，之前我们已经给大家讲了很多看源码的技巧了

所以接着我们当然很想知道的是，Producer到底是如何发送网络请求到NameServer去拉取Topic路由数据的，其实这里就对应了tryToFindTopicPublishInfo()方法内的一行代码，我们看看。

org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#tryToFindTopicPublishInfo
>this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic);

通过这行代码，他就可以去从NameServer拉取某个Topic的路由数据，然后更新到自己本地的缓存里去了。

具体的发送请求到NameServer的拉取过程，其实之前都大致讲解到了，简单来说，就是封装一个Request请求对象，然后通过底层的Netty客户端发送请求到NameServer，接收到一个Response响应对象。

然后他就会从Response响应对象里取出来自己需要的Topic路由数据，更新到自己本地缓存里去，更新的时候会做一些判断，比如Topic路由数据是否有改变过，等等，然后把Topic路由数据放本地缓存就可以了

### 110 对于一条消息，Producer是如何选择MessageQueue去发送的？
Producer发送消息的时候，上来不管三七二十一，其实会先检查一下要发送消息的Topic的路由数据是否在本地缓存，如果不在的话，就会通过底层的Netty网络通信模块去发送一个请求到NameServer去拉取Topic路由数据，然后缓存在Producer的本地。

当你拿到了一个Topic的路由数据之后，其实接下来就应该选择要发送消息到这个Topic的哪一个MessageQueue上去了！

因为大家都知道，Topic是一个逻辑上的概念，一个Topic的数据往往是分布式存储在多台Broker机器上的，因此Topic本质是由多个MessageQueue组成的。

每个MessageQueue都可以在不同的Broker机器上，当然也可能一个Topic的多个MessageQueue在一个Broker机器上

### 到底应该发送到这个Topic的哪个MessageQueue上去呢？

只要你知道了要发送消息到哪个MessageQueue上去，然后就知道这个MessageQueue在哪台Broker机器上，接着就跟那台Broker机器建立连接，发送消息给他就可以了。

发送消息的核心源码是在DefaultMQProducerImpl.sendDefaultImpl()方法中的，在这个方法里，只要你获取到了Topic的路由数据，不管从本地缓存获取的，还是从NameServer拉取到的，接着就会执行下面的核心代码。

org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendDefaultImpl #567
> MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName);

这行代码其实就是在选择Topic中的一个MessageQueue，然后发送消息到这个MessageQueue去，在这行代码里面，实现了一些Broker故障自动回避机制

org.apache.rocketmq.client.latency.MQFaultStrategy# selectOneMessageQueue
 核心代码
````

    int index = tpInfo.getSendWhichQueue().getAndIncrement();
    for (int i = 0; i < tpInfo.getMessageQueueList().size(); i++) {
        int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size();
        if (pos < 0)
            pos = 0;
        MessageQueue mq = tpInfo.getMessageQueueList().get(pos);
        if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) {
            if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName))
                return mq;
        }
    }
````

上面的代码其实非常的简单，他先获取到了一个自增长的index，大家注意到没有？

接着其实他核心的就是用这个index对Topic的MessageQueue列表进行了取模操作，获取到了一个MessageQueue列表的位置，然后返回了这个位置的MessageQueue。

说实话，你只要自己去试试就知道了，这种操作就是一种简单的负载均衡的算法，比如一个Topic有8个MessageQueue，那么可能第一次发送消息到MessageQueue01，第二次就发送消息到MessageQueue02，以此类推，就是轮询把消息发送到各个MessageQueue而已！

这就是最基本的MessageQueue选择算法，那万一某个Broker故障了呢？此时发送消息到哪里去呢？

所以其实这个算法里有很多别的代码，都是实现Broker规避机制的

### 111 我们的系统与RocketMQ Broker之间是如何进行网络通信的？
Producer是如何从Topic路由数据中选择一个MessageQueue出来的，在选择一个MessageQueue出来之后，接着其实就应该要把消息投递到那个MessageQueue所在的Broker上去了

#### Producer是如何把消息发送给Broker的呢？

其实这块代码就在DefaultMQProducerImpl.sendDefaultImpl()方法中，在这个方法里，先是获取到了MessageQueue所在的broker名称

org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendDefaultImpl #570
````
 brokersSent[times] = mq.getBrokerName();
````

获取到了这个brokerName之后，接着其实就可以使用如下的代码把消息投递到那个Broker上去了
org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendDefaultImpl #583
````
sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime);
````

这个代码里是如何把消息投递出去，在这个方法里，先有一些较为简单的逻辑

org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl#sendKernelImpl 
````
    String brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());
    if (null == brokerAddr) {
        tryToFindTopicPublishInfo(mq.getTopic());
        brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());
    }
````

代码片段其实非常简单，就是通过brokerName去本地缓存找他的实际的地址，如果找不到，就去找NameServer拉取Topic的路由数据，然后再次在本地缓存获取broker的实际地址，你有这个地址了，才能给人家进行网络通信。

接下来的源码就很繁琐细节了，他就是用自己的方式去封装了一个Request请求出来，这里涉及到了各种信息的封装，包括了请求头，还有一大堆所有你需要的数据，都封装在Request里了。

他在这里做的事情，大体上包括了给消息分配全局唯一ID、对超过4KB的消息体进行压缩，在消息Request中包含了生产者组、Topic名称、Topic的MessageQueue数量、MessageQueue的ID、消息发送时间、消息的flag、消息扩展属性、消息重试次数、是否是批量发送的消息、如果是事务消息则带上prepared标记，等等。

总之，这里就是封装了很多很多的数据就对了，这些东西都封装到一个Request里去，然后在底层还是通过Netty把这个请求发送出去，发送到指定的Broker上去就可以了

这里Producer和Broker之间都是通过Netty建立长连接，然后基于长连接进行持续的通信的


### 112 当Broker获取到一条消息之后，他是如何存储这条消息的？

Broker通过Netty网络服务器获取到一条消息，接着就会把这条消息写入到一个CommitLog文件里去，一个Broker机器上就只有一个CommitLog文件，所有Topic的消息都会写入到一个文件里去

然后同时还会以异步的方式把消息写入到ConsumeQueue文件里去，因为一个Topic有多个MessageQueue，任何一条消息都是写入一个MessageQueue的，那个MessageQueue其实就是对应了一个ConsumeQueue文件

所以一条写入MessageQueue的消息，必然会异步进入对应的ConsumeQueue文件

同时还会异步把消息写入一个IndexFile里，在里面主要就是把每条消息的key和消息在CommitLog中的offset偏移量做一个索引，这样后续如果要根据消息key从CommitLog文件里查询消息，就可以根据IndexFile的索引来了

#### 写入流程
一步一步分析一下他在这里写入这几个文件的一个流程

首先Broker收到一个消息之后，必然是先写入CommitLog文件的，那么这个CommitLog文件在磁盘上的目录结构大致如何呢？看下面

CommitLog文件的存储目录是在${ROCKETMQ_HOME}/store/commitlog下的，里面会有很多的CommitLog文件，每个文件默认是1GB大小，一个文件写满了就创建一个新的文件，文件名的话，就是文件中的第一个偏移量，如下面所示。文件名如果不足20位的话，就用0来补齐就可以了。

>00000000000000000000

>000000000003052631924

在把消息写入CommitLog文件的时候，会申请一个putMessageLock锁

也就是说，在Broker上写入消息到CommitLog文件的时候，都是串行的，不会让你并发的写入，并发写入文件必然会有数据错乱的问题

````
   protected final PutMessageLock putMessageLock;
   
   putMessageLock.lock();
````

接着其实会对消息做出一通处理，包括设置消息的存储时间、创建全局唯一的消息ID、计算消息的总长度，然后会走一段很关键的源码，把消息写入到MappedFile里去，这个其实我们之前还讲解过里面的黑科技

org.apache.rocketmq.store.MappedFile# appendMessagesInner

````
        ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice();
        byteBuffer.position(currentPos);
        AppendMessageResult result;
        if (messageExt instanceof MessageExtBrokerInner) {
            result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBrokerInner) messageExt);
        } else if (messageExt instanceof MessageExtBatch) {
            result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBatch) messageExt);
        } else {
            return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR);
        }
        this.wrotePosition.addAndGet(result.getWroteBytes());
        this.storeTimestamp = result.getStoreTimestamp();
        return result;
````

上面源码片段中，其实最关键的是cb.doAppend()这行代码，这行代码其实是把消息追加到MappedFile映射的一块内存里去，并没有直接刷入磁盘中

至于具体什么时候才会把内存里的数据刷入磁盘，其实要看我们配置的刷盘策略，另外就是不管是同步刷盘还是异步刷盘，假设你配置了主从同步，一旦你写入完消息到CommitLog之后，接下来都会进行主从同步复制的。

### 113 一条消息写入CommitLog文件之后，如何实时更新索引文件？
Broker收到一条消息之后，其实就会直接把消息写入到CommitLog里去，但是他写入刚开始仅仅是写入到MappedFile映射的一块内存里去，后续是根据刷盘策略去决定是否立即把数据从内存刷入磁盘的

#### 消息写入CommitLog之后，然后消息是如何进入ConsumeQueue和IndexFile的。

实际上，Broker启动的时候会开启一个线程，ReputMessageService，他会把CommitLog更新事件转发出去，然后让任务处理器去更新ConsumeQueue和IndexFile

在DefaultMessageStore的start()方法里，在里面就是启动了这个ReputMessageService线程。

这个DefaultMessageStore的start()方法就是在Broker启动的时候调用的，所以相当于是Broker启动就会启动这个线程。
org.apache.rocketmq.store.DefaultMessageStore#start
````
        this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);
        this.reputMessageService.start();
````

ReputMessageService线程的运行逻辑

org.apache.rocketmq.store.DefaultMessageStore$ReputMessageService#run
````
      public void run() {
            DefaultMessageStore.log.info(this.getServiceName() + " service started");

            while (!this.isStopped()) {
                try {
                    Thread.sleep(1);
                    this.doReput();
                } catch (Exception e) {
                    DefaultMessageStore.log.warn(this.getServiceName() + " service has exception. ", e);
                }
            }

            DefaultMessageStore.log.info(this.getServiceName() + " service end");
        }

        @Override
        public String getServiceName() {
            return ReputMessageService.class.getSimpleName();
        }

    }
````
也就是说，在这个线程里，每隔1毫秒，就会把最近写入CommitLog的消息进行一次转发，转发到ConsumeQueue和IndexFile里去，通过的是doReput()方法来实现的，我们再看doReput()方法里的实现逻辑

org.apache.rocketmq.store.DefaultMessageStore$ReputMessageService#doReput

````
    DispatchRequest dispatchRequest =
                                   DefaultMessageStore.this.commitLog.checkMessageAndReturnSize(result.getByteBuffer(), false, false);
````

这段代码意思非常的清晰明了，就是从commitLog中去获取到一个DispatchRequest，拿到了一份需要进行转发的消息，也就是从CommitLog中读取的

接着他就会通过调用doDispatch()方法去把消息进行转发，一个是转发到ConsumeQueue里去，一个是转发到IndexFile里去
org.apache.rocketmq.store.DefaultMessageStore$ReputMessageService#doDispatch
````
        public void doDispatch(DispatchRequest req) {
              for (CommitLogDispatcher dispatcher : this.dispatcherList) {
                  dispatcher.dispatch(req);
              }
          }
````

实际上正常来说这个CommitLogDispatcher的实现类有两个，分别是CommitLogDispatcherBuildConsumeQueue和CommitLogDispatcherBuildIndex，他们俩分别会负责把消息转发到ConsumeQueue和IndexFile

ConsumeQueueDispatche的源码实现逻辑，其实非常的简单，就是找到当前Topic的messageQueueId对应的一个ConsumeQueue文件

一个MessageQueue会对应多个ConsumeQueue文件，找到一个即可，然后消息写入其中。

会调用 org.apache.rocketmq.store.DefaultMessageStore#putMessagePositionInfo
````
     public void putMessagePositionInfo(DispatchRequest dispatchRequest) {
           ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId());
           cq.putMessagePositionInfoWrapper(dispatchRequest);
       }
````

再来看看IndexFile的写入逻辑，其实也很简单，无非就是在IndexFile里去构建对应的索引罢了

org.apache.rocketmq.store.DefaultMessageStore$CommitLogDispatcherBuildIndex#dispatch
````
     public void dispatch(DispatchRequest request) {
        if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) {
            DefaultMessageStore.this.indexService.buildIndex(request);
        }
    }
````
 当我们把消息写入到CommitLog之后，有一个后台线程每隔1毫秒就会去拉取CommitLog中最新更新的一批消息，然后分别转发到ConsumeQueue和IndexFile里去，这就是他底层的实现原理。
 
 
### 114 RocketMQ是如何实现同步刷盘以及异步刷盘两种策略的？

数据写入到Broker之后的存储流程，包括数据直接写入CommitLog，而且直接进入的是MappedFile映射的一块内存，不是直接进入磁盘，同时有一个后台线程会把CommitLog里更新的数据给写入到ConsumeQueue和IndexFile里去

写入CommitLog的数据进入到MappedFile映射的一块内存里之后，后续会执行刷盘策略

比如是同步刷盘还是异步刷盘，如果是同步刷盘，那么此时就会直接把内存里的数据写入磁盘文件，如果是异步刷盘，那么就是过一段时间之后，再把数据刷入磁盘文件里去

#### 底层到底是如何执行不同的刷盘策略的?

org.apache.rocketmq.store.CommitLog#putMessage  #670

````
       handleDiskFlush(result, putMessageResult, msg);
       handleHA(result, putMessageResult, msg);
    
       return putMessageResult;
````

末尾有两个方法调用，一个是handleDishFlush()，一个是handleHA()

顾名思义，一个就是用于决定如何进行刷盘的，一个是用于决定如何把消息同步给Slave Broker的。

关于消息如何同步给Slave Broker，这个我们就不看了，因为涉及到Broker高可用机制，这里展开说就太多了，其实大家有兴趣可以自己慢慢去研究，我们这里主要就是讲解一些RocketMQ的核心源码原理。

所以我们重点进入到handleDiskFlush()方法里去，看看他是如何处理刷盘的。

org.apache.rocketmq.store.CommitLog#handleDiskFlush

````
      public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) {
            //同步
            // Synchronization flush
            if (FlushDiskType.SYNC_FLUSH == this.defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) {
              
            }
            // 异步
            // Asynchronous flush
            else {
               
            }
        }
````
上面代码我们就看的很清晰了，其实他里面是根据你配置的两种不同的刷盘策略分别处理的，我们先看第一种，就是同步刷盘的策略是如何处理的。

同步代码
````
       final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;
        if (messageExt.isWaitStoreMsgOK()) {
            GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes());
            service.putRequest(request);
            boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout());
            if (!flushOK) {
                log.error("do groupcommit, wait for flush failed, topic: " + messageExt.getTopic() + " tags: " + messageExt.getTags()
                    + " client address: " + messageExt.getBornHostString());
                putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT);
            }
        } else {
            service.wakeup();
        }
````

其实上面就是构建了一个GroupCommitRequest，然后提交给了GroupCommitService去进行处理，然后调用request.waitForFlush()方法等待同步刷盘成功

万一刷盘失败了，就打印日志。具体刷盘是由GroupCommitService执行的，他的doCommit()方法最终会执行同步刷盘的逻辑，里面有如下代码。

org.apache.rocketmq.store.CommitLog.GroupCommitService#doCommit

````
    CommitLog.this.mappedFileQueue.flush(0);
````
上面那行代码一层一层调用下去，最终刷盘其实是靠的MappedByteBuffer的force()方法
````
    this.mappedByteBuffer.force();
````

这个MappedByteBuffer就是JDK NIO包下的API，他的force()方法就是强迫把你写入内存的数据刷入到磁盘文件里去，到此就是同步刷盘成功了。

#### 那么如果是异步刷盘呢？
org.apache.rocketmq.store.CommitLog#handleDiskFlush

````
        if (!this.defaultMessageStore.getMessageStoreConfig().isTransientStorePoolEnable()) {
            flushCommitLogService.wakeup();
        } else {
            commitLogService.wakeup();
        }
````

其实这里就是唤醒了一个flushCommitLogService组件

FlushCommitLogService其实是一个线程，他是个抽象父类，他的子类是CommitRealTimeService，所以真正唤醒的是他的子类代表的线程
  ````
       abstract class FlushCommitLogService extends ServiceThread {
              protected static final int RETRY_TIMES_OVER = 10;
          }
      
          class CommitRealTimeService extends FlushCommitLogService {
          }
  ````
具体在子类线程的run()方法里就有定时刷新的逻辑
org.apache.rocketmq.store.CommitLog.CommitRealTimeService#run

````
  result = CommitLog.this.mappedFileQueue.commit(0);
  
````
其实简单来说，就是每隔一定时间执行一次刷盘，最大间隔是10s，所以一旦执行异步刷盘，那么最多就是10秒就会执行一次刷盘。

### 115 当Broker上的数据存储超过一定时间之后，磁盘数据是如何清理的？
broker不停的接收数据，然后磁盘上的数据越来越多，但是万一磁盘都放满了，那怎么办呢？

先简单给大家说一下，其实默认broker会启动后台线程，这个后台线程会自动去检查CommitLog、ConsumeQueue文件，因为这些文件都是多个的，比如CommitLog会有多个，ConsumeQueue也会有多个。

然后如果是那种比较旧的超过72小时的文件，就会被删除掉，也就是说，默认来说，broker只会给你把数据保留3天而已，当然你也可以自己通过fileReservedTime来配置这个时间，要保留几天的时间。

这个定时检查过期数据文件的线程代码，在DefaultMessageStore这个类里，他的start()方法中会调用一个addScheduleTask()方法，里面会每隔10s定时调度执行一个后台检查任务

org.apache.rocketmq.store.DefaultMessageStore#addScheduleTask

````
    this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
        @Override
        public void run() {
            DefaultMessageStore.this.cleanFilesPeriodically();
        }
    }, 1000 * 60, this.messageStoreConfig.getCleanResourceInterval(), TimeUnit.MILLISECONDS);

````

上面就可以看到了，其实他是每隔10s，就会执行一个调度任务

这个调度任务里就会执行DefaultMessageStore.this.cleanFilesPeriodically()方法，其实就是会去周期性的清理掉磁盘上的数据文件，也就是超过72小时的CommitLog、ConsumeQueue文件

org.apache.rocketmq.store.DefaultMessageStore#cleanFilesPeriodically
````
    private void cleanFilesPeriodically() {
            this.cleanCommitLogService.run();
            this.cleanConsumeQueueService.run();
        }
````

在清理文件的时候，他会具体判断一下，如果当前时间是预先设置的凌晨4点，就会触发删除文件的逻辑，这个时间是默认的；或者是如果磁盘空间不足了，就是超过了85%的使用率了，立马会触发删除文件逻辑。

上面两个条件，第一个是说如果磁盘没有满 ，那么每天就默认一次会删除磁盘文件，默认就是凌晨4点执行，那个时候必然是业务低峰期，因为凌晨4点大部分人都睡觉了，无论什么业务都不会有太高业务量的。

第二个是说，如果磁盘使用率超过85%了，那么此时可以允许继续写入数据，但是此时会立马触发删除文件的逻辑；如果磁盘使用率超过90%了，那么此时不允许在磁盘里写入新数据，立马删除文件。这是因为，一旦磁盘满了，那么你写入磁盘会失败，此时你MQ就彻底故障了。

所以一旦磁盘满了，也会立马删除文件的。

在删除文件的时候，无非就是对文件进行遍历，如果一个文件超过72小时都没修改过了，此时就可以删除了，哪怕有的消息你可能还没消费过，但是此时也不会再让你消费了，就直接删除掉。

这就是RocketMQ的一整套文件删除的逻辑和机制。

### 116 我们系统中的Consumer作为消费者是如何创建出来的？

消息写入到Broker去，他会把消息写入到CommitLog、ConsumeQueue、IndexFile里去

那么现在Broker上有了数据了，接着当然是某个业务系统里会启动一个Consumer，指定自己要消费哪个Topic的数据

接着Consumer就会从指定的Topic上消费数据过来了，然后消息交给你的业务代码来处理

#### Cosumer是如何创建和启动的呢？

其实我们平时创建的一般都是DefaultMQPushConsumerImpl，然后会调用他的start()方法来启动他，那么今天我们就来看看启动Consumer的时候都会干什么。

首先在启动的时候
org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl#start

````
    this.mQClientFactory = MQClientManager.getInstance().getOrCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook);
````

这个Consumer一旦启动，必然是要跟Broker去建立长连接的，底层绝对也是基于Netty去做的，建立长连接之后，才能不停的通信拉取消息

所以这个MQClientFactory底层封装了Netty网络通信的东西
org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl#start 其余代码
````
        this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup());
        this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel());
        this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy());
        this.rebalanceImpl.setmQClientFactory(this.mQClientFactory);
````
RebalanceImpl的东西，还给他设置了Consumer分组，还有MQClientFactory在里面

那么这个东西，其实大家一看名字就应该知道了，他就是专门负责Consumer重平衡的。

假设你的ConsumerGroup里加入了一个新的Consumer，那么就会重新分配每个Consumer消费的MessageQueue，如果ConsumerGroup里某个Consumer宕机了，也会重新分配MessageQueue，这就是所谓的重平衡
org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl#start  向下走
````
         this.pullAPIWrapper = new PullAPIWrapper(
            mQClientFactory,
            this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode());
        this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookList);

````
专门用来拉取消息的API组件

继续向下

````
   
        if (this.defaultMQPushConsumer.getOffsetStore() != null) {
            this.offsetStore = this.defaultMQPushConsumer.getOffsetStore();
        } else {
            switch (this.defaultMQPushConsumer.getMessageModel()) {
                case BROADCASTING:
                    this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
                    break;
                case CLUSTERING:
                    this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup());
                    break;
                default:
                    break;
            }
            this.defaultMQPushConsumer.setOffsetStore(this.offsetStore);
        }
        this.offsetStore.load();
````
这个东西一看，顾名思义，就是用来存储和管理Consumer消费进度offset的一个组件

接下来源码里还有一些东西，其实都不是太核心的了，最核心的无非就是这三个组件，首先Consumer刚启动，必须依托Rebalancer组件，去进行一下重平衡，自己要分配一些MessageQueue去拉取消息。

接着拉取消息，必须要依托PullAPI组件通过底层网络通信去拉取。在拉取的过程中，必然要维护offset消费进度，此时就需要OffsetStore组件。万一要是ConsumerGroup里多了Consumer或者少了Consumer，又要依托Rebalancer组件进行重平衡了。


### 117 一个消费组中的多个Consumer是如何均匀分配消息队列的？

一个业务系统部署多台机器的时候，每个系统里都启动了一个Consumer，多个Consumer会组成一个ConsumerGroup，也就是消费组，此时就会有一个消费组内的多个Consumer同时消费一个Topic，而且这个Topic是有多个MessageQueue分布在多个Broker上的

那么现在问题就来了，假设咱们一个业务系统部署在两台机器上，对应一个消费组里就有两个Consumer，那么现在一个Topic有三个MessageQueue，该怎么分配呢？



#### Consumer的负载均衡

Consumer启动的时候，讲到了几个关键的组件，分别是重平衡组件、消息拉取组件、消费进度组件

其实里面有一个Balancer重平衡组件，就是在这里专门负责多个Consumer的负载均衡的

实际上，每个Consumer在启动之后，都会干一件事情，就是向所有的Broker进行注册，并且持续保持自己的心跳，让每个Broker都能感知到一个消费组内有哪些Consumer

然后，每个Consumer在启动之后，其实重平衡组件都会随机挑选一个Broker，从里面获取到这个消费组里有哪些Consumer存在

此时重平衡组件一旦知道了消费组内有哪些Consumer之后，接着就好办了，无非就是把Topic下的MessageQueue均匀的分配给这些Consumer了，这个时候其实有几种算法可以进行分配，但是比较常用的一种算法就是简单的平均分配。

比如现在一共有3个MessageQueue，然后有2个Consumer，那么此时就会给1个Consumer分配2个MessageQueue，同时给另外1个Consumer分配剩余的1个MessageQueue。

假设有4个MessageQueue的话，那么就可以2个Consumer每个人分配2个MessageQueue了

总之，一切都是平均分配的，尽量保证每个Consumer的负载是差不多的。

这样的话，一旦MessageQueue负载确定了之后，下一步其实Consumer就知道自己要消费哪几个MessageQueue的消息了，就可以连接到那个Broker上去，从里面不停的拉取消息过来进行消费了

### 118 Consumer是如何从Broker上拉取一批消息过来处理的？

####  消费组到底是个什么概念？
消费者组的意思，就是让你给一组消费者起一个名字。比如说我们有一个Topic叫做“TopicOrderPaySuccess”，那么假设有库存系统、积分系统、营销系统、仓储系统他们都要去消费这个Topic中的数据。

此时我们应该给那四个系统分别起一个消费组的名字，比如说：stock_consumer_group，marketing_consumer_group，credie_consumer_group，wms_consumer_group。

设置消费组的方式是在代码里进行的

````
     DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4");
````
然后比如库存系统部署了4台机器，每台机器上的消费者组的名字都是“stock_consumer_group”，那么这4台机器就同属于一个消费者组，以此类推，每个系统的几台机器都是属于各自的消费者组的。

不同消费者之间的关系，假设库存系统和营销系统作为两个消费者组，都订阅了“TopicOrderPaySuccess”这个订单支付成功消息的Topic，那么此时假设订单系统作为生产者发送了一条消息到这个Topic

那么此时这条消息是怎么被消费的呢？

其实正常情况下来说，这条消息进入Broker之后，库存系统和营销系统作为两个消费组，每个组都会拉取到这条消息。

也就是说这个订单支付成功的消息，库存系统会获取到一条，营销系统也会获取到一条，他们俩都会获取到这条消息。

但是下一个问题来了，库存系统这个消费组里，他有两台机器，是两台机器都获取到这条消息？还是说只有一台机器会获取到这条消息？

答案是，正常情况下来说，库存系统的两台机器中只有一台机器会获取到这条消息，营销系统也是同理。

我们看下面的图，示意了对于一条订单支付成功的消息，库存系统的一台机器获取到了，营销系统的一台机器也获取到了

可能从Master Broker拉取的，可能从Slave Broker拉取的

不同的系统应该设置不同的消费组，如果不同的消费组订阅了同一个Topic，对Topic里的一条消息，每个消费组都会获取到这条消息。

#### 集群模式消费 vs 广播模式消费

默认情况下我们都是集群模式，也就是说，一个消费组获取到一条消息，只会交给组内的一台机器去处理，不是每台机器都可以获取到这条消息的。

但是我们可以通过如下设置来改变为广播模式：

>consumer.setMessageModel(MessageModel.BROADCASTING);

如果修改为广播模式，那么对于消费组获取到的一条消息，组内每台机器都可以获取到这条消息。但是相对而言广播模式其实用的很少，常见基本上都是使用集群模式来进行消费的。

#### MessageQueue、CommitLog、ConsumeQueue之间的关系

Topic中的多个MessageQueue会分散在多个Broker上，在每个Broker机器上，一个MessageQueue就对应了一个ConsumeQueue，当然在物理磁盘上其实是对应了多个ConsumeQueue文件的，但是我们大致也理解为一 一对应关系。

但是对于一个Broker机器而言，存储在他上面的所有Topic以及MessageQueue的消息数据都是写入一个统一的CommitLog的

然后对于Topic的各个MessageQueue而言，就是通过各个ConsumeQueue文件来存储属于MessageQueue的消息在CommitLog文件中的物理地址，就是一个offset偏移量

#### MessageQueue与消费者的关系
对于一个Topic上的多个MessageQueue，是如何由一个消费组中的多台机器来进行消费的呢？

其实这里的源码实现细节是较为复杂的，但是我们可以简单的大致理解为，他会均匀的将MessageQueue分配给消费组的多台机器来消费。

举个例子，假设我们的“TopicOrderPaySuccess”有4个MessageQueue，这4个MessageQueue分布在两个Master Broker上，每个Master Broker上有2个MessageQueue。

然后库存系统作为一个消费组里有两台机器，那么正常情况下，当然最好的就是让这两台机器每个都负责2个MessageQueue的消费了，比如库存系统的机器01从Master Broker01上消费2个MessageQueue，然后库存系统的机器02从Master Broker02上消费2个MessageQueue，这样不就把消费的负载均摊到两台Master Broker上去了？

大致可以认为一个Topic的多个MessageQueue会均匀分摊给消费组内的多个机器去消费，这里的一个原则就是，一个MessageQueue只能被一个消费机器去处理，但是一台消费者机器可以负责多个MessageQueue的消息处理。

#### Push模式 vs Pull模式

这两个消费模式本质是一样的，都是消费者机器主动发送请求到Broker机器去拉取一批消息下来。

Push消费模式本质底层也是基于这种消费者主动拉取的模式来实现的，只不过他的名字叫做Push而已，意思是Broker会尽可能实时的把新消息交给消费者机器来进行处理，他的消息时效性会更好。

一般我们使用RocketMQ的时候，消费模式通常都是基于他的Push模式来做的，因为Pull模式的代码写起来更加的复杂和繁琐，而且Push模式底层本身就是基于消息拉取的方式来做的，只不过时效性更好而已。

Push模式的实现思路我们这里简单说一下：当消费者发送请求到Broker去拉取消息的时候，如果有新的消息可以消费那么就会立马返回一批消息到消费机器去处理，处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。

所以消费机器在Push模式下会处理完一批消息，立马发起请求拉取下一批消息，消息处理的时效性非常好，看起来就跟Broker一直不停的推送消息到消费机器一样。

另外Push模式下有一个请求挂起和长轮询的机制，也要给大家简单介绍一下。

当你的请求发送到Broker，结果他发现没有新的消息给你处理的时候，就会让请求线程挂起，默认是挂起15秒，然后这个期间他会有后台线程每隔一会儿就去检查一下是否有的新的消息给你，另外如果在这个挂起过程中，如果有新的消息到达了会主动唤醒挂起的线程，然后把消息返回给你。

当然其实消费者进行消息拉取的底层源码是非常复杂的，涉及到大量的细节，但是他的核心思路大致就是如此，我们只要知道，其实哪怕是用常见的Push模式消费，本质也是消费者不停的发送请求到broker去拉取一批一批的消息就行了。

#### Broker是如何将消息读取出来返回给消费机器的？
这里要涉及到两个概念，分别是ConsumeQueue和CommitLog。

假设一个消费者机器发送了拉取请求到Broker了，他说我这次要拉取MessageQueue0中的消息，然后我之前都没拉取过消息，所以就从这个MessageQueue0中的第一条消息开始拉取好了。

于是，Broker就会找到MessageQueue0对应的ConsumeQueue0，从里面找到第一条消息的offset

接着Broker就需要根据ConsumeQueue0中找到的第一条消息的地址，去CommitLog中根据这个offset地址去读取出来这条消息的数据，然后把这条消息的数据返回给消费者机器

所以其实消费消息的时候，本质就是根据你要消费的MessageQueue以及开始消费的位置，去找到对应的ConsumeQueue读取里面对应位置的消息在CommitLog中的物理offset偏移量，然后到CommitLog中根据offset读取消息数据，返回给消费者机器。

#### 消费者机器如何处理消息、进行ACK以及提交消费进度？
消费者机器拉取到一批消息之后，就会将这批消息回调我们注册的一个函数
````
      consumer.registerMessageListener(new MessageListenerConcurrently() {
    
        @Override
        public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs,
            ConsumeConcurrentlyContext context) {
            System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs);
            return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
        }
    });
````

当我们处理完这批消息之后，消费者机器就会提交我们目前的一个消费进度到Broker上去，然后Broker就会存储我们的消费进度，比如我们现在对ConsumeQueue0的消费进度假设就是在offset=1的位置，那么他会记录下来一个ConsumeOffset的东西去标记我们的消费进度

那么下次这个消费组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始继续拉取，不用重头开始拉取了。

#### 如果消费组中出现机器宕机或者扩容加机器，会怎么处理？

会进入一个rabalance的环节，也就是说重新给各个消费机器分配他们要处理的MessageQueue。

给大家举个例子，比如现在机器01负责MessageQueue0和Message1，机器02负责MessageQueue2和MessageQueue3，现在机器02宕机了，那么机器01就会接管机器02之前负责的MessageQueue2和MessageQueue3。

或者如果此时消费组加入了一台机器03，此时就可以把机器02之前负责的MessageQueue3转移给机器03，然后机器01就仅仅负责一个MessageQueue2的消费了，这就是负载重平衡的概念


#### 消费源码流程

首先，拉取消息的源码入口是在DefaultMQPushConsumerImpl类的pullMessage()方法中的，这个里面涉及到了拉取请求、消息流量控制、通过PullAPIWrapper与服务端进行网络交互、服务端根据ConsumeQueue文件拉取消息，等一系列的事情

代码之后在贴




