
> 从0开始带你成为消息中间件实战高手 的笔记



### 01 一个真实电商订单系统的整体架构、业务流程及负载情况

#### 一个订单系统的业务流程

<img src="https://s1.ax1x.com/2020/07/20/U4vBKH.jpg" alt="U4vBKH.jpg" border="0" />


####   一个订单系统的非核心流程
<img src="https://s1.ax1x.com/2020/07/20/U4xISO.jpg" alt="U4xISO.jpg" border="0" />

### 02 授人以渔：能概括一下你们系统的架构设计、业务流程以及负载情况吗？
> 首先你得知道现在你的系统，或者行业里其他普遍的一些系统，都有哪些技术难点和痛点，面临哪些问题。

>  接着要针对这些问题，反过来去学习一个技术，学习完这个技术之后，一定要思考如何将技术代入到真实的环境中，
  去解决对应的问题。
  
### 03 系统面临的现实问题：下订单的同时还要发券、发红包、Push推送，性能太差！

> 系统的压力来自哪

+ 电商这种系统,用户都会集中在晚上几个小时,为什么就不分析了, 他的访问曲线不是平均的,而是集中在一起的

<img src="https://s1.ax1x.com/2020/07/20/U5SORf.jpg" alt="U5SORf.jpg" border="0" />

+ 这种架构的压力都会落在单点上,上图就是mysql, 依据经验 4核8g mysql 最好1000-1500 能2000,16核32SSD 1万应该是极限,可能会出问题

+ 上面业务的问题, 在流程8中 ,支付回调中 ,推送,优惠券等,会有大量业务,太耗时


### 04 授人以渔：你们系统的核心流程性能如何？有没有哪个环节拖慢了速度？
> 普通项目还是业务处理和sql

> rpc 和第三方接口调用 ,同步的

要思考，在当前这样的系统压力下：

+ 系统的核心业务流程性能如何？
+ 核心流程的每个步骤要耗费多长时间？
+ 现在核心流程的性能你满意吗？是否还有优化的空间？
+ 在系统高峰期的时候，机器和数据库负载很高，是否对核心流程的性能有影响？
+ 如果有影响的话，会有多大的影响？

那你就想，你的这个系统做一个SaaS云平台的模式，提供给几万个公司，百万用户使用，不就可以了？你要自己去模拟这个场景。

然后，你按照文中的思路去推算出系统高峰期的负载，以及你的线上系统的机器的压力，到底要部署多少机器去满足这个压力。 


### 05 系统面临的现实问题：订单退款时经常流程失败，无法完成退款！
> 退款流程

<img src="https://s1.ax1x.com/2020/07/20/U5Fwxx.jpg" alt="U5Fwxx.jpg" border="0" />

+ 问题一: 和下单一样,流程过长  

+ 问题二: 如果退款失败怎么办

+ 下单之后不付款怎么办
> 下单之后会占用库存, 不付款 ,别人也买不了这个商品

给这种订单一个超时时间,超过 取消订单

+ 如果 下单之后不付款的单子 巨多,怎么处理
> 不停地去扫描表  效率太差,占用太多资源


### 06 授人以渔：你们系统出现过核心流程链路失败的情况吗？
> low 系统..

假设下 如果失败,可以保存失败操作,开始重试

### 07 系统面临的现实问题：第三方客户系统的对接耦合性太高，经常出问题！
> 还是上面的 订单支付 流程

+ 还是流程中有很多对接的第三方系统 ,比如支付,库仓 发货

<img src="https://s1.ax1x.com/2020/07/20/U5EDs0.jpg" alt="U5EDs0.jpg" border="0" />


+ 积分,发货,优惠券等系统都是和订单系统耦合在一起的

+ 这样订单系统和第三物流系统 也算耦合到一起, 会带来不确认性, 会基于网络,第三方系统大量原因导致 性能差

### 08 授人以渔：你们有没有跟第三方系统对接过，有遇到什么问题吗？
> 同步调第三方系统 真的一言难尽,  会各种坑, 随时保存日志,记得甩锅

### 09 系统面临的现实问题：大数据团队需要订单数据，该怎么办？

#### 大数据到底是干嘛的？
> 通过实时 采集来的数据,分析用户行为和其他结论 

每天如果有100万用户来访问你的APP，积累下来的一些浏览行为、访问行为、交易行为都是各种数据，这个数据量很大，所以你可以称之为“大数据”

大数据团队每天要负责的事情，说白了就是去尽可能的搜集每天100万用户在你的APP上的各种行为数据。

#### 大数据与我们系统的关系
> 订单数据等  他们需要分析每天几十万个订单，从中提取出老板最关心的APP交易数据报表！ 

方法

   +   select 查询
       
        大sql  而且查询次数不低,  数据库的cpu 和io 占用会很高, 其他操作就效率很低了
        

### 10 授人以渔：你们有没有遇到过自己系统的数据，其他团队需要获取的？
> 没有 

假设有, 比如微服务 过来取数据, 加缓存( 可以是缓存,也可以是新表), 异步同步  查询走缓存 

### 11 系统面临的现实问题：秒杀活动时数据库压力太大，该怎么缓解？

> 上面系统的缺点 是同步流程太长,效率太差

 假如在平时晚上的高峰使用期，最顶峰的时候大概是每秒2000左右的请求压力到订单系统上来。
 
 如果用户每秒会发起2000个请求到我们的订单系统的各类接口，包括下单接口、退款接口、查询接口等等，那么你觉得我们的订单系统每秒会执行多少条SQL在订单数据库上？
 
 一般你可以认为平均每个接口会执行2~3次的数据库操作。
 
 一般一个接口根据业务复杂度的不同，有的接口可能处理一个请求要执行五六次数据库操作，有的接口可能是1次数据库操作+两三个其他系统的接口调用（比如库存系统、营销系统）。

 总之，一般来说，业务系统的接口处理逻辑，基本都集中在对自己的数据库的操作以及对其他系统的调用上
 
 
#### 双11之类的大促活动有多恐怖

如果有200万用户参与双11活动，在双11购物最高峰的时候，假设会达到每秒至少1万的QPS。

也就是说，光是系统被请求的QPS就会达到1万以上，那么系统请求数据库的QPS就会达到2万以上。16核32G SSD 数据库性能，是无论如何扛不住每秒2万请求的。 

### 12 授人以渔：你们系统会不会遇到流量洪峰的场景，导致瞬时压力过大？
> 没有

假设有,异步处理, mq 削峰,  失败了就  开重试 ,最大重试次数还是失败  ,取消吧


### 13 阶段性复习：一张思维导图给你梳理高并发订单系统面临的技术痛点！

<img src="https://s1.ax1x.com/2020/07/21/UovGkR.jpg" alt="UovGkR.jpg" border="0" />

### 14 阶段性复习：放大100倍压力，也要找出你系统的技术挑战！

+ 第一，大家先思考一下系统的核心业务流程，当然不是指那种查询之类的操作。所谓核心链路指的是对你的系统进行的数据更新的操作，这才是核心链路，因为查询操作一般来说不涉及复杂的业务逻辑，主要是对数据的展示。
    
    对你的系统的核心链路分析一下，有哪些步骤，这些步骤各自的性能如何，综合起来让你的核心链路的性能如何？在这里是否有改进的空间？



+ 第二，大家可以思考一下，在你的系统中，是否有类似后台线程定时补偿的逻辑？

    比如订单长时间未支付就要自动关闭它，你们系统里有没有那种后台线程，会定时扫描你的数据，对异常数据进行补偿、自动修复等操作的？

    如果有的话，这种数据一般量有多大？如果没有，你可以思考一下，你们系统的核心数据是否需要类似的后台自动扫描机制？


+ 第三，大家可以思考一下，在你的系统里有没有跟第三方系统进行耦合？就是一些核心流程里需要同步调用第三方系统进行查询、更新等操作，第三方系统是否对你的核心链路有性能和稳定性上的影响？

+ 第四，大家可以思考一下，在你的核心链路中，是否存在那种关键步骤可能会失败的情况？万一失败了该怎么办？



+ 第五，大家可以思考一下，平时是否存在其他系统需要获取你们数据的情况？他们是如何获取你们数据的？

    是直接跑SQL从你们数据库里查询？或者是调用你们的接口来获取数据？是否存在这种情况？如果有，对你们有什么影响吗？



+ 第六，你们的系统是否存在流量洪峰的情况，有时候突然之间访问量增大好几倍，是否会对你们的系统产生无法承受的压力？

### 15 解决订单系统诸多问题的核心技术：消息中间件到底是什么？
#### 同步调用
用户发起一个请求，系统A收到请求，接着系统A必须立马去调用系统B，直到系统B返回了，系统A才能返回结果给用户，这种模式其实就是所谓的“同步调用”。

#### 消息中间件的异步

为系统A仅仅是发个消息到MQ，至于系统B什么时候获取消息，有没有获取消息，他是不管的。所以这种情况下，我们说系统A和系统B是异步调用。

#### 作用 

主要的作用有这么几个，包括异步化提升性能，降低系统耦合，流量削峰，等等

MQ进行流量削峰的效果，系统A发送过来的每秒1万请求是一个流量洪峰，然后MQ直接给扛下来了，都存储自己本地磁盘，这个过程就是流量削峰的过程，瞬间把一个洪峰给削下来了，让系统B后续慢慢获取消息来处理。

### 16 授人以渔：结合自己的系统问题思考一下，MQ有什么用处？

假设
1. 上传文件比如file是一个系统 具体业务一个系统 他们直接用mq ,file系统接收到文件存到文件服务器写一个消息给mq然后就返给用户结果 业务系统从mq接受消息进行业务处理 把处理结果写一个消息给mq , file系统接受消息更新文件处理结果， 当用户需要刷新文件上传结果的时候查询数据库就可以了
2. 在和硬件对接比如接受考勤信息也会用mq
3. 日志
4. 异步短信提示等

### 17 领导的要求：你来对 Kafka、RabbitMQ 以及 RocketMQ 进行技术选型调研

+ RabbitMQ  单机吞吐量 万级别, 基本不丢,功能全,web管理页面很方便, 集群模式麻烦 ,基于 erlang 开发

+ Kafka   10 万级，高吞吐 ,基本不丢 ,分布式, 功能不全, 但适合大数据和日志系统

+ RocketMQ 10 万级，支撑高吞吐, 基本不丢 ,分布式, 功能全, 可视化界面

### 18 授人以渔：你们公司主要使用的 MQ 是哪种？为什么要选用它？

 小项目和以前的 很多都是Rabbit ,熟悉,而且能支撑服务需求
 
 但现在Roket 好像用的不少

### 19 新技术引入：给团队分享 RocketMQ 的架构原理和使用方式
+ RocketMQ是如何集群化部署来承载高并发访问的？
+ 如果RocketMQ中要存储海量消息，如何实现分布式存储架构？

#### MQ如何集群化部署来支撑高并发访问？
发送消息到MQ的系统会把消息分散发送给多台不同的机器，假设一共有1万条消息，分散发送给10台机器，可能每台机器就是接收到1000条消息

每台机器上部署的RocketMQ进程一般称之为Broker，每个Broker都会收到不同的消息，然后就会把这批消息存储在自己本地的磁盘文件里

#### 高可用保障：万一Broker宕机了怎么办？
RocketMQ的解决思路是Broker主从架构以及多副本策略。

Master Broker收到消息之后会同步给Slave Broker，这样Slave Broker上就能有一模一样的一份副本数据！

这样同一条消息在RocketMQ整个集群里不就有两个副本了，一个在Master Broker里，一个在Slave Broker里！

这个时候如果任何一个Master Broker出现故障，还有一个Slave Broker上有一份数据副本，可以保证数据不丢失，还能继续对外提供服务，保证了MQ的可靠性和高可用性

#### 数据路由：怎么知道访问哪个Broker？

有一个NameServer的概念，他也是独立部署在几台机器上的 , 如果是集群, 注册的时候 会在每个NameServer 都注册


### 20 授人以渔：结合你对其他 MQ 的了解，思考 RocketMQ 的设计有何特点？

kafka集群架构分为leader和follower，每个topic下分为partition，partition分为primary和replication，partition主节点和副本节点存储在不同的节点上来保证集群的高可用，每个消息的partition可以分配到不同的节点来分布式存储，follower来同步leader节点的信息

kafka和Rocketmq都是分布式的消息系统，

在集群化部署方面，kafka通过zk进行节点协调，而rocketmq通过自身namesrv进行节点协调，所以在协调节点的设计上rocket显得更加轻。 

存储方面，在Kafka中，是1个topic（就是一个业务场景）有多个partition（对应3个物理文件目录），当需要存储数据的时候，会把topic中一个parition大文件分成多个小文件段。通过多个小文件段，就轻松实现定期清除或删除已经消费完文件。降低磁盘占用。

在rocketmq中，采用的是混合型的存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。

两者在对比上，Kafka采用的是独立型的存储结构，每个队列一个文件。RocketMQ采用混合型存储结构的缺点在于，会存在较多的随机读操作，因此读的效率偏低。同时消费消息需要依赖ConsumeQueue，构建该逻辑消费队列需要一定开销。 

主从备份方面：生产者向kafka写入消息时，一般会写入多个分区（partition），kafka提供冗余机制，即每个分区都有多个相同的备份(replica)，kafka把分区所有副本均匀分配到其他broker上，并从这些副本中挑选一个作为leader副本对外提供服务，其他副本称为follwer副本。当leader副本所在的broker有可能宕机，这时follower副本会竞争成为leader，继续提供服务。 

而当生产者者向rocketmq写入消息时，会将数据写入集群中的相关master broker上，而每个master broker都有1到多个slave broker, 这样在一定程度上保证master出现了不可恢复的故障时，不丢失数据。 同时如果master宕机了，消费者会自动重连到相应的salve上，不会出现消费停滞，那么同时在master和slave数据同步分为同步复制(有一定的效率损失)和异步复制(数据不一致) 

在生产和消费消息方面: 在kafka中，每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic，物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处，而这种能力实现的底层我想应该就是通过zk来完成的。 在rocketmq中，NameSrv提供

### 21 设计生产架构之前的功课：消息中间件路由中心的架构原理是什么？

四个核心的部分：
+ 第一块就是他的NameServer，这个东西很重要，他要负责去管理集群里所有Broker的信息，让使用MQ的系统可以通过他感知到集群里有哪些Broker。

+ 第二块就是Broker集群本身了，必须得在多台机器上部署这么一个集群，而且还得用主从架构实现数据多副本存储和高可用。
+ 第三块就是向MQ发送消息的那些系统了，这些系统一般称之为生产者，这里也有很多细节是值得深究的，因为这些生产者到底是如何
    
    从NameServer拉取路由信息的？如何选择Broker机器建立连接以及发送消息的？

+ 第四块就是从MQ获取消息的那些系统，这些系统一般称之为消费者。

#### 关于RocketMQ NameServer设计原理

NameServer 如果要高可用 就要多部署几台,做成集群

每个Broker启动都得向所有的NameServer进行注册

### 系统如何从NameServer获取Broker信息？

每个系统自己每隔一段时间，定时发送请求到NameServer去拉取最新的集群Broker信息  自己主动去NameServer拉取Broker信息 的。

#### 如果Broker挂了，NameServer是怎么感知到的？
这个问题，靠的是Broker跟NameServer之间的心跳机制，Broker会每隔30s给所有的NameServer发送心跳，告诉每个NameServer自己目前还活着。

#### Broker挂了，系统是怎么感知到的？
刚开始集群里有10个Broker，各个系统从NameServer那里得知，都以为有10个Broker。

结果此时突然挂了一个Broker，120s没发心跳给NameServer，NameServer是知道现在只有9个Broker了。

但是此时其他系统是不知道只有9个Broker的，还以为有10个Broker，此时可能某个系统就会发送消息到那个已经挂掉的Broker上去，此时是绝对不可能成功发送消息的

而且过一会儿，系统又会重新从NameServer拉取最新的路由信息了，此时就会知道有一个Broker已经宕机了。

### 22 授人以渔：要是没有这个路由中心，消息中间件可以正常运作么？
假设这个NameServer集群整体都故障了，失去了这个NameServer集群之后：
+ RocketMQ还能正常运行吗？
+ 生产者还能发送消息到Broker吗？
+ 消费者还能从Broker拉取消息吗？

首先启动注册中心nameserver，每个nameserver之间互不通信，

启动broker时，会把自己的信息注册到每一个nameserver，broker每30s发送心跳包给注册中心，注册中心更新broker的最后更新时间。

nameserver会定时10秒检测更新时间是否超过120s，超过则将这个broker路由原信息剔除。

生产者和消费者定时去获取broker的路由信息，根据轮询生产消息和消费消息的负载。

当注册中心挂了，本地还会有缓存信息能够继续通信。

### 23 设计生产架构之前的功课：Broker的主从架构原理是什么？

#### Master Broker是如何将消息同步给Slave Broker的？

为了保证MQ的数据不丢失而且具备一定的高可用性，所以一般都是得将Broker部署成Master-Slave模式的，也就是一个Master Broker对应一个Slave Broker

然后Master需要在接收到消息之后，将数据同步给Slave，这样一旦Master Broker挂了，还有Slave上有一份数据。

说明：
+ Slave Broker也会向所有的NameServer进行注册
+ Slave Broker也会向所有的NameServer每30s发送心跳

RocketMQ的Master-Slave模式采取的是Slave Broker不停的发送请求到Master Broker去拉取消息。

所以首先要明白这一点，就是RocketMQ自身的Master-Slave模式采取的是Pull模式拉取消息

#### RocketMQ 实现读写分离了吗？

有可能从Master Broker获取消息，也有可能从Slave Broker获取消息

作为消费者的系统在获取消息的时候会先发送请求到Master Broker上去，请求获取一批消息，此时Master Broker是会返回一批消息给消费者系统的

然后Master Broker在返回消息给消费者系统的时候，会根据当时Master Broker的负载情况和Slave Broker的同步情况，向消费者系统建议下一次拉取消息的时候是从Master Broker拉取还是从Slave Broker拉取。

要是这个时候Master Broker负载很重，本身要抗10万写并发了，你还要从他这里拉取消息，给他加重负担，那肯定是不合适的。

所以此时Master Broker就会建议你从Slave Broker去拉取消息。

或者举另外一个例子，本身这个时候Master Broker上都已经写入了100万条数据了，结果Slave Broke不知道啥原因，同步的特别慢，才同步了96万条数据，落后了整整4万条消息的同步，这个时候你作为消费者系统可能都获取到96万条数据了，那么下次还是只能从Master Broker去拉取消息。

因为Slave Broker同步太慢了，导致你没法从他那里获取更新的消息了。

所以这一切都会由Master Broker根据情况来决定

#### 如果Slave Broke挂掉了有什么影响？
有一点影响，但是影响不太大

因为消息写入全部是发送到Master Broker的，然后消息获取也可以走Master Broker，只不过有一些消息获取可能是从Slave Broker去走的。

所以如果Slave Broker挂了，那么此时无论消息写入还是消息拉取，还是可以继续从Master Broke去走，对整体运行不影响。

只不过少了Slave Broker，会导致所有读写压力都集中在Master Broker上。

#### 如果Master Broker挂掉了该怎么办？

在RocketMQ 4.5版本之前，都是用Slave Broker同步数据，尽量保证数据不丢失，但是一旦Master故障了，Slave是没法自动切换成Master的。

所以在这种情况下，如果Master Broker宕机了，这时就得手动做一些运维操作，把Slave Broker重新修改一些配置，重启机器给调整为Master Broker，这是有点麻烦的，而且会导致中间一段时间不可用

所以这种Master-Slave模式不是彻底的高可用模式，他没法实现自动把Slave切换为Master

#### 基于Dledger实现RocketMQ高可用自动切换
在RocketMQ 4.5之后，这种情况得到了改变，因为RocketMQ支持了一种新的机制，叫做Dledger

简单来说，把Dledger融入RocketMQ之后，就可以让一个Master Broker对应多个Slave Broker，也就是说一份数据可以有多份副本，比如一个Master Broker对应两个Slave Broker。

然后依然会在Master和Slave之间进行数据同步

此时一旦Master Broker宕机了，就可以在多个副本，也就是多个Slave中，通过Dledger技术和Raft协议算法进行leader选举，直接将一个Slave Broker选举为新的Master Broker，然后这个新的Master Broker就可以对外提供服务了。

整个过程也许只要10秒或者几十秒的时间就可以完成，这样的话，就可以实现Master Broker挂掉之后，自动从多个Slave Broker中选举出来一个新的Master Broker，继续对外服务，一切都是自动的。

### 24 授人以渔：Broker主从同步有没有数据不一致问题？

问题：
+ 假设如果没有RocketMQ 4.5新版本引入的Dledger技术，仅仅是靠之前的Master-Slave主从同步机制，那么在Master崩溃的时
候，可能会造成多长时间的系统不可用？这个时候如何能够尽快的恢复集群运行？依赖手工运维的话，如何能尽快的去完成这个运
维操作？
+ 在RocketMQ 4.5之后引入了Dledger技术可以做到自动选举新的Master，那么在Master崩溃一直到新的Master被选举出来的这
个过程中，你觉得对于使用MQ的系统而言，会处于一个什么样的状态呢？

+ 希望大家去研究一下Kafka和RabbitMQ的多副本和高可用机制，Kafka是如何在集群里维护多个副本的？出现故障的时候能否实
现自动切换？RabbitMQ是如何在集群里维护多个数据副本的？出现故障的时候能否实现自动切换？

+ 既然有主从同步机制，那么有没有主从数据不一致的问题？Slave永远落后Master一些数据，这就是主从不一致。那么这种不一致
有没有什么问题？有办法保证主从数据强制一致吗？这样做又会有什么缺点呢？

可以和kafka一样做一个设置 保证只有消息至少被所有follwer同步成功后才算消息写入成功，这样即使leader挂了，从新选举出的follwer也会拥有全部的消息，只不过消息写入吞吐量会下降，这是肯定的 

所有的消息中间件主从要做的强一致性这里都要牺牲吞吐量，必须等待同步写到从节点，写入不成功就返回异常，具体场景具体考虑吧 

kafka这里可以配置多种模式，还可以直接发送后不等待写入成功就返回，还有一个是等待leader写入成功在返回，

Broker 采用主从架构存在延迟， 必然存在主从同步数据不一致的问题。 

1. producer 生产消息， 存放到 主Broker， 从Broker主动定时拉取消息。
 
2. consumer 拉取消息， 向 主Broker 拉取消息， 主Broker会记录消费相关信息， 然后从Broker再向主Broker同步。
  
3. 保证数据一致， 可以在producer写入消息， consumer 拉取消息后提交commitLog时改成同步， 多台机器成功之后才彻底成功。

###  25 落地第一步：设计一套高可用的消息中间件生产部署架构

#### NameServer集群化部署，保证高可用性
NameServer的设计是采用的Peer-to-Peer的模式来做的，也就是可以集群化部署，但是里面任何一台机器都是独立运行的，跟其他的机器没有任何通信。

每台NameServer实际上都会有完整的集群路由信息，包括所有的Broker节点信息，我们的数据信息，等等。所以只要任何一台NameServer存活下来，就可以保证MQ系统正常运行，不会出现故障。

#### 基于Dledger的Broker主从架构部署

采用RocketMQ 4.5以前的那种普通的Master-Slave架构来部署，能在一定程度上保证数据不丢失，也能保证一定的可用性。

但是那种方式的缺陷是很明显的，最大的问题就是当Master Broker挂了之后，没办法让Slave Broker自动切换为新的MasterBroker，需要手工做一些运维操作，修改配置以及重启机器才行，这个非常麻烦。

所以选择基于Dledger的主备自动切换的功能来进行生产架构的部署。

而且Dledger技术是要求至少得是一个Master带两个Slave，这样有三个Broke组成一个Group，也就是作为一个分组来运行。一旦Master宕机，他就可以从剩余的两个Slave中选举出来一个新的Master对外提供服务。

ps:每个Broker（不论是Master和Slave）都会把自己注册到所有的NameServer上去。


#### Broker是如何跟NameServer进行通信的？
Broker会每隔30秒发送心跳到所有的NameServer上去，然后每个NameServer都会每隔10s检查一次有没有哪个Broker超过120s没发送心跳的，
如果有，就认为那个Broker已经宕机了，从路由信息里要摘除这个Broker。

在RocketMQ的实现中，采用的是TCP长连接进行通信。

也就是说，Broker会跟每个NameServer都建立一个TCP长连接，然后定时通过TCP长连接发送心跳请求过去

#### 使用MQ的系统都要多机器集群部署

很多的系统使用RocketMQ，有些系统是作为生产者往MQ发送消息，有些系统是作为消费者从MQ获取消息，当然还有的系统是既作为生产者，又作为消费者，所以我们要考虑这些系统的部署。

对于这些系统的部署本身不应该在MQ的考虑范围内，但是我们还是应该给出一个建议，就是无论作为生产者还是消费者的系统，都应该多机器集群化部署，保证他自己本身作为生产者或者消费者的高可用性。

#### MQ的核心数据模型：Topic到底是什么？
Topic其实就是一个数据集合的意思，不同类型的数据你得放不同的Topic里去。

要是你有一些商品数据要发送消息到MQ里，你就应该创建一个Topic叫做“topic_product_info”，代表里面都是商品数据，那些想

要从MQ里获取商品数据的系统就可以从“topic_product_info”里获取了。

所以简单来说，你的系统如果要往MQ里写入消息或者获取消息，首先得创建一些Topic，作为数据集合存放不同类型的消息，比如说订单Topic，商品Topic，等等。

#### Topic作为一个数据集合是怎么在Broker集群里存储的？

首先我们来想一下，比如我们有一个订单Topic，可能订单系统每天都会往里面投递几百万条数据，然后这些数据在MQ集群上还得保留好多天，那么最终可能会有几千万的数据量，这还只是一个Topic。

那么如果有很多的Topic，并且里面都有大量的数据，最终加起来的总和也许是一个惊人的数字，此时这么大量的数据本身是不太可能存放在一台机器上的。


分布式存储。

我们可以在创建Topic的时候指定让他里面的数据分散存储在多台Broker机器上，比如一个Topic里有1000万条数据，此时有2台Broker，那么就可以让每台Broker上都放500万条数据。

这样就可以把一个Topic代表的数据集合分布式存储在多台机器上了

另外很重要的一件事是，每个Broke在进行定时的心跳汇报给NameServer的时候，都会告诉NameServer自己当前的数据情况，

比如有哪些Topic的哪些数据在自己这里，这些信息都是属于路由信息的一部分。

#### 生产者系统是如何将消息发送给Broker的？

+ 在发送消息之前，得先有一个Topic，然后在发送消息的时候你得指定你要发送到哪个Topic里面去。

+ 接着既然你知道你要发送的Topic，那么就可以跟NameServer建立一个TCP长连接，然后定时从他那里拉取到最新的路由信息，包括:

    集群里有哪些Broker，集群里有哪些Topic，每个Topic都存储在哪些Broker上

+ 然后生产者系统自然就可以通过路由信息找到自己要投递消息的Topic分布在哪几台Broker上，此时可以根据负载均衡算法，从里面选择一台Broke机器出来，比如round robine轮询算法，或者是hash算法，都可以。

+ 总之，选择一台Broker之后，就可以跟那个Broker也建立一个TCP长连接，然后通过长连接向Broker发送消息即可.Broker收到消息之后就会存储在自己本地磁盘里去

这里唯一要注意的一点，就是生产者一定是投递消息到Master Broker的，然后Master Broker会同步数据给他的Slave Brokers，实现
一份数据多份副本，保证Master故障的时候数据不丢失，而且可以自动把Slave切换为Master提供服务。


#### 消费者是如何从Broker上拉取消息的？

消费者系统其实跟生产者系统原理是类似的，他们也会跟NameServer建立长连接，然后拉取路由信息，接着找到自己要获取消息的Topic在哪几台Broker上，就可以跟Broker建立长连接，从里面拉取消息了。

#### 整体架构：高可用、高并发、海量消息、可伸缩

整个这套生产架构是实现完全高可用的，因为NameServer随便一台机器挂了都不怕，他是集群化部署的，每台机器都有完整的路由信息；

Broker随便挂了一台机器也不怕，挂了Slave对集群没太大影响，挂了Master也会基于Dledger技术实现自动Slave切换为Master；

生产者系统和消费者系统随便挂了一台都不怕，因为他们都是集群化部署的，其他机器会接管工作。

而且这个架构可以抗下高并发，因为假设订单系统对订单Topic要发起每秒10万QPS的写入，那么只要订单Topic分散在比如5台Broker上，实际上每个Broker会承载2万QPS写入，也就是说高并发场景下的10万QPS可以分散到多台Broker上抗下来。

然后集群足以存储海量消息，因为所有数据都是分布式存储的，每个Topic的数据都是存储在多台Broker机器上的，用集群里多台Master Broker就足以存储海量的消息。

所以，用多个Master Broker部署的方式，加上Topic分散在多台Broker上的机制，可以抗下高并发访问以及海量消息的分布式存储。

然后每个Master Broker有两个Slave Broker结合Dledger技术可以实现故障时的自动Slave-Master切换，实现高可用性。

最后，这套架构还具备伸缩性，就是说如果要抗更高的并发，存储跟多的数据，完全可以在集群里加入更多的Broker机器，这样就可以线性扩展集群了。

### 27 部署一个小规模的 RocketMQ 集群，为压测做好准备

<a  href="/src/main/resources/note/中间件/rocket配置.md"> Rocket 配置  </a>

### 28 授人以渔：动手完成一个小规模的RocketMQ集群的部署进行练习


### 29 生产运维：如何对RocketMQ集群进行可视化的监控和管理？

#### RocketMQ的大优势：可视化的管理界面
整个RocketMQ集群的元数据都集中在了NameServer里，包括有多少Broker，有哪些Topic，有哪些Producer，有哪些Consumer，目前集群里有多少消息，等等。

是RocketMQ里既然有大量的信息可以让我们进行监控和查看，他自然会提供一些办法来让我们看到，这就是他最大的优势之一，一个可视化的管理界面。

我们可以随便找一台机器，用NameServer的三台机器中的任意一台机器就可以，在里面执行如下命令拉取RocketMQ运维工作台的源码：
>git clone https://github.com/apache/rocketmq-externals.git

然后进入rocketmq-console的目录：
>cd rocketmq-externals/rocketmq-console

执行以下命令对rocketmq-cosole进行打包，把他做成一个jar包：
>mvn package -DskipTests

然后进入target目录下，可以看到一个jar包，接着执行下面的命令启动工作台：
>java -jar rocketmq-console-ng-1.0.1.jar --server.port=8080 --rocketmq.config.namesrvAddr=127.0.0.1:9876

这里务必要在启动的时候设置好NameServer的地址，如果有多个地址可以用分号隔开，接着就会看到工作台启动了，然后就通过浏览器访问那台机器的8080端口就可以了，就可以看到精美的工作台界面。

#### 如何通过工作台进行集群监控

你可以看到各个Broker的分组，哪些是Master，哪些是Slave，他们各自的机器地址和端口号，还有版本号

包括最重要的，就是他们每台机器的生产消息TPS和消费消息TPS，还有消息总数。

这是非常重要的，通过这个TPS统计，就是每秒写入或者被消费的消息数量，就可以看出RocketMQ集群的TPS和并发访问量。

#### 机器本身的监控应该如何做？
有了这个东西，我们是可以在压测的时候看到整个RocketMQ的TPS了，也就是Transaction PerSecond，就是每秒事务的意思，在这里就是每秒消息数量的意思。

但是我们要同时看到集群每台机器的CPU、IO、磁盘、内存、JVM GC的负载和情况怎么办呢？

其实这些东西都有很好的监控系统可以去看了，比如说Zabbix、Open-Falcon等等，一般公司都会用这些东西来监控机器的性能和资源使用率。

### 30 授人以渔：你们公司的MQ集群是如何进行监控和管理的？

假设

核心链路用的是rabbitmq，使用的监控是其自带的可视化控制面板rabbitmq_management， 运维的同事平时主要看概览，包括集群内节点状态（观察集群内broker状态），以及相关messag rates，消费监听者的连接进程(消费系统当前是否正常)，以及消息队列中的消息处理情况（比如失败的，是否存在消息堆积等， 总得来说，他们主要需求是监控Rabbit内部状态、确认RabbitMQ可用并且能够响应、观察队列状态检测消费者异常、检测消息通信结构中不合需求的配置更改等 如果我来负责 我除了上述的一些指标外，我还会关心，当前各个队列的准备完成的数据有多少，没有被ack的有多少，另外我会用到admin 管理，方便管理帐户信息及权限管理（非常方便），管理vhost(虚拟主机起到消息的逻辑区分)等， 此外我还需要部署Nagios：监控系统或服务状态异常时发出邮件或短信报警第一时间通知我，在状态恢复后发出正常的邮件或短信通知


### 31 RocketMQ生产集群准备：进行OS内核参数和JVM参数的调整

#### 对RocketMQ集群进行OS内核参数的调整
1. vm.overcommit_memory
    “vm.overcommit_memory”这个参数有三个值可以选择，0、1、2。
    
    如果值是0的话，在你的中间件系统申请内存的时候，os内核会检查可用内存是否足够，如果足够的话就分配内存给你，如果感觉剩余
    内存不是太够了，干脆就拒绝你的申请，导致你申请内存失败，进而导致中间件系统异常出错。
    
    因此一般需要将这个参数的值调整为1，意思是把所有可用的物理内存都允许分配给你，只要有内存就给你来用，这样可以避免申请内
    存失败的问题。
    
    比如我们曾经线上环境部署的Redis就因为这个参数是0，导致在save数据快照到磁盘文件的时候，需要申请大内存的时候被拒绝了，进
    而导致了异常报错。
    
    可以用如下命令修改：
    >echo 'vm.overcommit_memory=1' >> /etc/sysctl.conf。

2. vm.max_map_count
    这个参数的值会影响中间件系统可以开启的线程的数量，同样也是非常重要的
    
    如果这个参数过小，有的时候可能会导致有些中间件无法开启足够的线程，进而导致报错，甚至中间件系统挂掉。
    
    他的默认值是65536，但是这个值有时候是不够的，比如我们大数据团队的生产环境部署的Kafka集群曾经有一次就报出过这个异常，说无法开启足够多的线程，直接导致Kafka宕机了。
    
    因此建议可以把这个参数调大10倍，比如655360这样的值，保证中间件可以开启足够多的线程。
    
    可以用如下命令修改：
    >echo 'vm.max_map_count=655360' >> /etc/sysctl.conf。

3. vm.swappiness

    这个参数是用来控制进程的swap行为的，这个简单来说就是os会把一部分磁盘空间作为swap区域，然后如果有的进程现在可能不是太
    活跃，就会被操作系统把进程调整为睡眠状态，把进程中的数据放入磁盘上的swap区域，然后让这个进程把原来占用的内存空间腾出
    来，交给其他活跃运行的进程来使用。
    
    如果这个参数的值设置为0，意思就是尽量别把任何一个进程放到磁盘swap区域去，尽量大家都用物理内存。
    
    如果这个参数的值是100，那么意思就是尽量把一些进程给放到磁盘swap区域去，内存腾出来给活跃的进程使用。
    
    默认这个参数的值是60，有点偏高了，可能会导致我们的中间件运行不活跃的时候被迫腾出内存空间然后放磁盘swap区域去。
    
    因此通常在生产环境建议把这个参数调整小一些，比如设置为10，尽量用物理内存，别放磁盘swap区域去。
    
    可以用如下命令修改：
    >echo 'vm.swappiness=10' >> /etc/sysctl.conf。

4. ulimit

    这个是用来控制linux上的最大文件链接数的，默认值可能是1024，一般肯定是不够的，因为你在大量频繁的读写磁盘文件的时候，或
    者是进行网络通信的时候，都会跟这个参数有关系
    
    对于一个中间件系统而言肯定是不能使用默认值的，如果你采用默认值，很可能在线上会出现如下错误：error: too many open files。
    
    因此通常建议用如下命令修改这个值：
    > echo 'ulimit -n 1000000' >> /etc/profile。


+ 中间件系统肯定要开启大量的线程（跟vm.max_map_count有关）
+ 而且要进行大量的网络通信和磁盘IO（跟ulimit有关）
+ 然后大量的使用内存（跟vm.swappiness和vm.overcommit_memory有关）

#### 对JVM参数进行调整

<a  href="/src/main/resources/note/中间件/borkerjvm配置.md"> 对JVM参数进行调整  </a>

#### 对RocketMQ核心参数进行调整
dledger的示例配置文件：rocketmq/distribution/target/apacherocketmq/conf/dledger

在这里主要是有一个较为核心的参数：sendMessageThreadPoolNums=16

这个参数的意思就是RocketMQ内部用来发送消息的线程池的线程数量，默认是16

其实这个参数可以根据你的机器的CPU核数进行适当增加，比如机器CPU是24核的，可以增加这个线程数量到24或者30，都是可以的。


### 32 授人以渔：你们公司的MQ集群是如何配置生产参数的？
> 无 

Kafka，因为Kafka设计之初的原则就是尽量少的依赖JVM，比如他的两个比较重要的特性：磁盘顺序写和零拷贝，并且大量使用OS的cache，所以，在给JVM分配内存的时候，只要分配足够运行的内存就行了，剩下的大部分内存都要留出来给OS cache，用来提高Kafka的写和读的并发。其他针对网络通信（线程数量貌似也没有那么大的需求）以及磁盘IO（ulimit）的配置大体相同，主要是尽量开大Linux的阈值，而不至于因为这些参数限制了Kafka的吞吐和运行效率。补充一个相关的知识，因为用的是磁盘顺序写，所以Kafka的机器只要HDD就可以了，不需要机械硬盘SSD。


1、os的内核参数，在分配内存分配策略为1，内核允许分配所有的物理内存 2、可以开启的最大线程数为默认3、进程的内存交换 设置为默认，服务器只部署了mq，默认的没什么问题； 4、文件的最大连接数默认值为1024 jvm参数：jvm配置的一团糟，注册中心与broker是部署在一起，分配的jvm参数都是2g，年轻代为1，因为为4核8g内存，还需要留内存给os； 中间件的配置参数：压测环境过程中，运维把sendMessaageThreadPooNums 设置为256；

###  33 对小规模RocketMQ集群进行压测，同时为生产集群进行规划

> 压测:在RocketMQ的TPS和机器的资源使用率和负载之间取得一个平衡。

#### 一次RocketMQ小规模集群的压测

 <a  href="/src/main/resources/note/中间件/rocket压测.md"> 一次RocketMQ小规模集群的压测  </a>
 
### 34 授人以渔：你们公司的MQ集群做过压测吗？生产集群是如何规划的？

+ 他们对MQ集群做过压测吗？
+ 使用什么样的机器配置做的压测？
+ 使用多大规模的集群做的压测？如何压测的？
+ 在压测的过程中发现单Broker的TPS最高有多少？
+ 在压测过程中，cpu负载、内存使用率、jvm gc频率、磁盘io负载、网卡流量负载，这些值都是如何变化的？
+ 在压测过后，是如何规划生产集群的？
+ 目前公司线上MQ集群的TPS多高？机器资源的负载情况如何？能否抗住？ 

### 35 阶段性复习：一张思维导图给你梳理消息中间件集群生产部署架构规划

<img src="https://s1.ax1x.com/2020/07/22/UbszE4.jpg" alt="UbszE4.jpg" border="0" />

### 36 阶段性复习：按照你们公司的真实负载，设计消息中间件集群生产架构
+ 你们系统有没有使用MQ技术的业务场景？
+ 你们公司是如何进行技术选型的？
+ 你能对RocketMQ、Kafka、RabbitMQ三种技术的架构原理都进行一个思考和横向对比吗？
+ 如果RocketMQ没有路由中心了能正常运转吗？
+ 主从同步是否有数据一致性问题？
+ 你们公司的MQ集群是采用什么样的部署架构？
+ 你有没有动手完成一个小规模RocketMQ集群的部署？
+ 你们公司都是如何对MQ集群进行可视化监控的？
+ 你们公司的MQ集群是如何调整生产参数的？公司的MQ集群做过压测吗？你们公司的MQ集群生产环境是如何部署的？

rocketmq，主要进行异步，削峰，分布式事务一致性。

kafka吞吐量高，但功能简单不适合我们电商业务，适合日志采集不需要业务操作。

rocketmq公司有对源码进行二次开发封装客户端，增加发送失败的系统自动补偿机制。

没有了路由中心能运行一段时间，新的broker，生产者，消费者不能注册，还有缓存信息。

主从同步双写不会出现数据丢失。

我们公司broker采用多主零从同步刷盘，nameserver集群部署，生产者，消费者集群部署


### 37 基于MQ实现订单系统的核心流程异步化改造，性能优化完成！

#### 改造订单系统

1. 下单核心流程环节太多，性能较差
2. 订单退款的流程可能面临退款失败的风险
3. 关闭过期订单的时候，存在扫描大量订单数据的问题
4. 跟第三方物流系统耦合在一起，性能存在抖动的风险
5. 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
6. 做秒杀活动时订单数据库压力过大

在用户支付完毕后，只要执行最核心的更新订单状态以及扣减库存就可以了，保证速度足够快。

然后诸如增加积分、发送优惠券、发送短信、通知发货的操作，都可以通过MQ实现异步化执行。

一旦你支付成功，实际上订单系统只需要更新订单状态（30ms）+扣减库存（80ms）+发送订单消息到
RocketMQ（10ms），一共120ms就可以了

#### 在订单系统中如何发送消息到RocketMQ？

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md" >官网的demo 看生产者</a>

#### 其他系统改造为从RocketMQ中获取订单消息

<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md" >官网的demo 看消费者</a>

### 38 授人以渔：如果在你们系统的核心流程引入MQ，应该如何改造系统？
采用大量mq进行不同系统的交互，比如一个消息，多方都要订阅。自己做的一个项目，发货系统的改造，由第三方同步请求我们，把参数检验和更新数据库，通知其他部门发货信息全部解藕，提高了对外接口的响应，通过mq的重试消费加告警保证本地事务的执行成功。推送给第三方消息。

数据产品的系统，每次操作都需要记录审计信息。这类信息不是必须保证不丢失的，而且时效性要求没有那么高，所以，我们在切面拦截到，并把信息发送到MQ中，异步处理；还有一类是比较关键的数据（不允许丢，时效性要求不高），首先一次请求，可能会在多个分布式系统差生这类数据，而且我们需要在这些数据都保存到数据库后，再从数据库中另外一个表中查出来（前面生成的数据都有触发器）发送到搜索平台。上面这种就比较复杂，我们本来就使用了分布式事务，但是为了加快性能，把上传搜索平台的消息做成了异步，但是因为是一个分布式事务，可能会发生操作失败回滚，但是消息已经发出去了，没办法取消，从而产生脏数据的问题，针对上述问题，我们采用了分布式事务+RocketMQ的事务消息的机制解决的（实际上用的是阿里云的GTS+RocketMQ事务）

### 39 基于MQ实现订单系统的第三方系统异步对接改造，解耦架构完成！

上面那6个问题,如果用mq 事实上1,4 都解决了


<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md" >官网的demo </a>

#### 什么叫做同步发送消息到RocketMQ？
 > 普通的send 就是同步发送
 
#### 什么叫做异步发送消息到RocketMQ？
<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#2%E5%8F%91%E9%80%81%E5%BC%82%E6%AD%A5%E6%B6%88%E6%81%AF" >官网的demo  异步发送</a>

主要代码片段
````
         // 启动Producer实例
          producer.start();
          producer.setRetryTimesWhenSendAsyncFailed(0);
          int messageCount = 100;
                  // 根据消息数量实例化倒计时计算器
          	final CountDownLatch2 countDownLatch = new CountDownLatch2(messageCount);
              	for (int i = 0; i < messageCount; i++) {
                          final int index = i;
                      	// 创建消息，并指定Topic，Tag和消息体
                          Message msg = new Message("TopicTest",
                              "TagA",
                              "OrderID188",
                              "Hello world".getBytes(RemotingHelper.DEFAULT_CHARSET));
                          // SendCallback接收异步返回结果的回调
                          producer.send(msg, new SendCallback() {
                              @Override
                              public void onSuccess(SendResult sendResult) {
                                   // 成功
                              }
                              @Override
                              public void onException(Throwable e) {
                	             // 异常
                              }
                      	});
              	}
```` 
 #### 什么叫做发送单向消息到RocketMQ？
 这种方式主要用在不特别关心发送结果的场景，例如日志发送。
 
  <a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#3%E5%8D%95%E5%90%91%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF" >官网的demo  单向消息</a>
  
#### 消费模式

rocket 两种消费模式

+ Push消费模式


<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#2%E6%B6%88%E8%B4%B9%E8%80%85%E6%A0%B7%E4%BE%8B" >官网的demo  Push消费模式</a>

注册后,就是Broker会主动把消息发送给你的消费者，你的消费者是被动的接收Broker推送给过来的消息，然后进行处理
````
// 实例化消费者
  DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name");
  ````
 
 + 做Pull消费模式
 
 
<a href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#82-omspullconsumer " >官网的demo 做Pull消费模式</a>

Broker不会主动推送消息给Consumer，而是消费者主动发送请求到Broker去拉取消息过来。


### 40 授人以渔：如果你们系统要对接第三方系统，应该如何设计？

mq对比:

kafka发送的三种模式：1.发送并忘记(单向) 2.同步发送 3.异步发送+回调函数，和rocket mq一样。 

kafa消费采用pull模式。 

rabbit mq confirm发送方确认模式：普通Confirm模式、批量确认模式、异步监听发送方确认模式。 

rabbit mq消费支持pull和push两种模式

kafka 消费是pull模式 客户端主动去服务端拉取消息 这样好处是可以更灵活的控制消费速度，要是服务端主动推送的话可能每个消费者消费速率不一样导致消费者消费不过来，kafka发送也有同步发送 异步发送带callback回调接口的，也可以发送后不关注结果配合acks=0 这样发送速率最大 但也是消息最不可靠的一种方式 大数据日志采集很适合 追求吞吐量

集群总结

1，zookeeper集群： 角色：leader,follower,observer 解读：只能写leader，可以从leader，follower和observer中读取数据，但observer没有投票权 
 
2，kafka集群： 角色：leader,follower 解读：只能读写leader，不可以从follower中读取数据，依靠zookeeper选举

3，rocketmq集群： 角色：master,slave 解读：只能写master，可以从master和slave中读取数据
   
4，redis集群： 角色：master,slave 解读：只能写master，可以从master和slave中读取数据

### 41 基于MQ实现订单数据同步给大数据团队，应该如何设计？

#### 大数据团队的几百行大SQL是如何影响订单数据库的？

实际上要解决这个问题，就必须要避免大数据团队直接查询订单数据库

比如最简单的办法，就是将订单数据落地到大数据团队自己的一个MySQL数据库中，然后从自己的MySQL数据库里统计报表。

还有MySQL Binlog同步系统

这种系统会监听MySQL数据库的Binlog，所谓Binlog大致可以理解为MySQL的增删改操作日志。

然后MySQL Binlog同步系统会将监听到的MySQL Binlog（也就是增删改操作日志）发送给你的系统，让你来处理这些增删改操作日志。

实际上大数据团队并没有必要仅仅只通过MySQL来出数据报表，完全可以采用Hadoop、Spark、Flink等大数据技术来出数据报表。

<img src="https://s1.ax1x.com/2020/07/22/UbTSOJ.jpg" alt="UbTSOJ.jpg" border="0" />

方案1: 刚开始的方案设计是通过job的方式拉去业务方的数据库来插入数据或更新es的数据存在时效性问题和业务数据多了厚性能问题，

方案2: 后来改造为canal监控binlog日志再通过canal adpater适配器写数据到rocketmq，消费者在消费消息进行业务插入后插入到es中

### 42 授人以渔：对其他团队要获取你们核心数据的问题，应该如何解决？

方案3:  mysql 主从, 主给业务系统用, 从给其他系统用

### 43 秒杀系统的技术难点以及秒杀商品详情页系统的架构设计

下问题清单：
1. 下单核心流程环节太多，性能较差
1. 订单退款的流程可能面临退款失败的风险
1. 关闭过期订单的时候，存在扫描大量订单数据的问题
1. 跟第三方系统耦合在一起，性能存在抖动的风险
1. 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
1. 做秒杀活动时订单数据库压力过大

1,4,5 都解决了


#### 秒杀活动压力过大怎么办？难道是加机器吗？
+ 第一个问题，秒杀活动目前压力过大，应该如何解决？是不是简单的堆机器或者加机器就可以解决的？
    
    这个是没问题的，订单系统自己是可以通过部署更多的机器进行线性扩展的。
+ 第二个问题来了，那么数据库呢？是不是也要部署更多的服务器，进行分库分表，然后让更多的数据库服务器来抗超高的数据库高并发访问？
    
    这个思路是这样的，所谓分库分表，就是把目前的一台数据库服务器变成多台数据库服务器，然后把一张订单表变成多张订单表。
    
答案是不太靠谱的，除非是技术能力比较弱的公司，没有厉害的架构师去利用已有的技术合理设计优秀的架构，才会用这种堆机器的方
法简单的来抗下超高的并发。

因为如果用堆机器的方法来解决这个问题，必然存在一个问题，就是随着你的用户量越来越大，你的并发请求越来越多，会导致你要不
停的增加更多的机器

#### 不归订单管的部分：高并发的商品详情页请求
其实秒杀活动主要涉及到的并发压力就是两块，一个是高并发的读，一个是高并发的写。

在秒杀的时候,必然出现一种场景，就是首先大量用户会拿着APP不停的刷新一个秒杀商品的页面

本质上来说是从商品技术团队负责的商品详情页系统中加载出来的

在秒杀活动的时候，他面临的第一个问题就是，可能几十万人，甚至百万级的用户，会同一时间频繁的访问同一个秒杀商品的页面
比如“3折抢购原价6888的手机，限售100台”这样的活动，可能有几十万人在8:30之前会集中访问这个秒杀商品的活动页面，对商品
详情页系统造成过巨大的访问压力。

#### 商品团队的秒杀架构优化：页面数据静态化

实际上商品技术团队针对这个问题，采取的是页面数据静态化+多级缓存的方案。

以首先需要将这个秒杀活动的商品详情页里的数据做成静态化的，也就是说提前就从数据库里把这个页面需要的数据都提取出来组装
成一份静态数据放在别的地方，避免每次访问这个页面都要访问后端数据库。

#### 商品团队的秒杀架构优化：多级缓存

多级缓存的架构，我们会使用CDN + Nginx + Redis的多级缓存架构

可以将一些静态化好的数据放在就近的一个CDN上。

这个CDN缓存就是我们多级缓存架构里的第一级缓存。

那如果因为缓存过期之类的问题，CDN上没有用户要加载的商品详情页数据怎么办呢？

此时用户就会发送请求到我们公司的机房里的机器上去请求加载这个商品的数据了，这个时候我们需要在Nginx这样的服务器里做一级
缓存。

在Nginx中是可以基于Lua脚本实现本地缓存的，我们可以提前把秒杀商品详情页的数据放到Nginx中进行缓存，如果请求发送过来，
可以从Nginx中直接加载缓存数据，不需要把请求转发到我们商品系统上去

Nginx上的缓存数据过期之类的问题，导致没找到我们需要的数据?

此时就可以由Nginx中的Lua脚本发送请求到Redis集群中去加载我们提前放进去的秒杀商品数据

如果在Redis中还是没有找到呢？

那么就由Nginx中的Lua脚本直接把请求转发到商品详情页系统里去加载就可以了，此时就会直接从数据库中加载数据出来

### 44 授人以渔：你们有没有类似秒杀的业务场景？如果没有，自己想一个出来！

+ 秒杀的场景，应该采取差不多的方案，cdn+nginx缓存+redis缓存商详的静态页面化。后台可以通过redis扣减库存，mq更新数据操作。

+ 二级缓存架构， Redis缓存+Eache本地缓存，库存这些数据也是直接提前从数据预热到Redis缓存中，基于Redis做库存扣减，秒杀页面是动态，通过js使用ajax分模块加载不同的商品信息缓存，后台有商品基本数据介绍变更直接发送到MQ，然后秒杀服务监听MQ去缓存这些数据到Redis和本地缓存中。后期数量大了可以按照文章说的三级缓存架构+商品页面通过Nginx静态化生成

### 45 基于MQ实现秒杀订单系统的异步化架构以及精准扣减库存的技术方案

> 访问的问题 解决了,还有下单的问题

#### 用答题的方法避免作弊抢购以及延缓下单
考虑第一个问题，有没有可能会有人自己写一个抢购的脚本或者作弊软件，疯狂的发送请求去抢商品

答案是肯定的，肯定是有人会写作弊的脚本或者软件

所以一般来说，现在你要参与抢购，都会让你点击按钮之后先进行答题，就是说先弹出来一个框，让你回答一个问题，回答正确了你才
能发起抢购的请求

#### 为秒杀独立出来一套订单系统
用户的下单抢购的请求发送出去之后，会达到我们的后台系统，对于后台系统而言，我们需要思考一下，是否直接使用我们目前已
有的订单系统去抗所有的请求？

答案是否定的，这么做会有问题。

假设你有100万用户在这个时间段很活跃都会来购买商品，但是可能只有其中50万用户在参与秒杀活动，同一时间发送了大量的抢购请
求到后台系统，但是同时还有很多其他的用户这个时候并不在参与秒杀系统，他们在进行其他商品的常规性浏览和下单。

因此这个时候如果你让秒杀下单请求和普通下单请求都由一套订单系统来承载，那么可能会导致秒杀下单请求耗尽了订单系统的资源，
或者导致系统不稳定，然后导致其他普通下单请求也出现问题，没有办法完成的下单。

所以一般我们会对订单系统部署两个集群，一个集群是秒杀订单系统集群，一个集群是普通订单系统集群

#### 基于Redis实现下单时精准扣减库存

首先需要做的一个事情，就是扣减库存。

因为大家都知道，秒杀商品一般是有数量的限制的，比如几十万人可能就抢购1万个特价商品。

所以当大量的请求到达后台系统之后，首先第一步，就可以先去扣减库存。

扣减库存应该怎么来扣呢？如果还是直接由订单系统调用库存系统的接口，然后访问库存数据库去扣减，那么势必导致瞬时压力过大，可能让库存系统的压力很大。

因此在秒杀场景下，一般会采用另外一个思路。

通常在秒杀场景下，一般会将每个秒杀商品的库存提前写入Redis中，然后当请求到来之后，就直接对Redis中的库存进行扣减

Redis是可以轻松用单机抗每秒几万高并发的，因此这里就可以抗下高并发的库存扣减

#### 抢购完毕之后提前过滤无效请求
其实在Redis中的库存被扣减完之后，就说明后续其他的请求都没有必要发送到秒杀系统中了，因为商品已经被抢购完毕了

此时我们可以让Nginx在接收到后续请求的时候，直接就把后续请求过滤掉。

比如一旦商品抢购完毕，可以在ZooKeeper中写入一个秒杀完毕的标志位，然后ZK会反向通知Nginx中我们自己写的Lua脚本，通过

Lua脚本后续在请求过来的时候直接过滤掉，不要向后转发了。

#### 瞬时高并发下单请求进入RocketMQ进行削峰

哪怕是有1万件商品同时被1万人秒杀成功了，那么可能瞬间会有1万请求涌入正常的订单系统进行后续的处理，此时可能还是会有瞬间上万请求访问到订单数据库中创建订单。

所以这个时候，完全可以引入RocketMQ进行削峰处理

对于秒杀系统而言，如果判断发现通过Redis完成了库存扣减，此时库存还大于0，就说明秒杀成功了需要生成订单，此时就
直接发送一个消息到RocketMQ中即可。

然后让普通订单系统从RocketMQ中消费秒杀成功的消息进行常规性的流程处理即可，比如创建订单，等等。

这样的话，瞬间上万并发的压力会被RocketMQ轻松抗下来，然后普通的订单系统可以根据自己的工作负载慢慢的从RocketMQ中拉取

秒杀成功的消息，然后进行后续操作就可以了，不会对订单数据库造成过大的压力。

否则如果你让瞬间产生的一万或者几万的订单请求直接访问订单数据库，必然还是会让他压力过大，需要额外增加机器，那是没有必要
的。

因此在这里利用RocketMQ抗下每秒几万并发的下单请求，然后让订单系统以每秒几千的速率慢慢处理就可以了，也就是延迟个可能几
十秒，这些下单请求就会处理完毕。

#### 秒杀架构的核心要点
较重要的有以下几点：
1. 在前端/客户端设置秒杀答题，错开大量人下单的时间，阻止作弊器刷单
2. 独立出来一套秒杀系统，专门负责处理秒杀请求
3. 优先基于Redis进行高并发的库存扣减，一旦库存扣完则秒杀结束
4. 秒杀结束之后，Nginx层过滤掉无效的请求，大幅度削减转发到后端的流量
5. 瞬时生成的大量下单请求直接进入RocketMQ进行削峰，订单系统慢慢拉取消息完成下单操作

首先必须要避免直接基于数据库进行高并发的库存扣减

后续占据99%的请求，都可以直接在Nginx层面被拦截掉，不会转发到后台系统造成任何压力

用入RocketMQ中进行削峰，让RocketMQ轻松抗下高并发压力，让订单系统慢慢消费和处理下单操作

架构优化的核心就是独立出来一套系统专门处理，避免高并发请求落在MySQL上

因为MySQL天生不擅长抗高并发，我们需要通过Redis、Nginx、RocketMQ这些天生轻松可以单机抗几万甚至十万并发的系统来优化架构。


### 46 授人以渔：如果你们有类似秒杀的瞬时高并发场景，应该如何改造？
> mq 削峰, redis 先处理数据,异步同步到数据库

### 47 阶段性复习：一张思维导图给你梳理全面引入MQ的订单系统架构

<img src="https://s1.ax1x.com/2020/07/23/UbXRx0.jpg" alt="UbXRx0.jpg" border="0" />

### 48 阶段性复习：思考一下，如果你们系统全面接入MQ，架构该如何设计？

问题：

1. 你们的系统中是否存在核心链路环节过多导致性能较差的问题？如果有的话，是否可以引入MQ进行适当异步化提升链路性能？

1. 你们的系统是否存在核心链路耦合了第三方系统，进而导致链路性能不稳定的问题？如果有，是否可以引入MQ进行第三方系统的解耦，避免核心链路的性能受到影响？
 
1. 你们的系统是否存在有其他团队直接耦合访问你们数据库的情况，进而导致你们的数据库性能不稳定？如果有的话，是否可以引入MQ来推送你们的核心数据出去，跟其他团队进行解耦？
 
1. 你们的系统是否存在瞬时超高并发的场景？如果有的话，是否可以引入MQ来进行瞬时流量削峰，避免为了应对瞬时超高并发从而不停的增加机器？


### 49 精益求精：深入研究一下生产者到底如何发送消息的？
问题清单：
1. 下单核心流程环节太多，性能较差
1. 订单退款的流程可能面临退款失败的风险
1. 关闭过期订单的时候，存在扫描大量订单数据的问题
1. 跟第三方系统耦合在一起，性能存在抖动的风险
1. 大数据团队要获取订单数据，存在不规范直接查询订单数据库的问题
1. 做秒杀活动时订单数据库压力过大

1,4,5,6 解决

#### 研究RocketMQ底层原理的顺序和思路

思路:
1. 对生产者往Broker集群发送消息的底层原理做一个研究
2. 看看Broker对于接收到的消息，到底是如何存储到磁盘上去的？
3. 基于DLedger技术部署的Broker高可用集群，到底如何进行数据同步的？
4. 消费者到底是基于什么策略选择Master或Slave拉取数据的？
5. 消费者是如何从Broker拉取消息回来，进行处理以及ACK的？如果消费者故障了会如何处理？

#### 创建Topic的时候为何要指定MessageQueue数量？
在创建Topic的时候需要指定一个很关键的参数，就是MessageQueue

就是你要指定你的这个Topic对应了多少个队列，也就是多少个MessageQueue。


#### Topic、MessageQueue以及Broker之间到底是什么关系？

MessageQueue就是RocketMQ中非常关键的一个数据分片机制，他通过MessageQueue将一个Topic的数据拆分为了很多个数据分片，然后在每个Broker机器上都存储一些MessageQueue。

通过这个方法，就可以实现Topic数据的分布式存储！

#### 生产者发送消息的时候写入哪个MessageQueue？

生产者会跟NameServer进行通信获取Topic的路由数据。

所以生产者从NameServer中就会知道，一个Topic有几个MessageQueue，哪些MessageQueue在哪台Broker机器上，哪些MesssageQueue在另外一台Broker机器上，这些都会知道

生产者会根据策略的把消息写入各个MessageQueue

可以让一个Topic中的数据分散在多个MessageQueue中，进而分散在多个Broker机器上. 这样就可以实现RocketMQ集群分布式存储海量的消息数据了！

#### 如果某个Broker出现故障该怎么办？
如果某个Broker临时出现故障了，比如Master Broker挂了，此时正在等待的其他Slave Broker自动热切换为Master Broker，那么这个时候对这一组Broker就没有Master Broker可以写入了

Producer中开启一个开关，就是sendLatencyFaultEnable

一旦打开了这个开关，那么他会有一个自动容错机制，比如如果某次访问一个Broker发现网络延迟有500ms，然后还无法访问，那么
就会自动回避访问这个Broker一段时间，比如接下来3000ms内，就不会访问这个Broker了。

这样的话，就可以避免一个Broker故障之后，短时间内生产者频繁的发送消息到这个故障的Broker上去，出现较多次数的异常。而是在
一个Broker故障之后，自动回避一段时间不要访问这个Broker，过段时间再去访问他。

### 50 授人以渔：Kafka、RabbitMQ有类似MessageQueue的数据分片机制吗

总结：kafka中分片是topic下分为多个partition，partition分为primary shard和副本等同于rocketmq master broker的messagequeue和slave，当发送消息时通过zk上的协调节点和zk存储topic消息对应partition来知道发送消息对应的partition在那个节点上

rabbitMQ没有，因为它仅仅是一个主从的系统，有两种模式，普通集群模式和镜像集群模式。在普通集群模式场景下，消息的元数据会存在于每个rabbitMQ的broker上，但是消息的本体只存在于一个broker上，这种情况有单点问题；而镜像集群模式是，每条消息在每个broker上都有，即每个broker有全量的消息，这种情况虽然没有单点问题，但是不能水平扩展。

但 RabbitMQ 官方有一个 sharding 的插件可以支持队列分片；https://github.com/rabbitmq/rabbitmq-sharding

### 51 精益求精：深入研究一下Broker是如何持久化存储消息的？

>Broker数据存储实际上才是一个MQ最核心的环节，他决定了生产者消息写入的吞吐量，决定了消息不能丢失，决定了消费者获取消息的吞吐量，这些都是由他决定的。

#### CommitLog消息顺序写入机制
+ 第一步，他会把这个消息直接写入磁盘上的一个日志文件，叫做CommitLog，直接顺序写入这个文件
    
    这个CommitLog是很多磁盘文件，每个文件限定最多1GB，Broker收到消息之后就直接追加写入这个文件的末尾，就跟上面的图里一
    样。如果一个CommitLog写满了1GB，就会创建一个新的CommitLog文件。
    
####  MessageQueue在数据存储中是体现在哪里呢？
+ 对Topic下的每个MessageQueue都会有一系列的ConsumeQueue文件。

    在Broker的磁盘上，会有下面这种格式的一系列文件：
    $HOME/store/consumequeue/{topic}/{queueId}/{fileName}
    
    {topic}指代的就是某个Topic，

    {queueId}指代的就是某个MessageQueue。
    
    然后对存储在这台Broker机器上的Topic下的一个MessageQueue，他有很多的ConsumeQueue文件，这个ConsumeQueue文件里
    存储的是一条消息对应在CommitLog文件中的offset偏移量。
    
当你的Broker收到一条消息先写入了CommitLog, 之后，其实他同时会将这条消息在CommitLog中的物理位置，也就是一个文
件偏移量，就是一个offset，写入到这条消息所属的MessageQueue对应的ConsumeQueue文件中去

实际上在ConsumeQueue中存储的每条数据不只是消息在CommitLog中的offset偏移量，还包含了消息的长度，以及tag
hashcode，一条数据是20个字节，每个ConsumeQueue文件保存30万条数据，大概每个文件是5.72MB。

所以实际上Topic的每个MessageQueue都对应了Broker机器上的多个ConsumeQueue文件，保存了这个MessageQueue的所有消息
在CommitLog文件中的物理位置，也就是offset偏移量。

#### 如何让消息写入CommitLog文件近乎内存写性能的？

> Broker是基于OS操作系统的PageCache和顺序写两个机制，来提升写入CommitLog文件的性能的。

首先Broker是以顺序的方式将消息写入CommitLog磁盘文件的，也就是每次写入就是在文件末尾追加一条数据就可以了，对文件进行
顺序写的性能要比对文件随机写的性能提升很多

另外，数据写入CommitLog文件的时候，其实不是直接写入底层的物理磁盘文件的，而是先进入OS的PageCache内存缓存中，然后后
续由OS的后台线程选一个时间，异步化的将OS PageCache内存缓冲中的数据刷入底层的磁盘文件。

采用磁盘文件顺序写+OS PageCache写入+OS异步刷盘的策略，基本上可以让消息写入CommitLog的性能
跟你直接写入内存里是差不多的，所以正是如此，才可以让Broker高吞吐的处理每秒大量的消息写入。

#### 同步刷盘与异步刷盘

上述的异步刷盘模式下，生产者把消息发送给Broker，Broker将消息写入OS PageCache中，就直接返回ACK给生产者了

有一定几率丢失消息

另外一种模式叫做同步刷盘，如果你使用同步刷盘模式的话，那么生产者发送一条消息出去，broker收到了消息，必须直接强制把这个
消息刷入底层的物理磁盘文件中，然后才会返回ack给producer，此时你才知道消息写入成功了。

除非是你的物理磁盘坏了导致数据丢失，否则正常来说数据就不会丢失了  ,但效率低

### 52 授人以渔：同步刷盘和异步刷盘分别适用于什么场景呢？

 1、rocket存储是存放到一个commitlog磁盘文件，所有的topic消息存储在一起，默认1g,
 
 为什么是1g，因为内存映射读取文件会传入偏移量，long类型再转化为int类型，32位所以最大为2g.
 
 在写入commitlog后，会有异步线程写入consumequeue文件，按topic和queue划分，存储了消息的在commitlog物理偏移量，消息大小等其他属性。
 
 commitlog采用顺序写和页缓存保证高效写入。
 
 页缓存每页8k可以设置多少页才刷盘一次。先讲消息写入缓存，再异步线程去将缓存刷入磁盘。
 
 同步刷盘数据不丢失(发送消息ack成功并且磁盘有数据)，异步刷盘吞吐量高，但broker宕机还没有刷入磁盘的内存数据会丢失。
 
 对于订单生成，金额相关可以同步，对于边缘业务可以异步。
 

### 53 精益求精：基于DLedger技术的Broker主从同步原理到底是什么？

> producer写入消息到broker之后，broker会将消息写入本地CommitLog磁盘文件里去，然后
  还有一些ConsumeQueue会存储Topic下各个MessageQueue的消息的物理位置。
  
 > 如果要让Broker实现高可用，那么必须有一个Broker组，里面有一个是Leader Broker可以写入数据，然后让
   Leader Broker接收到数据之后，直接把数据同步给其他的Follower Broker


#### 基于DLedger技术替换Broker的CommitLog

Broker上述高可用架构就是基于DLedger技术来实现的

DLedger技术实际上首先他自己就有一个CommitLog机制，你把数据交给他，他会写入CommitLog磁盘文件里去，这是他能干的第一件事情。

如果基于DLedger技术来实现Broker高可用架构，实际上就是用DLedger先替换掉原来Broker自己管理的CommitLog，由DLedger来管理CommitLog

<img src="https://s1.ax1x.com/2020/07/23/UOLa8A.jpg" alt="UOLa8A.jpg" border="0" />

#### DLedger是如何基于Raft协议选举Leader Broker的？

首先基于DLedger替换各个Broker上的CommitLog管理组件了，那么就是每个Broker上都有一个DLedger组件了

实际上DLedger是基于Raft协议来进行Leader Broker选举的

发起一轮一轮的投票，通过三台机器互相投票选出来一个人作为Leader

+ 三台Broker机器启动的时候，他们都会投票自己作为Leader，然后把这个投票发送给其他Broker。

    例子，Broker01是投票给自己的，Broker02是投票给自己的，Broker03是投票给自己的，他们都把自己的投票发送给了别
    人。
    
    此时在第一轮选举中，Broker01会收到别人的投票，他发现自己是投票给自己，但是Broker02投票给Broker02自己，Broker03投票给
    Broker03自己，似乎每个人都很自私，都在投票给自己，所以第一轮选举是失败的。
    
 + 接着每个人会进入一个随机时间的休眠，比如说Broker01休眠3秒，Broker02休眠5秒，Broker03休眠4秒。
    
    此时Broker01必然是先苏醒过来的，他苏醒过来之后，直接会继续尝试投票给自己，并且发送自己的选票给别人。
    
    接着Broker03休眠4秒后苏醒过来，他发现Broker01已经发送来了一个选票是投给Broker01自己的，此时他自己因为没投票，所以会
    尊重别人的选择，就直接把票投给Broker01了，同时把自己的投票发送给别人。  2也一样
    
    只要有（3台机器 / 2） + 1个人投票给某个人，就会选举他当Leader，这个（机器数量 / 2） + 1就是大多数的意思
    
依靠这个随机休眠的机制，基本上几轮投票过后，一般都是可以快速选举出来一个Leader。

### DLedger是如何基于Raft协议进行多副本同步的？

> 和zk 的,多数提交+2pc 差不多

数据同步会分为两个阶段，一个是uncommitted阶段，一个是commited阶段

+ 首先Leader Broker上的DLedger收到一条数据之后，会标记为uncommitted状态，然后他会通过自己的DLedgerServer组件把这个uncommitted数据发送给Follower Broker的DLedgerServer。

+ 接着Follower Broker的DLedgerServer收到uncommitted消息之后，必须返回一个ack给Leader Broker的DLedgerServer，然后如
果Leader Broker收到超过半数的Follower Broker返回ack之后，就会将消息标记为committed状态。

+ 然后Leader Broker上的DLedgerServer就会发送commited消息给Follower Broker机器的DLedgerServer，让他们也把消息标记为
comitted状态。

#### 如果Leader Broker崩溃了怎么办？

如果Leader Broker挂了，此时剩下的两个Follower Broker就会重新发起选举，他们会基于DLedger还是采用Raft协议的算法，去选举
出来一个新的Leader Broker继续对外提供服务，而且会对没有完成的数据同步进行一些恢复性的操作，保证数据不会丢失。

### 54 授人以渔：采用Raft协议进行主从数据同步，会影响TPS吗？

会降低tps，其实和刷盘一样道理，可以采用异步同步的方式提高tps，最终还是得看使用场景采取合适的方式

### 55 精益求精：深入研究一下消费者是如何获取消息处理以及进行ACK的？

#### 消费组到底是个什么概念？
消费者组的意思，就是让你给一组消费者起一个名字。比如我们有一个Topic叫“TopicOrderPaySuccess”，然后假设有库存系统、积分系统、营销系统、仓储系统他们都要去消费这个Topic中的数据。

正常情况下来说，这条消息进入Broker之后，库存系统和营销系统作为两个消费组，每个组都会拉取到这条消息。

也就是说这个订单支付成功的消息，库存系统会获取到一条，营销系统也会获取到一条，他们俩都会获取到这条消息

但一般情况下来说，库存系统的两台机器中只有一台机器会获取到这条消息，营销系统也是同理。

#### 集群模式消费 vs 广播模式消费
> 默认消费 ,就是集群模式,消费组内有一个消费者消费

为广播模式：
consumer.setMessageModel(MessageModel.BROADCASTING);

修改为广播模式，那么对于消费组获取到的一条消息，组内每台机器都可以获取到这条消息。但是相对而言广播模式其实用的很
少，常见基本上都是使用集群模式来进行消费的。


#### MessageQueue、CommitLog、ConsumeQueue之间的关系
Topic中的多个MessageQueue会分散在多个Broker上，在每个Broker机器上，一个MessageQueue就对应了一个ConsumeQueue，
当然在物理磁盘上其实是对应了多个ConsumeQueue文件的，但是我们大致也理解为一 一对应关系。

然后对于Topic的各个MessageQueue而言，就是通过各个ConsumeQueue文件来存储属于MessageQueue的消息在CommitLog文
件中的物理地址，就是一个offset偏移量

#### MessageQueue与消费者的关系

>  最好MessageQueue和消费者 数量对应

一个Topic的多个MessageQueue会均匀分摊给消费组内的多个机器去消费，这里的一个原则就是，一个
MessageQueue只能被一个消费机器去处理，但是一台消费者机器可以负责多个MessageQueue的消息处理

#### Push模式 vs Pull模式

一个消费组内的多台机器是分别负责一部分MessageQueue的消费的

两种消费模式了，一个是Push，一个是Pull。

实际上，这两个消费模式本质是一样的，都是消费者机器主动发送请求到Broker机器去拉取一批消息下来

Push模式的实现思路我这里简单说一下：当消费者发送请求到Broker去拉取消息的时候，如果有新的消息可以消费那么就会立马返回
一批消息到消费机器去处理，处理完之后会接着立刻发送请求到Broker机器去拉取下一批消息。

消息处理的时效性非常好，看起来就跟Broker一直不停的推送消息到消费机器一样。

Push模式下有一个请求挂起和长轮询的机制，也要给大家简单介绍一下。

当你的请求发送到Broker，结果他发现没有新的消息给你处理的时候，就会让请求线程挂起，默认是挂起15秒，然后这个期间他会有
后台线程每隔一会儿就去检查一下是否有的新的消息给你，另外如果在这个挂起过程中，如果有新的消息到达了会主动唤醒挂起的线
程，然后把消息返回给你。

#### Broker是如何将消息读取出来返回给消费机器的？

+ 假设一个消费者机器发送了拉取请求到Broker了，他说我这次要拉取MessageQueue0中的消息，然后我之前都没拉取过消息，所以就
从这个MessageQueue0中的第一条消息开始拉取好了。

    于是，Broker就会找到MessageQueue0对应的ConsumeQueue0，从里面找到第一条消息的offset

    接着Broker就需要根据ConsumeQueue0中找到的第一条消息的地址，去CommitLog中根据这个offset地址去读取出来这条消息的数
    据，然后把这条消息的数据返回给消费者机器
    
其实消费消息的时候，本质就是根据你要消费的MessageQueue以及开始消费的位置，去找到对应的ConsumeQueue读取里面对
应位置的消息在CommitLog中的物理offset偏移量，然后到CommitLog中根据offset读取消息数据，返回给消费者机器。

#### 消费者机器如何处理消息、进行ACK以及提交消费进度？

当我们处理完这批消息之后，消费者机器就会提交我们目前的一个消费进度到Broker上去，然后Broker就会存储我们的消费进度

那么他会记录下来一个ConsumeOffset的东西去标记我们的消费进度

那么下次这个消费组只要再次拉取这个ConsumeQueue的消息，就可以从Broker记录的消费位置开始继续拉取，不用重头开始拉取了。

#### 如果消费组中出现机器宕机或者扩容加机器，会怎么处理？

会进入一个rabalance的环节，也就是说重新给各个消费机器分配他们要处理的MessageQueue。

大家举个例子，比如现在机器01负责MessageQueue0和Message1，机器02负责MessageQueue2和MessageQueue3，现在机器
02宕机了，那么机器01就会接管机器02之前负责的MessageQueue2和MessageQueue3。

ps: 扩容,重新分配可能出现重复消费


### 56 授人以渔：消费者到底什么时候可以认为是处理完消息了？

+ 消费组：
    消费topic的一个组群；不同消费组能同时拉取到消息；公用相同的consumequeue，但是储存的消费进度不一样；
    
    消费进度是以topic@消费组名为key，存储消费进度的；"topic@_group":{6:13,5:15,0:28,7:13,2:20,1:23,3:17,4:16} 由于线上都是集群部署消费者，有多个消费者实例（ip@instanceName），消费者启动的时候，就会重新消费的负载，在消费客服端的宕机与扩容都会重新分配新的messagequeue；
    
    包装一个messagequeue只能被一个consumer消费，一个consumer可以消费多个topic的messagqueue 集群模式：一个messagequeue只能一个consume instance消费,消费进度存在服务端；
    
    广播模式：一个messagequeue可以被所有的消费端消费，消费进度存储在客户端； 
    
    topic：主题，存储的时候对数据进行分片，多个messagequeue；
    
    多个topic的数据存储在commitlog磁盘；每个topic都对应一个consumequeue，consumequeue含多个文件，
    
    每个文件30万条数据记录，每条数据20字节，每个文件5.85m；前8字节存commitlog的物理偏移量，4位存长度，8位存tags和hashcode；
    
    consumeOffset存的是consumequeue的逻辑偏移量即下标

+ ack模式有两种  一种自动还有一种手动

    默认每次拉取32条消息，可以设置每次消费为1；

+ 一个消费组内多台机器分配messagequeue，有5种算法，保证一个message只能被一个consume消费； 

    5、主要从brokerpull取，push模式即consumer和broker建立长连接，有消息就一直拉取，处理完一批消息后就去处理下一批消息；
    
    没有消息时会挂起，有消息达到会后台线程唤醒； 从哪里开始获取消费进度还需要去翻源码；
     
     6、在消费进度获取到consumequeue的逻辑偏移量，找到queue对应的consumequeue的下标，获取commitlig的物理偏移量和消息长度，通过tags去过滤消息，通过mmap内存映射读取文件返回给消费端； 
     
     7、消息消费者通过回调，获取到消息处理完后通过ack机制告诉服务端消息是否消费成功，成功后提交消费进度，消费进度不是每消费一次就立马更新一次； 
     
     8、有后台线程去轮询消费端与服务端的负载关系与重新分配；
     

### 57 精益求精：消费者到底是根据什么策略从Master或Slave上拉取消息的？

> 消息消费，可以从Master Broker拉取，也可以从Slave Broker拉取，具体是要看机器负载来定。

刚开始消费者都是连接到Master Broker机器去拉取消息的，然后如果Master Broker机器觉得自己负载比较高，就会
告诉消费者机器，下次可以从Slave Broker机器去拉取。

#### CommitLog基于os cache提升写性能的回顾

这个ConsumeQueue文件的读取是如何进行性能优化的，其实他本质就是基于os cache来进行优化的

1. broker收到一条消息，会写入CommitLog文件，但是会先把CommitLog文件中的数据写入os cache(操作系统管理的缓存)

2. 然后os自己有后台线程，过一段时间后会异步把os cache缓存中的CommitLog文件的数据刷入磁盘中去

就是依靠这个写入CommitLog时先进入os cache缓存，而不是直接进入磁盘的机制，就可以实现broker写CommitLog文件的性能是
内存写级别的，这才能实现broker超高的消息接入吞吐量

#### 一个很关键的问题：ConsumeQueue文件也是基于os cache的

关键的问题，那就是ConsumeQueue会被大量的消费者发送的请求给高并发的读取，所以
ConsumeQueue文件的读操作是非常频繁的，而且同时会极大的影响到消费者进行消息拉取的性能和消费吞吐量。

所以实际上broker对ConsumeQueue文件同样也是基于os cache来进行优化的

也就是说，对于Broker机器的磁盘上的大量的ConsumeQueue文件，在写入的时候也都是优先进入os cache中的

而且os自己有一个优化机制，就是读取一个磁盘文件的时候，他会自动把磁盘文件的一些数据缓存到os cache中。

而且大家之前知道ConsumeQueue文件主要是存放消息的offset，所以每个文件很小，30万条消息的offset就只有5.72MB而已。所以

实际上ConsumeQueue文件们是不占用多少磁盘空间的，他们整体数据量很小，几乎可以完全被os缓存在内存cache里。

所以实际上在消费者机器拉取消息的时候，第一步大量的频繁读取ConsumeQueue文件，几乎可以说就是跟读内存里的数据的性能是
一样的，通过这个就可以保证数据消费的高性能以及高吞吐

#### 第二个关键问题：CommitLog是基于os cache+磁盘一起读取的

看第二个比较关键的问题，在进行消息拉取的时候，先读os cache里的少量ConsumeQueue的数据，这个性能是极高的，
然后第二步就是要根据你读取到的offset去CommitLog里读取消息的完整数据了。

答案是：两者都有

因为CommitLog是用来存放消息的完整数据的，所以内容量是很大的，毕竟他一个文件就要1GB，所以整体完全有可能多达几个TB。

明显是不可能的，因为os cache用的也是机器的内存，一般多也就几十个GB而已，何况Broker自身的JVM也要用一些内存，留个os
cache的内存只是一部分罢了，比如10GB~20GB的内存，所以os cache对于CommitLog而言，是无法把他全部数据都放在里面给你
读取的！

也就是说，os cache对于CommitLog而言，主要是提升文件写入性能，当你不停的写入的时候，很多最新写入的数据都会先停留在os
cache里，比如这可能有10GB~20GB的数据。

之后os会自动把cache里的比较旧的一些数据刷入磁盘里，腾出来空间给更新写入的数据放在os cache里，所以大部分数据可能多达几
个TB都是在磁盘上的

最终结论来了，当你拉取消息的时候，可以轻松从os cache里读取少量的ConsumeQueue文件里的offset，这个性能是极高的，
但是当你去CommitLog文件里读取完整消息数据的时候，会有两种可能。

+ 第一种可能，如果你读取的是那种刚刚写入CommitLog的数据，那么大概率他们还停留在os cache中，此时你可以顺利的直接从os
cache里读取CommitLog中的数据，这个就是内存读取，性能是很高的。

+ 第二种可能，你也许读取的是比较早之前写入CommitLog的数据，那些数据早就被刷入磁盘了，已经不在os cache里了，那么此时你
就只能从磁盘上的文件里读取了，这个性能是比较差一些的。

#### 什么时候会从os cache读？什么时候会从磁盘读？

这个问题很简单了，如果你的消费者机器一直快速的在拉取和消费处理，紧紧的跟上了生产者写入broker的消息速率，那么你每次
拉取几乎都是在拉取最近人家刚写入CommitLog的数据，那几乎都在os cache里。

但是如果broker的负载很高，导致你拉取消息的速度很慢，或者是你自己的消费者机器拉取到一批消息之后处理的时候性能很低，处理
的速度很慢，这都会导致你跟不上生产者写入的速率。

#### Master Broker什么时候会让你从Slave Broker拉取数据？

问题的解答，本质是对比你当前没有拉取消息的数量和大小，以及最多可以存放在os cache内存里的消息的大小，

如果你没拉取的消息超过了最大能使用的内存的量，那么说明你后续会频繁从磁盘加载数据，此时就让你从slave broker去加载数据了！

比如: os cache里面是5万-8万的数据, 你的偏移量是在2万, 那你还会把 2万-5万的数据 读到cache  ,

### 58 授人以渔：消费者是跟所有Broker建立连接，还是跟部分Broker建立连接？


（1）消费者机器到底是跟少数几台Broker建立连接，还是跟所有Broker都建立连接？这是不少朋友之前在评论区提出的问题，但是我
想这里大家肯定都有自己的答案了。

（2）RocketMQ是支持主从架构下的读写分离的，而且什么时候找Slave Broker读取大家也都了解的很清楚了，那么大家思考一下，
Kafka、RabbitMQ他们支持主从架构下的读写分离吗？支持Slave Broker的读取吗？为什么呢？

（3）如果支持读写分离的话，有没有一种可能，就是出现主从数据不一致的问题？比如有的数据刚刚到Master Broker和部分Slave
Broker，但是你刚好是从那个没有写入数据的Slave Broker去读取了？

（4）消费吞吐量似乎是跟你的处理速度有很大关系，如果你消费到一批数据，处理太慢了，会导致你严重跟不上数据写入的速度，这
会导致你后续几乎每次拉取数据都会从磁盘上读取，而不是os cache里读取，所以你觉得你在拉取到一批消息处理的时候，应该有哪些
要点需要注意的？


1. 消费者机器跟少数的broker建立长连接。
    会跟nameserver这台机器建立连接，然后去找对应的监听消费者的机器去连接，不是所有的机器都会连接的

2. 当主节点内存不够，积压比较多会从从节点拉取。

3. 主节点挂掉  肯定会存在主从数据不一致，延迟的情况。所以为了考虑消息的一致性，主从同步双写。
4. 消费消息时提高响应速度。


### 59 探秘黑科技：RocketMQ 是如何基于Netty扩展出高性能网络通信架构的？

#### Reactor主线程与长短连接

+ Reactor模式: (netty )
    
    两个线程池, 主线程池和工作线程池,
    
+ 长连接、短连接
    
    长连接:你建立一个连接 -> 发送请求 -> 接收响应 -> 发送请求 -> 接收响应 -> 发送请求 -> 接收响应
     
     大家会发现，当你建立好一个长连接之后，可以不停的发送请求和接收响应，连接不会断开，等你不需要的时候再断开就行了
                  
    短连接 :   建立连接 -> 发送请求 -> 接收响应 -> 断开连接，下一次你要发送请求的时候，这个过程得重新来一遍

+ 基于Reactor线程池监听连接中的请求
    
    >通过 SocketChannel 打开一个长链接
    
    这个线程池里默认是3个线程！
    
    Reactor主线程建立好的每个连接SocketChannel，都会交给这个Reactor线程池里的其中一个线程去监听请求
    
+ 基于Worker线程池完成一系列准备工作
    
    Worker线程池，他默认有8个线程，此时Reactor线程收到的这个请求会交给Worker线程池中的一个线程进行处理
+ 基于业务线程池完成请求的处理 
    
    Worker线程完成了一系列的预处理之后，比如SSL加密验证、编码解码、连接空闲检查、网络连接管理，等等，接着就需 要对这个请求进行正式的业务处理了
    
    你接收到了消息，肯定是要写入CommitLog文件的，后续还有一些ConsumeQueue之类的事情需要处理，类似这种操作，就是业务处理逻辑
    
+ 为什么这套网络通信框架会是高性能以及高并发的？

    分配一个Reactor主线程出来，就是专门负责跟各种Producer、Consumer之类的建立长连接。
    
    一旦连接建立好之后，大量的长连接均匀的分配给Reactor线程池里的多个线程。
    
    每个Reactor线程负责监听一部分连接的请求，这个也是一个优化点，通过多线程并发的监听不同连接的请求，可以有效的提升大量并
    发请求过来时候的处理能力，可以提升网络框架的并发能力。
    
    接着后续对大量并发过来的请求都是基于Worker线程池进行预处理的，当Worker线程池预处理多个请求的时候，Reactor线程还是可
    以有条不紊的继续监听和接收大量连接的请求是否到达。
    
    而且最终的读写磁盘文件之类的操作都是交给业务线程池来处理的，当他并发执行多个请求的磁盘读写操作的时候，不影响其他线程池
    同时接收请求、预处理请求，没任何的影响。
    
    所以最终的效果就是：
    + Reactor主线程在端口上监听Producer建立连接的请求，建立长连接
    + Reactor线程池并发的监听多个连接的请求是否到达
    + Worker请求并发的对多个请求进行预处理
    + 业务线程池并发的对多个请求进行磁盘读写业务操作 
    

###  60 授人以渔：BIO、NIO、AIO以及Netty之间的关系是什么？

bio 同步阻塞模型，一个线程负责建立负责业务操作，

nio同步非阻塞，监听线程链接请求后注册到selector选择器上 然后通过轮训各个客户端的请求 如果有读写请求则交给另外的线程去处理 ，

netty只不过是对nio进行了封装 加了线程池还有一些操作

### 61 探秘黑科技：基于mmap内存映射实现磁盘文件的高性能读写

#### mmap：Broker读写磁盘文件的核心技术
Broker中大量的使用mmap技术去实现CommitLog这种大磁盘文件的高性能读写优化的。

通过之前的学习，我们知道了一点，就是Broker对磁盘文件的写入主要是借助直接写入os cache来实现性能优化的，因为直接写入os
cache，相当于就是写入内存一样的性能，后续等os内核中的线程异步把cache中的数据刷入磁盘文件即可

#### 传统文件IO操作的多次数据拷贝问题

读写数据,会分别发生两次数据拷贝,一共四次

#### RocketMQ是如何基于mmap技术+page cache技术优化的？
JDK NIO包下的MappedByteBuffer.map()函数干的事情，底层就是基于mmap技术实现的。 他的的本质就是操作对外内存


因为刚开始你建立映射的时候，并没有任何的数据拷贝操作，其实磁盘文件还是停留在那里，只不过他把物理上的磁盘文件的一些地址
和用户进程私有空间的一些虚拟内存地址进行了一个映射

另外这里给大家说明白的一点是，这个mmap技术在进行文件映射的时候，一般有大小限制，在1.5GB~2GB之间

所以RocketMQ才让CommitLog单个文件在1GB，ConsumeQueue文件在5.72MB，不会太大。

这样限制了RocketMQ底层文件的大小，就可以在进行文件读写的时候，很方便的进行内存映射了
     
#### 基于mmap技术+pagecache技术实现高性能的文件读写
接下来就可以对这个已经映射到内存里的磁盘文件进行读写操作了，比如要写入消息到CommitLog文件，你先把一个CommitLog文件
通过MappedByteBuffer的map()函数映射其地址到你的虚拟内存地址。

接着就可以对这个MappedByteBuffer执行写入操作了，写入的时候他会直接进入PageCache中，然后过一段时间之后，由os的线程
异步刷入磁盘中 

他就是从PageCache里拷贝到磁盘文件里而已！这个就是你使用mmap技术之后，相比于传统磁盘IO的一个性能优化。

而且PageCache技术在加载数据的时候，还会将你加载的数据块的临近的其他数据块也一起加载到PageCache里去。

在你读取数据的时候，他会判断PageCache有没有,有的话,其实也仅仅发生了一次拷贝，而不是两次拷贝，所以这个性能相较于传统IO来说，肯定又是提高了。

#### 预映射机制 + 文件预热机制
（1）内存预映射机制：Broker会针对磁盘上的各种CommitLog、ConsumeQueue文件预先分配好MappedFile，也就是提前对一些
可能接下来要读写的磁盘文件，提前使用MappedByteBuffer执行map()函数完成映射，这样后续读写文件的时候，就可以直接执行
了。

（2）文件预热：在提前对一些文件完成映射之后，因为映射不会直接将数据加载到内存里来，那么后续在读取尤其是CommitLog、
ConsumeQueue的时候，其实有可能会频繁的从磁盘里加载数据到内存中去。

### 62 授人以渔：思考一个小问题，Java工程师真的只会Java就可以了吗？
> ,,,,  wo

### 63 抛砖引玉：通过本专栏的大白话讲解之后，再去深入阅读一些书籍和源码

### 64 授人以渔：一个学习方法的探讨，如何深入研究一个技术？
> 学习路径:1 官网，百度入门案例，环境部署与搭建，简单的使用层面。 2.项目实战，在哪些业务场景可以使用。3.阅读源码，多读几遍，每次读一遍的感悟不一样，毕竟不同时间自己的技术积累和心得都有提高


### 65 阶段性复习：一张思维导图带你梳理 RocketMQ 的底层实现原理
<img src="https://s1.ax1x.com/2020/07/23/UXVXDO.jpg" alt="UXVXDO.jpg" border="0" />

### 66 阶段性复习：在深度了解RocketMQ底层原理的基础之上，多一些主动思考

（1）Kafka、RabbitMQ他们有类似的数据分片机制吗？他们是如何把一个逻辑上的数据集合概念（比如一个Topic）给在物理上拆分
为多个数据分片的？然后拆分后的多个数据分片又是如何在物理的多台机器上分布式存储的？


（2）为什么一定要让MQ实现数据分片的机制？如果不实现数据分片机制，让你来设计MQ中一个数据集合的分布式存储，你觉得好设
计吗？

（3）同步刷盘和异步刷盘两种策略，分别适用于什么不同的场景呢？

（4）异步刷盘可以提供超高的写入吞吐量，但是有丢失数据的风险，这个适用于什么业务场景？在你所知道的业务场景，或者工作接
触过的业务场景中，有哪些场景需要超高的写入吞吐量，但是可以适度接受数据丢失？

（5）同步刷盘会大幅度降低写入吞吐量，但是可以让你的数据不丢失，你接触哪些场景，是严格要求数据务必不能丢失任何一条，但
是吞吐量并没有那么高的呢？

（6）Kafka、RabbitMQ他们的broker收到消息之后是如何写入磁盘的？采用的是同步刷盘还是异步刷盘的策略？为什么？

（7）每次写入都必须有超过半数的Follower Broker都写入消息才可以算做一次写入成功，那么大家思考一个问题，这样做是不是会对
Leader Broker的写入性能产生影响？是不是会降低TPS？是不是必须要在所有的场景都这么做？为什么呢？

（8）一般我们获取到一批消息之后，什么时候才可以认为是处理完这批消息了？是刚拿到这批消息就算处理完吗？还是说要对这批消
息执行完一大堆的数据库之类的操作，才算是处理完了？

（9）如果获取到了一批消息，还没处理完呢，结果机器就宕机了，此时会怎么样？这些消息会丢失，再也无法处理了吗？如果获取到
了一批消息，已经处理完了，还买来得及提交消费进度，此时机器宕机了，会怎么样呢？

（10）消费者机器到底是跟少数几台Broker建立连接，还是跟所有Broker都建立连接？这是不少朋友之前在评论区提出的问题，但是
我想这里大家肯定都有自己的答案了。

（11）RocketMQ是支持主从架构下的读写分离的，而且什么时候找Slave Broker读取大家也都了解的很清楚了，那么大家思考一下，
Kafka、RabbitMQ他们支持主从架构下的读写分离吗？支持Slave Broker的读取吗？为什么呢？

（12）如果支持读写分离的话，有没有一种可能，就是出现主从数据不一致的问题？比如有的数据刚刚到Master Broker和部分Slave
Broker，但是你刚好是从那个没有写入数据的Slave Broker去读取了？

（13）消费吞吐量似乎是跟你的处理速度有很大关系，如果你消费到一批数据，处理太慢了，会导致你严重跟不上数据写入的速度，这
会导致你后续几乎每次拉取数据都会从磁盘上读取，而不是os cache里读取，所以你觉得你在拉取到一批消息处理的时候，应该有哪些
要点需要注意的？

1.kafka也有类似的数据分片机制，通过逻辑数据集合，分为多个分区，每个分区指定一个consumequeue。多个数据分片，btokername相同的数据分片储存在对应的机器上。

2.如果不设计数据分片，消费消息并发比较低。

3.同步刷盘适合对数据不丢失要求比较高，订单金额相关，对于日志一些其他数据可以用异步刷盘。

8.需要返回ack.

9.重试消费

### 67 生产案例：从 RocketMQ 全链路分析一下为什么用户支付后没收到红包？
    
#### 客服反馈的一个奇怪问题：支付之后没有收到红包
> 消息丢失

#### 消息丢失的 三种情况

+ 订单系统推送消息到MQ的过程
    
     如网络状况
     
+ 消息到达MQ了，MQ自己会导致消息丢失
    
    如消息在 PageCache ,还没写到磁盘,服务器出了问题
    
    写到磁盘,磁盘坏了
    
+  红包系统拿到了消息，丢失
    
    拿到了消息，但是他把消息搞丢了，结果红包还没来得及发
    
### 68 发送消息零丢失方案：RocketMQ事务消息的实现流程分析

#### 解决消息丢失的第一个问题：订单系统推送消息丢失
在RocketMQ中，有一个非常强悍有力的功能，就是事务消息的功能，凭借这个事务级的消息机制，就可以让我们确保订单系统推送给
出去的消息一定会成功写入MQ里，绝对不会半路就搞丢了。

+ 发送half消息到MQ去，试探一下MQ是否正常
    
    消息的状态是half状态，这个时候红包系统是看不见这个half消息的
    
+  万一要是half消息写入失败了呢？
    
    可能你发现报错了，可能MQ就挂了，或者这个时候网络就是故障了，所以导致你的half消息都没发送成功，总之你现在肯定没法跟
    MQ通信了。
    
    这个时候你的订单系统就应该执行一系列的回滚操作，比如对订单状态做一个更新，让状态变成“关闭交易”，同时通知支付系统自动
    进行退款，这才是正确的做法。
 
 + half消息成功之后，订单系统完成自己的任务
 
    一旦half消息写成功了，就说明MQ肯定已经收到这条消息了，MQ还活着，而且目前你是可以跟MQ正常沟通的。
    
+ 如果订单系统的本地事务执行失败了怎么办？
    
    订单系统的数据库当时也有网络异常，或者数据库挂了，总而言之，就是你想把订单更新为“已完成”这个状态，是干不成了。
    
    这个时候其实也很简单，直接就是让订单系统发送一个rollback请求给MQ就可以了。这个意思就是说，你可以把之前我发给你的half
    消息给删除掉了，因为我自己这里都出问题了，已经无力跟你继续后续的流程了
    
+ 如果订单系统完成了本地事务之后，接着干什么？
    
    如果订单系统成功完成了本地的事务操作，比如把订单状态都更新为“已完成”了，此时你就可以发送一个commit请求给MQ，要求
    让MQ对之前的half消息进行commit操作，让红包系统可以看见这个订单支付成功消息 
 
 + 让流程严谨一些：如果发送half消息成功了，但是没收到响应呢？
    
    如果我们把half消息发送给MQ了，MQ给保存下来了，但是MQ返回给我们的响应我们没收到呢？
    
    其实RocketMQ这里有一个补偿流程，他会去扫描自己处于half状态的消息，如果我们一直没有对这个消息执行commit/rollback操
    作，超过了一定的时间，他就会回调你的订单系统的一个接口
    
    他会问问你：这个消息到底怎么回事？你到底是打算commit这个消息还是要rollback这个消息？
    
    去查一下数据库，看看这个订单当前的状态，如果发现订单状态是“已关闭”，此时就知道，你必然得发送rollback请求给MQ去删除之前那个half消息了！
    
+ 如果rollback或者commit发送失败了呢？
    
    如果订单系统是收到了half消息写入成功的响应了，同时尝试对自己的数据库更新了，然后根据失败或者成功去执行了rollback或者commit请求，发送给MQ了，结果因为网络故障，导致rollback或者commit请求发送失败了呢？
    
    本质这个MQ的回调就是一个补偿机制，如果你的half消息响应没收到，或者rollback、commit请求没发送成功，他都会来找你问问对
    half消息后续如何处理
    
    再假设一种场景，如果订单系统收到了half消息写入成功的响应了，同时尝试对自己的数据库更新了，然后根据失败或者成功去执行了
    rollback或者commit请求，发送给MQ了。很不巧，mq在这个时候挂掉了，导致rollback或者commit请求发送失败，怎么办？
    
    如果是这种情况的话，那就等mq自己重启了，重启之后他会扫描half消息，然后还是通过上面说到的补偿机制，去回调你的接口

### 69 RocketMQ黑科技解密：事务消息机制的底层实现原理

#### half 消息是如何对消费者不可见的？

之前的底层原理剖析的环节都知道，其实你写入一个Topic，最终是定位到这个Topic的某个MessageQueue，然后定位到
一台Broker机器上去，然后写入的是Broker上的CommitLog文件，同时将消费索引写入MessageQueue对应的ConsumeQueue文件

同时消息的offset会写入MessageQueue对应的ConsumeQueue，这个ConsumeQueue是属于OrderPaySuccuessTopic的，然后红
包系统按理说会从这个ConsumeQueue里获取到你写入的这个half消息。

但是实际上红包系统却没法看到这条消息，其本质原因就是RocketMQ一旦发现你发送的是一个half消息，他不会把这个half消息的
offset写入OrderPaySuccessTopic的ConsumeQueue里去。

他会把这条half消息写入到自己内部的“RMQ_SYS_TRANS_HALF_TOPIC”这个Topic对应的一个ConsumeQueue里去

RocketMQ是写入内部Topic的ConsumeQueue的，不是写入你指定的OrderPaySuccessTopic的ConsumeQueue的

所以你的红包系统自然无法从OrderPaySuccessTopic的ConsumeQueue中看到这条half消息了

#### 在什么情况下订单系统会收到half消息成功的响应？
必须要half消息进入到RocketMQ内部的RMQ_SYS_TRANS_HALF_TOPIC的ConsumeQueue文件了，此时就会认为half消息写入成功了，然后就会返回响应给订单系统。

所以这个时候，一旦你的订单系统收到这个half消息写入成功的响应，必然就知道这个half消息已经在RocketMQ内部了。

#### 假如因为各种问题，没有执行rollback或者commit会怎么样？
假如因为网络故障，订单系统没收到half消息的响应，或者说自己发送的rollback/commit请求失败了，那么RocketMQ
会干什么？

其实这个时候他会在后台有定时任务，定时任务会去扫描RMQ_SYS_TRANS_HALF_TOPIC中的half消息，如果你超过一定时间还是
half消息，他会回调订单系统的接口，让你判断这个half消息是要rollback还是commit

#### 如果执行rollback操作的话，如何标记消息回滚？

假设我们的订单系统执行了rollback请求，那么此时就需要对消息进行回滚。

因为RocketMQ都是顺序把消息写入磁盘文件的，所以在这里如果你执行rollback，他的本质就是用一个OP操作来标记half消息的状态

RocketMQ内部有一个OP_TOPIC，此时可以写一条rollback OP记录到这个Topic里，标记某个half消息是rollback了

假设你一直没有执行commit/rollback，RocketMQ会回调订单系统的接口去判断half消息的状态，但是他最多就
是回调15次，如果15次之后你都没法告知他half消息的状态，就自动把消息标记为rollback。

#### 如果执行commit操作，如何让消息对红包系统可见？

你执行commit操作之后，RocketMQ就会在OP_TOPIC里写入一条记录，标记half消息已经是commit状态了。

接着需要把放在RMQ_SYS_TRANS_HALF_TOPIC中的half消息给写入到OrderPaySuccessTopic的ConsumeQueue里去，然后我们的
红包系统可以就可以看到这条消息进行消费了

### 70 为什么解决发送消息零丢失方案，一定要使用事务消息方案？
<img src="https://s1.ax1x.com/2020/07/23/UXK2vR.jpg" alt="UXK2vR.jpg" border="0" />

我们真的有必要使用这么复杂的机制去确保消息到达MQ，而且绝对不会丢吗？

#### 一个小思考：能不能基于重试机制来确保消息到达MQ？

只要我们在代码中发送消息到MQ之后，同步等待MQ返回响应给我们，一直等待，如果半路中有网络异常或者MQ内部异常，我
们肯定会收到一个异常，比如网络错误，或者请求超时之类的。

如果我们在收到异常之后，就认为消息到MQ发送失败了，然后再次重试尝试发送消息到MQ，接着再次同步等待MQ返回响应给我
们，这样反复重试，是否可以确保消息一定会到达MQ？

确实如此，而且在基于Kafka作为消息中间件的消息零丢失方案中，对于发送消息这块，因为Kafka本身不具备RocketMQ这种事务消
息的高级功能，所以一般我们都是对Kafka会采用同步发消息 + 反复重试多次的方案，去保证消息成功投递到Kafka的。

但是如果是在类似我们目前这个较为复杂的订单业务场景中，仅仅采用同步发消息 + 反复重试多次的方案去确保消息绝对投递到MQ
中，似乎还是不够的

#### 先执行订单本地事务，还是先发消息到MQ？

如果我们先执行订单本地事务，接着再发送消息到MQ的话

有一个问题，假设你刚执行完成了订单本地事务了，结果还没等到你发送消息到MQ，结果你的订单系统突然崩溃了！

这就导致你的订单状态可能已经修改为了“已完成”，但是消息却没发送到MQ去！ 这就是这个方案最大的隐患

如果出现这种场景，那你的多次重试发送MQ之类的代码根本没机会执行！而且订单本地事务还已经执行成功了，你的消息没发送出
去，红包系统没机会派发红包，必然导致用户支付成功了，结果看不到自己的红包！

#### 把订单本地事务和重试发送MQ消息放到一个事务代码中

如果把订单本地事务代码和发送MQ消息的代码放到一个事务代码中呢？

````

    @transactional
    punlic  void  pay(){
        try{
            updateOrder()    ;
            mq.send(message);
        }catch(Excepton e){
            for(){
                //  重试
            }
            
            throw new XXXEXception();
        }
    }
````

方法上加入事务，在这个事务方法中，我们哪怕执行了orderService.finishOrderPay()，但是其实也仅仅执行了一些增删改SQL语句，还没提交订单本地事务。

如果发送MQ消息失败了，而且多次重试还不奏效，则我们抛出异常会自动回滚订单本地事务；

如果你刚执行了orderService.finishOrderPay()，结果订单系统直接崩溃了，此时订单本地事务会回滚，因为根本没提交过。

但是对于这个方案，还是非常的不理想，原因就出在那个MQ多次重试的地方

假设用户支付成功了，然后支付系统回调通知你的订单系统说，有一笔订单已经支付成功了，这个时候你的订单系统卡在多次重试MQ
的代码那里，可能耗时了好几秒种，此时回调通知你的系统早就等不及可能都超时异常了。

而且你把重试MQ的代码放在这个逻辑里，可能会导致订单系统的这个接口性能很差


#### 你就一定可以依靠本地事务回滚吗？
方法上加了事务注解，但是代码里还有更新Redis缓存和Elasticsearch数据的代码逻辑，如果你要是已经完成了订单数据库更新、Redis缓存更新、ES数据更新了，结果没法送MQ呢订单系统崩溃了。

虽然订单数据库的操作会回滚，但是Redis、Elasticsearch中的数据更新会自动回滚吗？

不会的，因为他们根本没法自动回滚，此时数据还是会不一致的。所以说，完全寄希望于本地事务自动回滚是不现实的。

#### 保证业务系统一致性的最佳方案：基于RocketMQ的事务消息机制

所以真正要保证消息一定投递到MQ，同时保证业务系统之间的数据完全一致，业内最佳的方案还是用基于RocketMQ的事务消息机制。


### 71 用支付后发红包的案例场景，分析RocketMQ事物消息的代码实现细节


#### 发送half事务消息出去
<a  href="https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md#6-%E6%B6%88%E6%81%AF%E4%BA%8B%E5%8A%A1%E6%A0%B7%E4%BE%8B"> Rocket 事物消息  官方demo  </a>


#### 假如half消息发送失败，或者没收到half消息响应怎么办？
在执行“producer.sendMessageInTransaction(msg, null)”的时候，收到一个异常，发现消息发送失败了。

我们可以把发送出去的half消息放在内存里，或者写入本地磁盘文件，后台开启一个线程去检查，如果一个half消息超
过比如10分钟都没有收到响应，那就自动触发回滚逻辑。

#### 如果half消息成功了，如何执行订单本地事务？
<img src="https://s1.ax1x.com/2020/07/24/UX8oIP.jpg" alt="UX8oIP.jpg" border="0" />

````
public class TransactionListenerImpl implements TransactionListener {
  private AtomicInteger transactionIndex = new AtomicInteger(0);
  private ConcurrentHashMap<String, Integer> localTrans = new ConcurrentHashMap<>();
  
  
  @Override
  public LocalTransactionState executeLocalTransaction(Message msg, Object arg) {
      int value = transactionIndex.getAndIncrement();
      int status = value % 3;
      localTrans.put(msg.getTransactionId(), status);
      return LocalTransactionState.UNKNOW;
  }
  @Override
  public LocalTransactionState checkLocalTransaction(MessageExt msg) {
      Integer status = localTrans.get(msg.getTransactionId());
      if (null != status) {
          switch (status) {
              case 0:
                  return LocalTransactionState.UNKNOW;
              case 1:
                  return LocalTransactionState.COMMIT_MESSAGE;
              case 2:
                  return LocalTransactionState.ROLLBACK_MESSAGE;
          }
      }
      return LocalTransactionState.COMMIT_MESSAGE;
  }
}
````














    
    
