回顾

1、full gc原理

2、遇到了full gc，到底应该怎么处理？

（1）尽可能避免

（2）尝试优化full gc的速率

3、我们应该如何操作来规避full gc？

4、我们应该如何操作来加快full gc的速度？

 

本节内容：

1、运营场景业务分析

有过C端互联网经验的同学应该都知道，一般在公司里面会有一个运营平台，有些公司叫营销平台。这个平台的作用主要就是，拉新，增涨营收。具体的手段就是通过一个平台发起运营活动，或者在各个平台投放广告，来实现用户增涨，流量增涨，营收增涨。

对于自己平台的存量用户，也会有大量的优惠活动，节日福利活动等方式来留住用户，或者增加营收。而我们本次要分析的案例，就是这样一个运营平台。

业务基本流程如下：

![picture.png](http://wechatapppro-1252524126.cdn.xiaoeknow.com/apppuKyPtrl1086/image/ueditor/63881400_1647077753.png?imageView2/2/q/80%7CimageMogr2/ignore-error/1)

可以看到，各种各演的运营活动，都会通过运营平台去产生，然后专门有一个抽离的运营消息推送服务来推送运营活动消息给用户。由于这些消息的数量非常庞大，对于一家几十万上百万的用户，一天假如有3次运营活动，就需要至少好几百万甚至上千万的的消息要推送。这个量还是非常大的，因此在实现的时候，实际上不会直接全量推送，而是生成推送消息，发送到MQ，然后通过一个消费者去消费这些运营活动消息，慢慢把千万级别的消息推送给用户。

![picture.png](http://wechatapppro-1252524126.cdn.xiaoeknow.com/apppuKyPtrl1086/image/ueditor/68836000_1647077753.png?imageView2/2/q/80%7CimageMogr2/ignore-error/1)

2、业务场景背景介绍

在这个场景中，一个很重要的要点就是运营消息推送平台生成消息推送到消息中间件儿，例如Kafka中，这个消息生成、推送至消息中间件儿的速率，很大程度上能影响到整个推送流程的速度。

因此，对这个消息生成，推送的过程，就需要做优化，当时做优化的主要思路就是：

（1）分布式生成消息，即多台机器，针对同一个运营活动，把用户群体分成n个部分来生成消息

（2）batch推送消息，减少与消息中间件儿的网络通信

（3）运营消息推送平台内，使用多线程并发推送batch消息

（4）把一些大批量的查询借助一些其他的数据搜索引擎或者缓存来提升效率，比如，借助本地jvm缓存，每台机器保存一些用户账号相关信息，比如使用ES来存储用户信息，提升多条件下的搜索查询效率，使用redis cluster缓存，避免使用数据库，导致效率低。具体的方式有很多，很多情况下都是结合起来使用的。不过这些不是咱们本节的重点，重点我们还是关注，怎么出的full gc问题，最后怎么解决的。

 

这些基本的思路其实还是比较简单的，细节上的优化主要就是batch的大小如何确定，多线程推送的时候，线程池的线程数量要怎么设置等等。

![picture.png](http://wechatapppro-1252524126.cdn.xiaoeknow.com/apppuKyPtrl1086/image/ueditor/68807800_1647077753.png?imageView2/2/q/80%7CimageMogr2/ignore-error/1)

 

3、问题现场及问题排查

然后在一次优化测试上线之后，运行一段时间，没有什么大问题，在某天Kafka服务发生了一次抖动，持续时间大概5-6s的时间，然后我们发现这个运营平台直接崩溃了。服务直接挂掉了。在优化之前，我们的推送效率有待提升，但是系统还是很稳定的。出现这个问题之后，紧急查看了报错日志，发现是堆内存OOM异常，导致的进程崩溃。

于是赶快下载了GC日志，下载了内存快照文件，最后发现，查看dump快照文件之后，发现是batch推送的时候线程池中线程持有了大量的大list<user>对象。

本来我们是打算找OOM的原因，解决OOM问题的，但是发现，GC日志中在oom前出现了大量的full gc，而出现大量full gc才最终导致系统走向了崩溃。

那么结合dump快照里面大量的list<user>对象，很容易想到，是因为出现了大量的list<user>导致的频繁full gc。而大量的list<user>都还存活，最终导致oom。

![picture.png](http://wechatapppro-1252524126.cdn.xiaoeknow.com/apppuKyPtrl1086/image/ueditor/69131600_1647077753.png?imageView2/2/q/80%7CimageMogr2/ignore-error/1)

那么为啥会有这么多次full gc，而又为什么有这么多的list<user>没有成为被回收掉呢？

通过代码排查，我们发现了一个很严重的代码bug。这个bug如果见过的人，其实很好规避。

我们在代码中采取的策略是，一组用户数据，2000个，封装成消息体，是按照200个一个list<user>去封装，然后推送到kafka中（注意：Kafka本身也有一个缓冲机制来实现batch发送，我们代码一条一条发送的时候，kafka会在本地客户端暂存，存到一定大小的时候，才会一批次推送到Kafka服务端）。正常来说是没啥问题的。因为200个消息体封装起来的大小本身也不大。

但是写代码的那个伙伴，因为运营的一个需求，把我们其中一段代码修改了一下。运营的具体需求是：要求在某个运营活动下，消息必须推送到用户侧，不能出现漏发，错发，多发的情况。

这个伙伴呢，他主要负责的是生成消息，推送至mq中间件儿这块儿不能出现错漏的情况。于是他针对我们原有的代码做了一个调整。

 

原有的代码是，把拿到的2000一组的用户数据，拆分成10个batch，然后推送至mq，假如推送失败了，我们不管他。漏了就漏了，问题不大。

 

修改后的代码是，另外针对这种特殊活动，添加了一些等待逻辑，等待Kafka给推送结果，推送成功，则执行完毕，推送失败则把失败的用户子集，重新加入到一个重试集合中去重试推送。

 

他的伪代码大概是这样子的：

// 获取一组用户信息

ArrayList list = getUserList();

 

​    List subList;

​    List failList = new ArrayList();

​    int index = 0;

// 拆分用户信息

while(true){

  if(index >= list.size()-1){

​        break;

​    }

subList = list.subList(index,index+200);

kafkaPushResult =sendToKafka();

// 等待发送结果

if (kafkaPushResult == false){

// 发送失败，第一次的话，就把发送失败的subList赋值给failList

  if (index == 0){

   failList = subList;

   }

// 不是第一次，就把发送失败的全部加入到failList中，等待后续继续发送

   failList.addAll(subList);

 }

}

看起来是不是没什么问题？事实上，当这个kafkaPushResult失败没有触发的时候，确实不会出什么问题，但是一旦触发了失败，就会出现问题了。原因在于对于list来说，subList不是新建了一个对象，而是，把大的list的其中一段，搞了两个指针去指向它，所以本质上，subList还是大的list的一部分，那么如果说频繁触发了发送失败的时候，failList这么写，一样相当于是大的list的一部分。那么执行了failList.addAll(subList);这段代码的时候，相当于是在把这部分失败的元素，加入到大的list里面去！

![picture.png](http://wechatapppro-1252524126.cdn.xiaoeknow.com/apppuKyPtrl1086/image/ueditor/65670400_1647077753.png?imageView2/2/q/80%7CimageMogr2/ignore-error/1)

所以，list是会越来越大的，大家想想，我们一次推送几十万用户的消息，如果一个线程池里面设置30-60个线程，几秒钟内，在Kafka抖动的这段时间，这些list会极速扩张好几千倍不止。也就是说，我们假设这5s内，一共能拿到10w的用户数据，那么在5s会膨胀到几千万甚至上亿的数据（肯定不止这么多，因为程序的循环速度，比我们数次数要快太多太多了。），紧接着，重试操作，会针对这些数据再次进行重试，而这个大的list又暂时不会释放，如果Kafka抖动的时间短，可能也就是重新推送成功之后，经历一次或几次full gc就没问题了，如果Kafka经常抖动，或者抖动时间长，那么就会造成频繁的full gc，甚至最终oom。

而事实上，我们时候复盘的时候，发现在程序运行的很长一段时间内，Kafka出现了短暂网络问题的时候，运营服务都会伴随这比较频繁的full gc，这个过程出现的时间非常短，并且在程序的最终结果上，并没有出现什么漏发，重复发的这种场景。于是这个问题就被忽略掉了。

 

就是这个代码bug导致我们这个服务，在Kafka抖动的这一段时间内，出现了大量full gc

最终程序撑不住走向了OOM。

 

我们遇到的这个场景，由于full gc产生的原因比较直接，所以分析的时候，很轻松就通过gc 日志+ dump快照文件迅速定位了问题。很多full gc场景，产生的原因各不相同，分析过程还是非常复杂的。

不过基本思路都是->系统日志->监控数据->gc日志->dump文件->代码反查->bug复现->解决问题。

 

4、问题解决

知道了问题，其实就很好解决了。归根究底还是因为在写代码的时候对api源码实现的不熟悉，同时也没有深入检查代码的习惯，导致在做一些业务处理的时候出现了意想不到的bug。

对于这个代码，其实我们只需要做一个改动就OK了。在外层初始化failList，并且在需要把subList数据作为数据源的操作的时候，使用addAll操作，把它加入到新的list中再进行操作即可。

failList = new ArrayList();

 if (index == 0){ failList = subList; } -> failList.addAll(subList)

在使用subList拆分list的时候，一定要熟悉一下这个subList的源码实现方式，往往JDK的一些优化手段，会给我们程序造成一些不必要的问题。

如果说，出现了频繁的full gc，很有可能是对象产生的速度和垃圾回收的速度匹配不上。回收的量还是不够。就有可能OOM了。