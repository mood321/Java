**疑问：上节课我们提到了一个期望停顿时间。那么如何来满足这个停顿时间呢？**

期望停顿时间，也就意味着，我尽量搞定，不一定真的能满足这个期望停顿时间。预测模型越准确，就越能接近。

 

1、思考：如何满足用户设定的停顿时间？

（1）首先要预测，在停顿时间范围(200ms)内，我能回收多少垃圾？ --- G1的停顿预测模型

对接下来要进行的回收进行一个预测，比如，我预测我能在200ms内回收2GB的垃圾，那我就选择2GB内存对应的的region来进行回收。

 

（2）那么我们预测的基石是什么？肯定不能凭空瞎预测吧？

所以，预测的基石，一定是要有历史数据，GC相关的历史数据，对吧？因为之后GC运行的情况才最能反应出来GC的能力到底有多强是不是？所以这一步，我们要先去拿到一些历史GC运行的数据，比如历史10次GC，每次GC多久，回收了多少垃圾，总的GC时间是多少。

![picture.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/76568700_1640327176.png)

 

（3）我应该怎么预测？拿到历史数据我该怎么用？

首先一个基本的逻辑，如果目标停顿时间短，就少收点分区，目标停顿时间长，就多收点分区。也就是说，我必须要知道，我**回收的能力**是多少，那么此时历次回收相关的历史数据就排上用场了，我拿这些历史数据做一个计算，看看**平均每秒能回收多少垃圾**，结合预期停顿时间，就能计算出，我这次能回收多少垃圾了。2G/200ms 10G/S

 

所以，我们需要一个历史数据的分析算法。来帮助我们分析到底回收的能力是多少。

G1本身是一个不断扩展的模型。同时，系统一直的不断的运行。有时候是高峰期，有时候是低峰期。

（4）一个简单的历史数据的分析算法模型 

求一下过去10次的GC，造成了多少停顿时间，最终计算出来一个平均每秒能够回收多少垃圾的值，例如，过去10次一共收集了10GB内存，一共花费了1s，那么200ms能够回收的垃圾就是2GB，根据这个计算值（2GB），选择一定数量的region分区

![picture.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/77211400_1640327176.png)

**思考：结合内存动态扩展思考，线性算法是否合理？**

**很显然，内存可能会动态增加至最大值，新生代，老年代region****数量可能也在变化，系统在不断运行，直接用简单粗暴的求平均是不合适的。**

 

2、如何搞一个合理的预测算法？

（1）一定得是距离本次预测越近的GC，影响比重就占的越高才行

3

1 0.2 2G 200ms

2 0.3

3 0.5

（2）G1中的**衰减标准差算法**预测模型

衰减因子：α，a，a是一个小于1的固定值，简单理解就是，这个值越小，那么最新的数据对结果的影响就越大。

 

（3）具体计算模型如下

衰减平均计算公式：

davg(n) = Vn， n=1

davg(n) = (1-a)*Vn + a*davg(n-1)， n>1

 

例如a = 0.6，GC次数为3，三次分别为，第一次回收了2GB，用时200ms，第二次回收了5GB，用时300ms，第三次回收了3GB，用时500ms

那么计算结果就如下：

davg(1) = 2GB/200ms

davg(2) = (1-0.6)*5GB/300ms + 0.6*2GB/200ms

davg(3) = (1-0.6)*3GB/500ms + 0.6( (1-0.6)*5GB/300ms +0.6 *2GB/200ms)

 

从这个演变过程中也能看的出来，我们计算出来的平均值davg(3)中，权重最大的就是最后一次GC，实际上，在G1中停顿预测模型，就是基于这么一套数学模型来实现的。这样就可以以最合理，最精准的GC预测算法，预测出来本次GC在目标停顿时间范围内能够回收多少垃圾。

 

3、基于衰减算法模型的垃圾回收过程

![picture.png](http://wechatapppro-1252524126.file.myqcloud.com/image/ueditor/75138600_1640327176.png)

在两种不同的预测模型中，很显然，衰减预测模型更能反应出当前JVM的GC运行情况，因此在这种算法模型下，可以更好的帮助G1完成垃圾回收，并且能够更好的满足目标停顿时间。