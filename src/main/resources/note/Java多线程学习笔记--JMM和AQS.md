### Java多线程学习笔记--JMM和AQS 
#### 部分基础概念
   +  这里先简单介绍一下各种锁，以便下文讲到相关概念时能有个印象。
      
      （1）synchronized
      
      java中的关键字，内部实现为监视器锁，主要是通过对象监视器在对象头中的字段来表明的。
      
      synchronized从旧版本到现在已经做了很多优化了，在运行时会有三种存在方式：偏向锁，轻量级锁，重量级锁。
      
      偏向锁，是指一段同步代码一直被一个线程访问，那么这个线程会自动获取锁，降低获取锁的代价。
      
      轻量级锁，是指当锁是偏向锁时，被另一个线程所访问，偏向锁会升级为轻量级锁，这个线程会通过自旋的方式尝试获取锁，不会阻塞，提高性能。
      
      重量级锁，是指当锁是轻量级锁时，当自旋的线程自旋了一定的次数后，还没有获取到锁，就会进入阻塞状态，该锁升级为重量级锁，重量级锁会使其他线程阻塞，性能降低。
      
      （2）CAS
      
      CAS，Compare And Swap，它是一种乐观锁，认为对于同一个数据的并发操作不一定会发生修改，在更新数据的时候，尝试去更新数据，如果失败就不断尝试。
      
      （3）volatile（非锁）
      
      java中的关键字，当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。（这里牵涉到java内存模型的知识，感兴趣的同学可以自己查查相关资料）
      
      volatile只保证可见性，不保证原子性，比如 volatile修改的变量 i，针对i++操作，不保证每次结果都正确，因为i++操作是两步操作，相当于 i = i +1，先读取，再加1，这种情况volatile是无法保证的。
      
      （4）自旋锁
      
      自旋锁，是指尝试获取锁的线程不会阻塞，而是循环的方式不断尝试，这样的好处是减少线程的上下文切换带来的开锁，提高性能，缺点是循环会消耗CPU。
      
      （5）分段锁
      
      分段锁，是一种锁的设计思路，它细化了锁的粒度，主要运用在ConcurrentHashMap中，实现高效的并发操作，当操作不需要更新整个数组时，就只锁数组中的一项就可以了。
      
      （5）ReentrantLock
      
      可重入锁，是指一个线程获取锁之后再尝试获取锁时会自动获取锁，可重入锁的优点是避免死锁。
      
      其实，synchronized也是可重入锁。
#### JMM
   - 并发编程领域的关键问题
      + 线程之间的通信
          > 线程的通信是指线程之间以何种机制来交换信息。在编程中，线程之间的通信机制有两种，共享内存和消息传递。
          在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信，典型的共享内存通信方式就是通过共享对象进行通信。
          在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信，在java中典型的消息传递方式就是wait()和notify()。
      
      + 线程之间的同步
          > 同步是指程序用于控制不同线程之间操作发生相对顺序的机制。
          在共享内存并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。
          在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。
   - JMM（Java 内存模型）
      >（JMM）解决了可见性和有序性的问题  而锁解决了原子性的问题，理想情况下我们希望做到“同步”和“互斥”
   
   - Java内存模型带来的问题
        +  可见性问题
            1. 线程1从主存中拷贝共享对象obj到它的CPU缓存，把对象obj的count变量改为2。
                但这个变更对运行的线程2不可见，因为这个更改还没有flush到主存中：要解决共享对象可见性这个问题，我们可以使用java volatile关键字或者是加锁 
            2. 竞争问题：线程A和线程B共享一个对象obj。假设线程A从主存读取Obj.count变量到自己的CPU缓存，同时，线程B也读取了Obj.count变量到它的CPU缓存，
                并且这两个线程都对Obj.count做了加1操作。此时，Obj.count加1操作被执行了两次，不过都在不同的CPU缓存中。如果这两个加1操作是串行执行的，
                那么Obj.count变量便会在原始值上加2，最终主存中的Obj.count的值会是3。然而下图中两个加1操作是并行的，不管是线程A还是线程B先flush计算结果到主存，
                最终主存中的Obj.count只会增加1次变成2，尽管一共有两次加1操作。 要解决上面的问题我们可以使用java synchronized代码块。
        + 重排序
            1. 除了共享内存和工作内存带来的问题，还存在重排序的问题：在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。
   - Java内存模型中的重排序
        + 重排序类型
             >重排序分3种类型。
             1. 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。
             2. 指令级并行的重排序。现代处理器采用了指令级并行技术（Instruction-LevelParallelism，ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
             3. 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。
        + 重排序与依赖性 
           - 数据依赖性
                >如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列3种类型
               
                |名称|代码示例|说明|
                | ---   | :---:   | :---: |
                |写后读|a=1;b=a| 写入一个位置 又去读取他|
                |写后写|a=1;a=2| 写入一个位置 又去写入他|
                |读后写|a=b;b=1| 读取一个变量 又去写入|
                上面3种情况，只要重排序两个操作的执行顺序，程序的执行结果就会被改变。
           - 控制依赖性
                ````
                 public void use(){
                    if(flag){
                        i=a*a;
                    }
                 }
                ````
                flag变量是个标记，用来标识变量a是否已被写入，在use方法中比变量i依赖if (flag)的判断，这里就叫控制依赖，如果发生了重排序，结果就不对了。
           
           - as-if-serial 
               ````
               不管如何重排序，都必须保证代码在单线程下的运行正确，连单线程下都无法正确，更不用讨论多线程并发的情况，所以就提出了一个as-if-serial的概念， 
               as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。
                   编译器、runtime和处理器都必须遵守as-if-serial语义。为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。
                   （强调一下，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。）
                   但是，如果操作之间不存在数据依赖关系，这些操作依然可能被编译器和处理器重排序。
               ````
           - 并发下重排序带来的问题
             ![](https://s2.ax1x.com/2019/09/01/n9NPcd.png)
            
             1. 这里假设有两个线程A和B，A首先执行init ()方法，随后B线程接着执行use ()方法。线程B在执行操作4时，能否看到线程A在操作1对共享变量a的写入呢？答案是：不一定能看到。
             由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。让我们先来看看，
             当操作1和操作2重排序时，可能会产生什么效果？操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还没有被线程A写入，这时就会发生错误！
             2. 当操作3和操作4重排序时会产生什么效果？
                 在程序中，操作3和操作4存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。
                 为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（Reorder Buffer，ROB）的硬件缓存中。
                 当操作3的条件判断为真时，就把该计算结果写入变量i中。猜测执行实质上对操作3和4做了重排序，问题在于这时候，a的值还没被线程A赋值。在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因）；
                 但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。
            
           -  解决在并发下的问题 
              + 内存屏障 
                 >Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序，从而让程序按我们预想的流程去执行。
                 1.  保证特定操作的执行顺序。
                 2. 影响某些数据（或则是某条指令的执行结果）的内存可见性。
                    编译器和CPU能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条Memory Barrier会告诉编译器和CPU：
                    不管什么指令都不能和这条Memory Barrier指令重排序。
                        Memory Barrier所做的另外一件事是强制刷出各种CPU cache，如一个Write-Barrier（写入屏障）将刷出所有在Barrier之前写入 cache 的数据，因此，任何CPU上的线程都能读取到这些数据的最新版本。
                        JMM把内存屏障指令分为4类，解释表格，StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。
                 
                |屏障类型|指令示例|说明
                |---|:---:|:---:|
                |LoadLoad Barriers|	Load1; LoadLoad; Load2|	确保Load1数据的装载，之前于Load2及所有后续装载指令的装载。|
                |StoreStore Barriers|	Store1; StoreStore; Store2|	确保Store1数据对其他处理器可见（刷新到内存），之前于Store2及所有后续存储指令的存储。|
                |LoadStore Barriers|	Load1; LoadStore; Store2|	确保Load1数据装载，之前于Store2及所有后续的存储指令刷新到内存。|
                |StoreLoad Barriers	|Store1; StoreLoad; Load2|	确保Store1数据对其他处理器变得可见（指刷新到内存），之前于Load2及所有后续装载指令的装载。StoreLoad Barriers会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。|
              + 临界区
                   >  临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM会在退出临界区和进入临界区这两个关键时间点做一些特别处理，虽然线程A在临界区内做了重排序，但由于监视器互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。
              + Happens-Before 
                - 定义
                    > 在Java 规范提案中为让大家理解内存可见性的这个概念，提出了happens-before的概念来阐述操作之间的内存可见性。对应Java程序员来说，理解happens-before是理解JMM的关键。JMM这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，程序员关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。因此，happens-before关系本质上和as-if-serial语义是一回事。·as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。
                - Happens-Before规则
                     1. 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。
                     
                     2. 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
                     
                     3. volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
                     
                     4. 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。
                     
                     5. start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。
                     
                     6. join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 
                     
                     7. 线程中断规则:对线程interrupt方法的调用happens-before于被中断线程的代码检测到中断事件的发生。
           + 实现原理
              > 内存语义：可以简单理解为 volatile，synchronize，atomic，lock 之类的在 JVM 中的内存方面实现原则
           
           - volatile 关键字的作用 关键字的作用（变量可见性、禁止重排序）
                           
               + 变量可见性
                   ````
                   其一是保证该变量对所有线程可见，这里的可见性指的是当一个线程修改了变量的值，那么新的
                   值对于其他线程是可以立即获取的
                   ````
               + 禁止重排序
                   ````
                 volatile 禁止了指令重排。
                 比 sychronized 更轻量级的同步锁
                 在访问 volatile 变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此 volatile 变量是一
                 种比 sychronized 关键字更轻量级的同步机制。volatile 适合这种场景：一个变量被多个线程共
                 享，线程直接给这个变量赋值。
                 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到 CPU 缓存中。如果计算机有
                 多个 CPU，每个线程可能在不同的 CPU 上被处理，这意味着每个线程可以拷贝到不同的 CPU
                 cache 中。而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache
                 这一步。
                   ````
               + 适用场景
                   ````
                 值得说明的是对 volatile 变量的单次读/写操作可以保证原子性的，如 long 和 double 类型变量，
                 但是并不能保证 i++这种操作的原子性，因为本质上 i++是读、写两次操作。在某些场景下可以
                 代替 Synchronized。但是,volatile 的不能完全取代 Synchronized 的位置，只有在一些特殊的场
                 景下，才能适用 volatile。总的来说，必须同时满足下面两个条件才能保证在并发环境的线程安
                 全：
                 （1）对变量的写操作不依赖于当前值（比如 i++），或者说是单纯的变量赋值（boolean
                 flag = true）。
                 （2）该变量没有包含在具有其他变量的不变式中，也就是说，不同的 volatile 变量之间，不
                 能互相依赖。只有在状态真正独立于程序内其他内容时才能使用 volatile
                 ps : concorrenthashmap 也有这个问题  读写两次操作 不保持原子性
           +  锁的内存语义
                 ````
                 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中。。
                 当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须从主内存中读取共享变量。
           + synchronized的实现原理
                ````
                 使用monitorenter和monitorexit指令实现的：
                 monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处
                 每个monitorenter必须有对应的monitorexit与之配对
                 任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态 
           + final的内存语义
            
              - 编译器和处理器要遵守两个重排序规则：
                 > 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。
                  初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能重排序。
              - final域为引用类型：
                  > 增加了如下规则：在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。
             - final语义在处理器中的实现：
                  1. 会要求编译器在final域的写之后，构造函数return之前插入一个StoreStore障屏。
                  2. 读final域的重排序规则要求编译器在读final域的操作前面插入一个LoadLoad屏障   
#### AQS
   +  什么是 AQS（ （ 抽象的队列同步器 ） 
        ````
        AbstractQueuedSynchronizer 类如其名，抽象的队列式的同步器，AQS 定义了一套多线程访问
        共享资源的同步器框架，许多同步类实现都依赖于它，如常用的
        ReentrantLock/Semaphore/CountDownLatch。
        ````
   +  实现 
        ````
        它维护了一个 volatile int state（代表共享资源）和一个 FIFO 线程等待队列（多线程争用资源被
        阻塞时会进入此队列）。这里 volatile 是核心关键词，具体 volatile 的语义，在此不述。state 的
        访问方式有三种:
        getState()
        setState()
        compareAndSetState()
        ````
   + AQS 定义两种资源共享方式
        ````
         Exclusive 独占资源 -ReentrantLock
         Exclusive（独占，只有一个线程能执行，如 ReentrantLock）
         Share 共享资源 -Semaphore/CountDownLatch
         Share（共享，多个线程可同时执行，如 Semaphore/CountDownLatch）
        ````
   + AQS框架提供的抽象方法
     ````
        AQS是一个框架，具体资源的获取/释放方式交由自定义同步器去实现，AQS这里只定义了一个
         接口，具体资源的获取交由自定义同步器去实现了（通过state的get/set/CAS)之所以没有定义成
         abstract，是因为独占模式下只用实现 tryAcquire-tryRelease，而共享模式下只用实现
         tryAcquireShared-tryReleaseShared。如果都定义成abstract，那么每个模式也要去实现另一模
         式下的接口。不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实
         现共享资源 state 的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/
         唤醒出队等），AQS 已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法：
     ````
     1. isHeldExclusively()：该线程是否正在独占资源。只有用到 condition 才需要去实现它。
     2.  tryAcquire(int)：独占方式。尝试获取资源，成功则返回 true，失败则返回 false。
     3. tryRelease(int)：独占方式。尝试释放资源，成功则返回 true，失败则返回 false。
     4.  tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0 表示成功，但没有剩余
     可用资源；正数表示成功，且有剩余资源。
     5. tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回
     true，否则返回 false。
   + 同步器 的实现是 ABS 核心（ state 资源状态计数）
        ````
        同步器的实现是 ABS 核心，以 ReentrantLock 为例，state 初始化为 0，表示未锁定状态。A 线程
        lock()时，会调用 tryAcquire()独占该锁并将 state+1。此后，其他线程再 tryAcquire()时就会失
        败，直到A线程unlock()到state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放
        锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，
        获取多少次就要释放多么次，这样才能保证 state 是能回到零态的。
        
        以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与
        线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后 countDown()一次，state
        会 CAS 减 1。等到所有子线程都执行完后(即 state=0)，会 unpark()主调用线程，然后主调用线程
        就会从 await()函数返回，继续后余动作。
        ````
   + ReentrantReadWriteLock 实现独占和共享两种 方式
        ````
     一般来说，自定义同步器要么是独占方法，要么是共享方式，他们也只需实现 tryAcquire-
     tryRelease、tryAcquireShared-tryReleaseShared 中的一种即可。但 AQS 也支持自定义同步器
     同时实现独占和共享两种方式，如 ReentrantReadWriteLock
     ````